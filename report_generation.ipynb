{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bcbf75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb489fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kumar\\AppData\\Local\\Temp\\ipykernel_21384\\2743022780.py:6: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "# Load API key\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Setup LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    openai_api_key=openai_api_key\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04950344",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generate structured report, summarize code comparision validation and inferences with visual differences and recommendations\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "Input from Block 1 -> Format ?\n",
    "Input from Block 2 -> Format ?\n",
    "Input from Block 3 -> Format ?\n",
    "\n",
    "Generate report:\n",
    "Components of report based on given input\n",
    "1. Validation metrics \n",
    "2. Code comparision inferences\n",
    "3. Recommendations\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9841987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create report\n",
    "def CreateReport(input1, input2, input3):\n",
    "    formatted_prompt = f\"\"\"\n",
    "   You are an AI report generator. Based on the inputs provided, create a structured report with the following three sections:\n",
    "\n",
    "   1. **Validation Metrics**\n",
    "      - Summarize model evaluation metrics (Accuracy, Precision, Recall, F1 Score, AUC, etc.).\n",
    "      - Highlight strengths or weaknesses in these metrics.\n",
    "\n",
    "   2. **Code Comparison Inferences**\n",
    "      - Compare implemented code functionalities with described goals or documentation (e.g., whitepaper).\n",
    "      - Identify alignments, discrepancies, or missing elements.\n",
    "      - Highlight improvements or regressions.\n",
    "\n",
    "   3. **Recommendations**\n",
    "      - Suggest improvements or next steps based on validation results and code comparison.\n",
    "      - Include actionable changes to improve accuracy, consistency, or system robustness.\n",
    "\n",
    "   ---\n",
    "\n",
    "   ### üî¢ Input Block 1 (Enhancements & Changes):\n",
    "   {input1}\n",
    "\n",
    "   ---\n",
    "\n",
    "   ### üß© Input Block 2 (Version-wise Comparison):\n",
    "   {input2}\n",
    "\n",
    "   ---\n",
    "\n",
    "   ### üìÑ Input Block 3 (Whitepaper vs Code Analysis):\n",
    "   {input3}\n",
    "\n",
    "   Generate the report in markdown format with bullet points for clarity.\n",
    "      \"\"\"\n",
    "\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=\"You are a helpful assistant that creates structured reports for ML systems.\"),\n",
    "        HumanMessage(content=formatted_prompt)\n",
    "    ])\n",
    "\n",
    "    return response.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a21e4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Summary ===\n",
      "# AI System Evaluation Report\n",
      "\n",
      "## 1. Validation Metrics\n",
      "\n",
      "- **Accuracy**: Improved from ~78% to 84% after enhancements.\n",
      "- **Precision, Recall, F1 Score, AUC**: Newly introduced metrics provide a comprehensive evaluation of model performance, especially for imbalanced classes.\n",
      "- **Strengths**:\n",
      "  - Significant improvement in accuracy and overall model performance due to feature engineering and model tuning.\n",
      "  - Introduction of multiple evaluation metrics offers deeper insights into model performance.\n",
      "- **Weaknesses**:\n",
      "  - No specific weaknesses identified in the validation metrics; however, continuous monitoring is recommended to ensure consistent performance across different datasets.\n",
      "\n",
      "## 2. Code Comparison Inferences\n",
      "\n",
      "- **Implemented Code vs. Described Goals**:\n",
      "  - **Alignments**:\n",
      "    - The codebase now includes modular scripts and a clean folder structure, aligning with best practices for production-grade systems.\n",
      "    - Feature engineering and preprocessing enhancements align with the goals of improving model input representation.\n",
      "    - The introduction of Random Forest and XGBoost models, along with hyperparameter tuning, aligns with the goal of improving model performance.\n",
      "    - The addition of a Streamlit app for real-time predictions aligns with the goal of creating a user-friendly interface.\n",
      "    - MLflow integration for experiment tracking aligns with the goal of enhancing reproducibility and collaboration.\n",
      "  - **Discrepancies**:\n",
      "    - The whitepaper lacks mention of the \"Sepal length (cm)\" feature used in the code.\n",
      "    - Validation metrics in the code exceed those specified in the whitepaper, indicating a need for documentation updates.\n",
      "- **Improvements/Regressions**:\n",
      "  - The system has improved significantly in terms of modularity, explainability, and user interaction compared to the initial version.\n",
      "  - No regressions identified; the system has evolved from a proof-of-concept to a professional-grade ML system.\n",
      "\n",
      "## 3. Recommendations\n",
      "\n",
      "- **Documentation Updates**:\n",
      "  - Update the whitepaper to include the \"Sepal length (cm)\" feature and revise validation metrics to reflect the improved performance.\n",
      "- **Model and Feature Enhancements**:\n",
      "  - Continue exploring additional feature engineering techniques to further enhance model performance.\n",
      "  - Consider implementing additional models or ensemble methods to further improve accuracy and robustness.\n",
      "- **System Robustness and Consistency**:\n",
      "  - Regularly validate the model on new datasets to ensure consistent performance.\n",
      "  - Implement automated testing for data preprocessing and model inference to catch potential errors early.\n",
      "- **User Interface and Deployment**:\n",
      "  - Gather user feedback on the Streamlit app to identify areas for improvement in user experience.\n",
      "  - Explore deployment options to scale the application for larger user bases if needed.\n",
      "\n",
      "By following these recommendations, the system can maintain its high performance and continue to evolve in response to user needs and technological advancements.\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Run the report generator\n",
    "if __name__ == \"__main__\":\n",
    "    input1 = '''New Features / Enhancements:\n",
    "    Added feature engineering pipeline for income, credit history, and loan term normalization.\n",
    "    Integrated missing value imputation using median/mode strategies.\n",
    "    Introduced XGBoost and Random Forest classifiers alongside logistic regression for improved model performance.\n",
    "    Implemented model selection and hyperparameter tuning using GridSearchCV.\n",
    "    Added streamlit-based frontend for interactive loan approval predictions.\n",
    "    Included model versioning with MLflow for tracking experiments.\n",
    "    üêõ Bug Fixes / Code Refactoring:\n",
    "    Fixed issue with incorrect encoding of categorical variables (replaced LabelEncoder with OneHotEncoder).\n",
    "    Refactored data loading and preprocessing into modular functions (data_utils.py).\n",
    "    Improved error handling and logging across preprocessing and inference scripts.\n",
    "    üìÅ Repository Structure Updates:\n",
    "    Created notebooks/, src/, and models/ directories for cleaner project organization.\n",
    "    Added requirements.txt and README.md with setup instructions.\n",
    "    Updated .gitignore to exclude model artifacts and environment files.\n",
    "    üìà Performance Changes:\n",
    "    Validation accuracy improved from ~78% to 84% with model tuning and feature engineering.\n",
    "    Reduced training time by 20% after optimizing preprocessing and model pipeline.\n",
    "    üìå Commit Comparison Highlights:\n",
    "    Compared commits: a1c2b3d (old baseline model) ‚Üí d4e5f6g (latest tuned system).\n",
    "    Major differences:\n",
    "    Introduction of new ML models and evaluation metrics.\n",
    "    UI integration for real-time prediction.\n",
    "    Codebase modularization and documentation improvements.'''\n",
    "\n",
    "    input2 = '''Structure & Organization\n",
    "    V1: Single Jupyter notebook; all logic inline.\n",
    "    V2: Modular scripts (data_utils.py, model.py, app.py); clean folder structure.\n",
    "    Inference: Shift from exploratory to production-grade code.\n",
    "    üßÆ Data Preprocessing\n",
    "    V1: Basic null handling and LabelEncoder.\n",
    "    V2: SimpleImputer, OneHotEncoder, scaling, ColumnTransformer.\n",
    "    Inference: More robust and reusable preprocessing pipeline.\n",
    "    üß† Feature Engineering\n",
    "    V1: Used raw features.\n",
    "    V2: Added domain-driven features (e.g., debt-to-income ratio, loan amount bins).\n",
    "    Inference: Better input representation, likely improved model performance.\n",
    "    ü§ñ Modeling\n",
    "    V1: Logistic Regression, no tuning.\n",
    "    V2: Added Random Forest, XGBoost, and GridSearchCV.\n",
    "    Inference: More powerful models with hyperparameter optimization.\n",
    "    üìä Evaluation\n",
    "    V1: Accuracy only.\n",
    "    V2: Precision, Recall, F1, AUC, confusion matrix.\n",
    "    Inference: Deeper insight into performance, especially for imbalanced classes.\n",
    "    üåê Deployment/UI\n",
    "    V1: No interface\n",
    "    V2: Streamlit app for real-time predictions.\n",
    "    Inference: User-friendly and deployable.\n",
    "\n",
    "    üìà Experiment Tracking\n",
    "    V1: None.\n",
    "    V2: MLflow used for tracking metrics and versions.\n",
    "    Inference: Enables reproducibility and team collaboration.\n",
    "\n",
    "    ‚úÖ Final Verdict:\n",
    "    V2 demonstrates a professional-grade ML system ‚Äî modular, explainable, user-facing, and maintainable ‚Äî a significant upgrade over the initial proof-of-concept in V1.\n",
    "    '''\n",
    "    input3 = '''Connecting\n",
    "    üß† AI Feature Mapping Validator\n",
    "    Compare functionalities between a Whitepaper and its Codebase\n",
    "    üìÑ Upload Whitepaper File\n",
    "\n",
    "    white paper.txt\n",
    "    Drag and drop file here\n",
    "    Limit 200MB per file ‚Ä¢ TXT, MD, PDF\n",
    "    white paper.txt\n",
    "    329.0B\n",
    "    üíª Upload Code File\n",
    "\n",
    "    model.ipynb\n",
    "    Drag and drop file here\n",
    "    Limit 200MB per file ‚Ä¢ PY, TXT, IPYNB\n",
    "    model.ipynb\n",
    "    8.9KB\n",
    "\n",
    "    ‚öñÔ∏è Comparing Functionalities\n",
    "    To perform a comprehensive comparison between the whitepaper and the code functionalities, let's break down the information provided and identify any discrepancies or updates needed:\n",
    "\n",
    "    Comparison of Features:\n",
    "    Whitepaper Features:\n",
    "\n",
    "    Sepal width (cm)\n",
    "    Petal length (cm)\n",
    "    Petal width (cm)\n",
    "    Code Features:\n",
    "\n",
    "    Sepal length (cm)\n",
    "    Sepal width (cm)\n",
    "    Petal length (cm)\n",
    "    Petal width (cm)\n",
    "    Missing Feature in Whitepaper:\n",
    "\n",
    "    Sepal length (cm) is used in the code but not mentioned in the whitepaper.\n",
    "    Comparison of Model:\n",
    "    Both the whitepaper and the code use Logistic Regression. There is no discrepancy here.\n",
    "    Comparison of Validation Metrics:\n",
    "    Whitepaper Metrics:\n",
    "\n",
    "    Accuracy: 90%\n",
    "    Precision: 90%\n",
    "    Recall: 85%\n",
    "    F1 Score: 88%\n",
    "    Code Metrics:\n",
    "\n",
    "    Accuracy: 96.67%\n",
    "    Precision: 96.67%\n",
    "    Recall: 96.67%\n",
    "    F1 Score: 96.67%\n",
    "    Comparison of Scores:\n",
    "\n",
    "    The code metrics are higher than those specified in the whitepaper. This indicates that the model performs better than the expectations set in the whitepaper.\n",
    "    Critical Validation Metrics:\n",
    "    The critical metrics (Accuracy, Precision, Recall, F1 Score) in the code are all greater than those in the whitepaper.\n",
    "    Conclusion:\n",
    "    Feature Discrepancy: The whitepaper is missing the feature \"Sepal length (cm)\" which is used in the code.\n",
    "    Validation Metrics Discrepancy: The code achieves higher validation metrics than those specified in the whitepaper.\n",
    "    Action Required:\n",
    "\n",
    "    White Paper Update Needed: The whitepaper is not updated. Please update the document to include the missing feature \"Sepal length (cm)\" and revise the validation metrics to reflect the improved performance of the model as demonstrated in the code.\n",
    "    This analysis ensures that the documentation accurately reflects the implementation and performance of the model.\n",
    "    '''\n",
    " \n",
    "    result = CreateReport(input1, input2, input3)\n",
    "    # \n",
    "    # print(\"=== Summary ===\")\n",
    "    # print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6abc4099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# AI System Evaluation Report\n",
       "\n",
       "## 1. Validation Metrics\n",
       "\n",
       "- **Accuracy**: Improved from ~78% to 84% after enhancements.\n",
       "- **Precision, Recall, F1 Score, AUC**: Newly introduced metrics provide a comprehensive evaluation of model performance, especially for imbalanced classes.\n",
       "- **Strengths**:\n",
       "  - Significant improvement in accuracy and overall model performance due to feature engineering and model tuning.\n",
       "  - Introduction of multiple evaluation metrics offers deeper insights into model performance.\n",
       "- **Weaknesses**:\n",
       "  - No specific weaknesses identified in the validation metrics; however, continuous monitoring is recommended to ensure consistent performance across different datasets.\n",
       "\n",
       "## 2. Code Comparison Inferences\n",
       "\n",
       "- **Implemented Code vs. Described Goals**:\n",
       "  - **Alignments**:\n",
       "    - The codebase now includes modular scripts and a clean folder structure, aligning with best practices for production-grade systems.\n",
       "    - Feature engineering and preprocessing enhancements align with the goals of improving model input representation.\n",
       "    - The introduction of Random Forest and XGBoost models, along with hyperparameter tuning, aligns with the goal of improving model performance.\n",
       "    - The addition of a Streamlit app for real-time predictions aligns with the goal of creating a user-friendly interface.\n",
       "    - MLflow integration for experiment tracking aligns with the goal of enhancing reproducibility and collaboration.\n",
       "  - **Discrepancies**:\n",
       "    - The whitepaper lacks mention of the \"Sepal length (cm)\" feature used in the code.\n",
       "    - Validation metrics in the code exceed those specified in the whitepaper, indicating a need for documentation updates.\n",
       "- **Improvements/Regressions**:\n",
       "  - The system has improved significantly in terms of modularity, explainability, and user interaction compared to the initial version.\n",
       "  - No regressions identified; the system has evolved from a proof-of-concept to a professional-grade ML system.\n",
       "\n",
       "## 3. Recommendations\n",
       "\n",
       "- **Documentation Updates**:\n",
       "  - Update the whitepaper to include the \"Sepal length (cm)\" feature and revise validation metrics to reflect the improved performance.\n",
       "- **Model and Feature Enhancements**:\n",
       "  - Continue exploring additional feature engineering techniques to further enhance model performance.\n",
       "  - Consider implementing additional models or ensemble methods to further improve accuracy and robustness.\n",
       "- **System Robustness and Consistency**:\n",
       "  - Regularly validate the model on new datasets to ensure consistent performance.\n",
       "  - Implement automated testing for data preprocessing and model inference to catch potential errors early.\n",
       "- **User Interface and Deployment**:\n",
       "  - Gather user feedback on the Streamlit app to identify areas for improvement in user experience.\n",
       "  - Explore deployment options to scale the application for larger user bases if needed.\n",
       "\n",
       "By following these recommendations, the system can maintain its high performance and continue to evolve in response to user needs and technological advancements."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
