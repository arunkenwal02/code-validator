{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ac416dc",
   "metadata": {},
   "source": [
    "## Check Docs Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e18532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from IPython.display import Markdown, display\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "import nbformat\n",
    "import streamlit as st\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913e4307",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. Load White Paper  (PDF)  \n",
    "2. Vectors Embeddings - Text and tables; later include images\n",
    "3. Chroma db\n",
    "4. Retrival - (Accuracy)\n",
    "5. Generation - \n",
    "    Validation-\n",
    "        Data sources - List all sources and metadata\n",
    "        Features - detect any change in features\n",
    "        Changes in Transformation steps\n",
    "        Model Details \n",
    "        Hyperparameter\n",
    "        List of Validation Metrics and resepctive scores\n",
    "        Compare Validation scores of white paper with model's validation scores\n",
    "        Brief of comparision of scores\n",
    "\n",
    "        List and Track of critical metrics - these should not be lower than mentioned (in white paper)\n",
    "\n",
    "6. Respective scores for tracked metrics (confidence on generation)\n",
    "7. If required update prompt and go back to step 4 and reiterate step 4 and 5. reason (geneation have \n",
    "   heiger confidence )\n",
    "8. Outout should be in structured format (This will be input for summary block with affitional 2 inputs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1358c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\",  \n",
    "                 temperature=0,\n",
    "                 openai_api_key= openai_api_key)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f30bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_notebook(file_path):\n",
    "    \"\"\"Read .ipynb notebook and extract content.\"\"\"\n",
    "    nb = nbformat.read(file_path, as_version=4)\n",
    "    content = []\n",
    "    for cell in nb.cells:\n",
    "        if cell.cell_type == 'markdown':\n",
    "            content.append(\"## Markdown Cell:\\n\" + cell.source)\n",
    "        elif cell.cell_type == 'code':\n",
    "            content.append(\"## Code Cell:\\n```python\\n\" + cell.source + \"\\n```\")\n",
    "    return \"\\n\\n\".join(content)\n",
    "\n",
    "\n",
    "def read_file(file_path):\n",
    "    \"\"\"Reads the content of a file.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def extract_functionalities_from_code(notebook_content):\n",
    "\n",
    "    \"\"\"Uses LLM to extract functionalities from Python code.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert Python code reviewer. Here is a Jupyter notebook:\n",
    "    {notebook_content}\n",
    "\n",
    "    The following is a Jupyter notebook content (code and markdown). \n",
    "    Please extract the following:\n",
    "    Analyze the notebook and answer:\n",
    "\n",
    "    1. List of features used in the model.\n",
    "    2. Name/type of ML model used, only name of model\n",
    "    3. Accuracy metrics (e.g., accuracy, F1, precision, recall, AUC, etc.), only metrics name. \n",
    "    4. What is the purpose of this notebook?\n",
    "    5. What are the main operations and their results?\n",
    "    6. Are there any errors or anomalies in outputs?\n",
    "    7. What conclusions can be drawn from the outputs?\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke([SystemMessage(content=\"You are a helpful assistant.\"), HumanMessage(content=prompt)])\n",
    "\n",
    "    return response.content.strip()\n",
    "\n",
    "\n",
    "def extract_functionalities_from_whitepaper(whitepaper_text):\n",
    "    \"\"\"Uses LLM to extract functionalities from whitepaper.\"\"\"\n",
    "    prompt = [\n",
    "        SystemMessage(content=\"You are a product analyst.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Here is the whitepaper or product requirement document:\n",
    "\n",
    "        {whitepaper_text}\n",
    "\n",
    "        List all functionalities or features the whitepaper mentions. Use bullet points.\n",
    "        \"\"\")\n",
    "            ]\n",
    "    return llm(prompt).content.strip()\n",
    "\n",
    "\n",
    "def compare_functionalities(whitepaper_funcs, code_funcs):\n",
    "    \"\"\"Compares two sets of functionalities using the LLM.\"\"\"\n",
    "    prompt = [\n",
    "        SystemMessage(content=\"You are a software QA expert.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Whitepaper Functionalities:\n",
    "        {whitepaper_funcs}\n",
    "\n",
    "        Code Functionalities:\n",
    "        {code_funcs}\n",
    "        Extract validation metrics from code funcs eg, precision, recall and other validation are in output cell.\n",
    "        Compare the two lists and identify which functionalities from the whitepape, if functionality is implemented in code but not available in white paper, print: white paper is not updated please update the document. and show details of each section \n",
    "        listmissing sections like feature and if model varies according to white paper and same for validation metrics.\n",
    "        compare validation scores : Compare scores of code function with white paper.\n",
    "        Also compare critical validation metrics: make sure critical metrics of code should be grater then white paper critical metrics\n",
    "        if thereis no changhe in metrics of docuemt and code_funcs: keep output 'white paper is updated please proceed to next steps. no other information is required'  \n",
    "        \n",
    "        \"\"\")\n",
    "            ]\n",
    "    return llm(prompt).content.strip()\n",
    "\n",
    "\n",
    "def read_notebook_with_outputs(file_path):\n",
    "    \"\"\"Read .ipynb notebook and include both code and output.\"\"\"\n",
    "    nb = nbformat.read(file_path, as_version=4)\n",
    "    cells_content = []\n",
    "\n",
    "    for cell in nb.cells:\n",
    "        if cell.cell_type == 'markdown':\n",
    "            cells_content.append(f\"## Markdown Cell:\\n{cell.source}\")\n",
    "        elif cell.cell_type == 'code':\n",
    "            code = f\"## Code Cell:\\n```python\\n{cell.source}\\n```\"\n",
    "            outputs = []\n",
    "\n",
    "            for output in cell.get(\"outputs\", []):\n",
    "                if output.output_type == \"stream\":\n",
    "                    outputs.append(f\"Output (stream):\\n{output.text}\")\n",
    "                elif output.output_type == \"execute_result\":\n",
    "                    # Display the result of the cell (e.g., print(2+2))\n",
    "                    result = output.get(\"data\", {}).get(\"text/plain\", \"\")\n",
    "                    outputs.append(f\"Output (execute_result):\\n{result}\")\n",
    "                elif output.output_type == \"error\":\n",
    "                    outputs.append(\"Error:\\n\" + \"\\n\".join(output.get(\"traceback\", [])))\n",
    "\n",
    "            full_output = \"\\n\".join(outputs)\n",
    "            if full_output:\n",
    "                code += f\"\\n\\n### Output:\\n```\\n{full_output}\\n```\"\n",
    "            cells_content.append(code)\n",
    "\n",
    "    return \"\\n\\n\".join(cells_content)\n",
    "\n",
    "def read_notebook(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return nbformat.read(f, as_version=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcfe8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    st.set_page_config(page_title=\"Functionality Coverage Checker\", layout=\"wide\")\n",
    "    \n",
    "    st.title(\"ðŸ§  AI Feature Mapping Validator\")\n",
    "    st.subheader(\"Compare functionalities between a Whitepaper and its Codebase\")\n",
    "\n",
    "    uploaded_whitepaper = st.file_uploader(\"ðŸ“„ Upload Whitepaper File\", type=[\"txt\", \"md\", \"pdf\"])\n",
    "    uploaded_code = st.file_uploader(\"ðŸ’» Upload Code File\", type=[\"py\", \"txt\", \"ipynb\"])\n",
    "\n",
    "    if uploaded_whitepaper and uploaded_code:\n",
    "        if st.button(\"Click to Process Files\"):\n",
    "            # Read whitepaper content\n",
    "            whitepaper = uploaded_whitepaper.read().decode(\"utf-8\")\n",
    "\n",
    "            # Handle .ipynb or other code files\n",
    "            if uploaded_code.name.endswith(\".ipynb\"):\n",
    "                # Write the raw content to a temp file\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix=\".ipynb\", mode='wb') as tmp_file:\n",
    "                    tmp_file.write(uploaded_code.read())\n",
    "                    temp_file_path = tmp_file.name\n",
    "\n",
    "                # notebook_contents = read_notebook(temp_file_path)\n",
    "                notebook_contents = read_notebook_with_outputs(temp_file_path)\n",
    "                code_funcs = extract_functionalities_from_code(notebook_contents)\n",
    "            else:\n",
    "                code = uploaded_code.read().decode(\"utf-8\")\n",
    "                code_funcs = extract_functionalities_from_code(code)\n",
    "\n",
    "            whitepaper_funcs = extract_functionalities_from_whitepaper(whitepaper)\n",
    "\n",
    "            st.markdown(\"### âš–ï¸ Comparing Functionalities\")\n",
    "            missing_funcs = compare_functionalities(whitepaper_funcs, code_funcs)\n",
    "            st.markdown(missing_funcs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c540a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29f7a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f1b071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c25394f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a51519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import nbformat\n",
    "# from openai import OpenAI\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.schema import SystemMessage, HumanMessage\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.schema import SystemMessage, HumanMessage\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# from dotenv import load_dotenv\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# import chromadb\n",
    "# from chromadb.config import Settings\n",
    "\n",
    "# import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3769bc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries = [\n",
    "#             \"Summary/Objective of white paper \",\n",
    "#             \"Features mentioned\",\n",
    "#             \"Preprocessing steps and data transformation steps\",\n",
    "#             \"Model selected for classification\",\n",
    "#             \"Training and resting methodology\",\n",
    "#             \"List of Hyper parameters and respective values\",\n",
    "#             \"What are list of validation scores and the performance scores?\",\n",
    "#             \"Ethical considerations\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b706715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryFun(query, embedding_model,collection):\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    l_docs = []\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=5)\n",
    "    for doc in results[\"documents\"][0]:\n",
    "        l_docs.append(doc)\n",
    "        # print(\"ðŸ”Ž Match:\", l_docs.append(doc))\n",
    "    return l_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81786eca",
   "metadata": {},
   "source": [
    "##  Check Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4fc45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb import PersistentClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723ab5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./chroma_openai1\"\n",
    "\n",
    "# Step 1: Load the persistent client\n",
    "chroma_client = PersistentClient(path=path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935ed8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "collections = chroma_client.list_collections()\n",
    "for col in collections:\n",
    "    print(col.name)\n",
    "    # print(col.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e06b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"notebook_f887cf79f48bf8b101631e9ebdb3ca7220bd2c6e47a6b82041ca192aa98cf16b\"\n",
    "# Step 2: Access the existing collection\n",
    "collection = chroma_client.get_collection(name=collection_name)\n",
    "data = collection.get()\n",
    "\n",
    "# Optional: View details\n",
    "print(\"IDs:\", data['ids'])\n",
    "print(\"Documents:\", data['documents'][:2])  # print only first 2 docs\n",
    "print(\"Metadata:\", data.get('metadatas'))  # only if metadata was stored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418da824",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7484104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: View details\n",
    "print(\"IDs:\", data['ids'])\n",
    "print(\"Documents:\", data['documents'][:2])  # print only first 2 docs\n",
    "print(\"Metadata:\", data.get('metadatas'))  # only if metadata was stored\n",
    "print(\"Embeddings:\", data['embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211070c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6279493b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253ac37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.get_collection(\"whitepaper_02958f41437b5bbcf9490a38b0edb5d41a365101ce7979d2822648a320dfdc73\")\n",
    "\n",
    "results = collections[6].get(\n",
    "    ids=[\"id1\", \"id2\"],    # optional\n",
    "    where={\"type\": \"pdf\"}, # optional\n",
    "    include=[\"documents\", \"metadatas\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1181a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d63925d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Embedding stored or not \n",
    "import os\n",
    "import nbformat\n",
    "from openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "import streamlit as st\n",
    "import fitz\n",
    "import tempfile\n",
    "import hashlib\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b40a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_hash(uploaded_file):\n",
    "    uploaded_file.seek(0)\n",
    "    hash_val = hashlib.sha256(uploaded_file.read()).hexdigest()\n",
    "    uploaded_file.seek(0)\n",
    "    return hash_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85886d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_pdf(uploaded_file):\n",
    "    doc = fitz.open(stream=uploaded_file.read(), filetype=\"pdf\")\n",
    "    extracted_text = \"\"\n",
    "    for page_num, page in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "        extracted_text += f\"\\n\\n--- Page {page_num + 1} ---\\n{text}\"\n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537bf1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collection_exists(collection_name):\n",
    "    try:\n",
    "        chroma_client.get_collection(collection_name)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60a085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=750,     # faster, smaller chunk\n",
    "        chunk_overlap=100   # reduced overlap\n",
    "    )\n",
    "    return text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b051a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_or_create_embeddings(uploaded_file, text, _embedding_model, collection_name):\n",
    "    chunks = create_chunks(text)\n",
    "\n",
    "    if collection_exists(collection_name):\n",
    "        collection = chroma_client.get_collection(collection_name)\n",
    "    else:\n",
    "        embeddings = _embedding_model.embed_documents(chunks)\n",
    "        print(embeddings)\n",
    "        collection = store_in_chromaDB(chunks, embeddings, collection_name)\n",
    "\n",
    "    return collection, chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad320aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import fitz\n",
    "\n",
    "def get_file_hash(file_obj):\n",
    "    file_obj.seek(0)\n",
    "    hash_val = hashlib.sha256(file_obj.read()).hexdigest()\n",
    "    file_obj.seek(0)\n",
    "    return hash_val\n",
    "\n",
    "def extract_from_pdf(file_obj):\n",
    "    file_obj.seek(0)\n",
    "    doc = fitz.open(stream=file_obj.read(), filetype=\"pdf\")\n",
    "    extracted_text = \"\"\n",
    "    for page_num, page in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "        extracted_text += f\"\\n\\n--- Page {page_num + 1} ---\\n{text}\"\n",
    "    return extracted_text\n",
    "\n",
    "file_path = \"Load Prediction Whitepaper.pdf\"\n",
    "\n",
    "with open(file_path, \"rb\") as f:\n",
    "    # 1. Compute hash\n",
    "    file_hash = get_file_hash(f)\n",
    "    # 2. Extract text\n",
    "    text = extract_from_pdf(f)\n",
    "    # 3. If needed, reset pointer for further use (not always needed)\n",
    "    f.seek(0)\n",
    "    # 4. Pass to embedding function (if needed)\n",
    "    collection, chunks = get_or_create_embeddings(\n",
    "        uploaded_file=f,              # If function needs file object\n",
    "        text=text,                    # If function needs text\n",
    "        _embedding_model=embedding_model,\n",
    "        collection_name=f\"whitepaper_{file_hash}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec64fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb95ef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_of_events():\n",
    "    # with open(\"push_events.json\", \"r\") as file:\n",
    "    #     data = json.load(file)\n",
    "    # owner = data[2]\n",
    "    # repo_name = data[3]\n",
    "    api_url = f\"https://api.github.com/repos/arunkenwal02/code-validator/events\"\n",
    "    response = requests.get(api_url)\n",
    "    events = response.json()\n",
    "    push_ids = [id['id'] for id in events]\n",
    "    print(push_ids)\n",
    "    data = ['52949273211','52821120274']\n",
    "    push_events = [e for e in events if e['type'] == 'PushEvent']\n",
    "\n",
    "    ids = [e['id'] for e in push_events]\n",
    "    try:\n",
    "        idx1 = ids.index(data[0])\n",
    "        idx2 = ids.index(data[1])\n",
    "    except ValueError:\n",
    "        return {\"error\": \"One or both push IDs not found in recent events.\"}\n",
    "\n",
    "    start = min(idx1, idx2)\n",
    "    end = max(idx1, idx2)\n",
    "\n",
    "    history_between = push_events[start:end+1]  \n",
    "    grouped_push_events = []\n",
    "    commits_list = []\n",
    "    for event in history_between:\n",
    "        push_id = event['id']\n",
    "        created_at = event['created_at']\n",
    "        repo = event['repo']['name']\n",
    "        commits_list = []\n",
    "\n",
    "        for commit in event[\"payload\"][\"commits\"]:\n",
    "            sha = commit['sha']\n",
    "            author = commit['author']['name']\n",
    "            message = commit['message']\n",
    "\n",
    "            commit_detail_url = f\"https://api.github.com/repos/arunkenwal02/code-validator/commits/{sha}\"\n",
    "            commit_detail_response = requests.get(commit_detail_url)\n",
    "\n",
    "            if commit_detail_response.status_code != 200:\n",
    "                diff = \"âŒ Failed to fetch diff\"\n",
    "            else:\n",
    "                commit_detail = commit_detail_response.json()\n",
    "                diffs = []\n",
    "                for file in commit_detail.get('files', []):\n",
    "                    patch = file.get('patch')\n",
    "                    if patch:\n",
    "                        diffs.append(f\"File: {file['filename']}\\n{patch}\")\n",
    "                diff = \"\\n\\n\".join(diffs) if diffs else \"No diff available\"\n",
    "\n",
    "            commits_list.append({\n",
    "                \"sha\": sha,\n",
    "                \"author\": author,\n",
    "                \"commit_message\": message,\n",
    "                \"code_diff\": diff\n",
    "            })\n",
    "\n",
    "        grouped_push_events.append({\n",
    "            \"push_id\": push_id,\n",
    "            \"repo\": repo,\n",
    "            \"created_at\": created_at,\n",
    "            \"commits\": commits_list\n",
    "        })\n",
    "    return grouped_push_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdfe6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = create_summary_of_events()\n",
    "test\n",
    "push id , commit summary, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a64e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499acb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc3313b",
   "metadata": {},
   "source": [
    "## One drive file access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacde4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import msal\n",
    "import requests\n",
    "import time\n",
    "import fitz\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv() \n",
    "Permission_ID =\"6a94cb3a-9869-4b54-ae0b-f4f523df2614\"\n",
    "client_id = Permission_ID\n",
    "authority = \"https://login.microsoftonline.com/consumers\"\n",
    "scopes = [\"Files.Read\"]\n",
    "source_folder = \"Documents/GitHub/code-validator/\"\n",
    "file_name = \"Load Prediction Whitepaper.pdf\"\n",
    "version_id = int(7)\n",
    "file_path = source_folder+file_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b81431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract updated version \n",
    "\n",
    "def access_token_key(client_id, authority):\n",
    "    scopes = [\"Files.Read\"]\n",
    "    app = msal.PublicClientApplication(client_id=client_id, authority=authority)\n",
    "    result = None\n",
    "\n",
    "    accounts = app.get_accounts()\n",
    "    if accounts:\n",
    "        result = app.acquire_token_silent(scopes, account=accounts[0])\n",
    "    if not result:\n",
    "        result = app.acquire_token_interactive(scopes=scopes)\n",
    "    if not result or \"access_token\" not in result:\n",
    "        print(\"MSAL Error:\", result)\n",
    "    access_token = result[\"access_token\"]\n",
    "\n",
    "    return access_token\n",
    "\n",
    "def get_raw_data(client_id, authority ,file_path ):\n",
    "    access_token= access_token_key(client_id=client_id, authority=authority)\n",
    "    url = f\"https://graph.microsoft.com/v1.0/me/drive/root:/{file_path}:/content\"\n",
    "    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "    time.sleep(2)\n",
    "    response = requests.get(url, headers=headers)\n",
    "    print(f\"Response code: {response.status_code}\")\n",
    "    if response.status_code == 200:\n",
    "        file_bytes = response.content\n",
    "        print(\"File read into memory!\")\n",
    "        return file_bytes\n",
    "    else:\n",
    "        print(\"Failed:\", response.status_code, response.text)\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_onedrive_whitepaper(file_bytes):\n",
    "    \n",
    "    # file_bytes is from above\n",
    "    doc = fitz.open(stream=file_bytes, filetype=\"pdf\")\n",
    "    text = \"\"\n",
    "    for page_num, page in enumerate(doc):\n",
    "        text += f\"\\n\\n--- Page {page_num + 1} ---\\n{page.get_text()}\"\n",
    "\n",
    "    print(\"First 1000 chars of PDF text:\", text)\n",
    "    \n",
    "    return text\n",
    " \n",
    "   \n",
    "def prev_version( client_id, authority, file_path, version_id):\n",
    "    access_token= access_token_key(client_id=client_id, authority=authority)\n",
    "\n",
    "    versions_url = f\"https://graph.microsoft.com/v1.0/me/drive/root:/{file_path}:/versions\"\n",
    "    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "    response = requests.get(versions_url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        versions = response.json()[\"value\"]\n",
    "        if len(versions) >= int(version_id):\n",
    "            # 3. Get the 2nd version (index 1)\n",
    "            version_id = versions[1]['id']\n",
    "            print(f\"2nd Version ID: {version_id}, Last Modified: {versions[1]['lastModifiedDateTime']}\")\n",
    "            \n",
    "            # 4. Fetch 2nd version's PDF bytes\n",
    "            download_url = f\"https://graph.microsoft.com/v1.0/me/drive/root:/{file_path}:/versions/{version_id}/content\"\n",
    "            version_response = requests.get(download_url, headers=headers)\n",
    "            if version_response.status_code == 200:\n",
    "                pdf_bytes = version_response.content  # This is your PDF in memory\n",
    "                \n",
    "                # 5. Extract text from the PDF (in memory, no save)\n",
    "                doc = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
    "                all_text = \"\"\n",
    "                for page_num, page in enumerate(doc):\n",
    "                    all_text += f\"\\n--- Page {page_num+1} ---\\n{page.get_text()}\"\n",
    "                \n",
    "                print(\"Extracted PDF text (first 1000 chars):\")\n",
    "                print(all_text[:1000])\n",
    "                return all_text\n",
    "                # You can use `all_text` as needed (search, LLM input, etc)\n",
    "            else:\n",
    "                print(\"Failed to download 2nd version:\", version_response.status_code, version_response.text)\n",
    "        else:\n",
    "            print(\"Less than 2 versions available!\")\n",
    "    else:\n",
    "        print(\"Failed to fetch versions:\", response.status_code, response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bde66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prev_version(client_id, authority, file_path, version_number):\n",
    "    \"\"\"\n",
    "    Fetch and parse the N-th version of a OneDrive file via Microsoft Graph.\n",
    "    version_number is 1-based: 1 = latest, 2 = previous, etc.\n",
    "    Returns the extracted PDF text.\n",
    "    \"\"\"\n",
    "    access_token = access_token_key(client_id=client_id, authority=authority)\n",
    "    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "\n",
    "    # 1) List versions\n",
    "    versions_url = f\"https://graph.microsoft.com/v1.0/me/drive/root:/{file_path}:/versions\"\n",
    "    response = requests.get(versions_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise RuntimeError(f\"Failed to fetch versions: {response.status_code} {response.text}\")\n",
    "\n",
    "    versions = response.json().get(\"value\", [])\n",
    "    if not versions:\n",
    "        raise RuntimeError(\"No versions found for this file.\")\n",
    "\n",
    "    # Sort DESC by lastModifiedDateTime so index 0 is latest, 1 is previous, etc.\n",
    "    def _parse_dt(v):\n",
    "        ts = v.get(\"lastModifiedDateTime\")\n",
    "        return datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\")) if ts else datetime.min\n",
    "    versions.sort(key=_parse_dt, reverse=True)\n",
    "\n",
    "    # 2) Show total + quick overview\n",
    "    total = len(versions)\n",
    "    print(f\"Total versions available: {total}\")\n",
    "    for i, v in enumerate(versions, start=1):\n",
    "        print(f\"{i}. id={v.get('id')} | lastModified={v.get('lastModifiedDateTime')} | size={v.get('size', 'NA')}\")\n",
    "\n",
    "    # 3) Validate requested version and pick it\n",
    "    if not (1 <= int(version_number) <= total):\n",
    "        raise ValueError(f\"Invalid version_number {version_number}. Only {total} versions exist.\")\n",
    "\n",
    "    selected = versions[int(version_number) - 1]   # 1-based â†’ 0-based\n",
    "    internal_id = selected[\"id\"]\n",
    "    print(f\"\\nSelected version #{version_number}: id={internal_id}, lastModified={selected.get('lastModifiedDateTime')}\")\n",
    "\n",
    "    # 4) Download that specific versionâ€™s content\n",
    "    download_url = f\"https://graph.microsoft.com/v1.0/me/drive/root:/{file_path}:/versions/{internal_id}/content\"\n",
    "    version_response = requests.get(download_url, headers=headers)\n",
    "    if version_response.status_code != 200:\n",
    "        raise RuntimeError(f\"Failed to download version #{version_number}: {version_response.status_code} {version_response.text}\")\n",
    "\n",
    "    pdf_bytes = version_response.content\n",
    "\n",
    "    # 5) Extract text from the PDF (in memory, no save)\n",
    "    doc = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
    "    all_text = \"\"\n",
    "    for page_num, page in enumerate(doc):\n",
    "        all_text += f\"\\n--- Page {page_num+1} ---\\n{page.get_text()}\"\n",
    "\n",
    "    print(\"\\nExtracted PDF text (first 1000 chars):\")\n",
    "    # print(all_text[:1000])\n",
    "    return all_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae9c5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "version_number = 1\n",
    "white_paper_text = prev_version(client_id, authority, file_path, version_number)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb0fc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "white_paper_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4bc5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_bytes  = get_raw_data(client_id=client_id, authority=authority,file_path = file_path)\n",
    "pdf_content = get_onedrive_whitepaper(file_bytes)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "version_id = 1\n",
    "prev_version(client_id=client_id, authority=authority,  file_path= file_path, version_id = version_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87ed6a9",
   "metadata": {},
   "source": [
    "## Get updated file from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e5ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd \n",
    "import requests\n",
    "import requests\n",
    "import base64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb8bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "push_event= pd.read_json('push_events.json', )\n",
    "latest_push_id = push_event[0].tolist()[0]\n",
    "latest_push_id\n",
    "\n",
    "# --- Usage ---\n",
    "owner = \"arunkenwal02\"\n",
    "repo = \"code-validator\"\n",
    "push_id = latest_push_id\n",
    "file_path = \"loan-approval-prediction_v2.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a0dcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sha_pair_from_push_id(owner, repo, push_id):\n",
    "    \"\"\"\n",
    "    Returns (before_sha, head_sha) for the given push_id.\n",
    "    If not found, returns (None, None).\n",
    "    \"\"\"\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}/events\"\n",
    "    resp = requests.get(url)\n",
    "    events = resp.json()\n",
    "    for event in events:\n",
    "        if event[\"type\"] == \"PushEvent\" and event[\"id\"] == str(push_id):\n",
    "            before_sha = event[\"payload\"][\"before\"]\n",
    "            head_sha = event[\"payload\"][\"head\"]\n",
    "            print(f\"Push ID: {push_id}\\nbefore: {before_sha}\\nhead: {head_sha}\")\n",
    "            return before_sha, head_sha\n",
    "    print(f\"Push ID {push_id} not found in recent events.\")\n",
    "    return None, None\n",
    "\n",
    "def fetch_latest_file_for_sha(owner, repo, file_path, sha_pairs):\n",
    "    \"\"\"\n",
    "    For each (sha_old, sha_new) in sha_pairs, check if file_path was updated.\n",
    "    If yes, download file from sha_new. Else, download most recently updated version.\n",
    "    \"\"\"\n",
    "    for i, (sha_old, sha_new) in enumerate(sha_pairs):\n",
    "        print(f\"\\nProcessing pair {i+1}: {sha_old} â†’ {sha_new}\")\n",
    "\n",
    "        # 1. Compare the two SHAs\n",
    "        compare_url = f\"https://api.github.com/repos/{owner}/{repo}/compare/{sha_old}...{sha_new}\"\n",
    "        compare_resp = requests.get(compare_url)\n",
    "        compare_data = compare_resp.json()\n",
    "\n",
    "        file_changed = False\n",
    "        for f in compare_data.get(\"files\", []):\n",
    "            if f[\"filename\"] == file_path:\n",
    "                file_changed = True\n",
    "                print(f\"File {file_path} was changed in this push.\")\n",
    "                break\n",
    "\n",
    "        if file_changed:\n",
    "            # Download updated file from sha_new\n",
    "            content_url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{file_path}\"\n",
    "            params = {\"ref\": sha_new}\n",
    "            file_resp = requests.get(content_url, params=params)\n",
    "            file_data = file_resp.json()\n",
    "            \n",
    "        # Check for 'content' key (base64-encoded)\n",
    "            if \"content\" in file_data:\n",
    "                nb_json = base64.b64decode(file_data[\"content\"]).decode(\"utf-8\")\n",
    "                notebook_dict = json.loads(nb_json)\n",
    "                return notebook_dict\n",
    "            else:\n",
    "                raise Exception(\"Notebook not found or could not fetch content. Details: \" + str(file_data))\n",
    "\n",
    "        else:\n",
    "            print(f\"File {file_path} was NOT changed between {sha_old} and {sha_new}.\")\n",
    "            # Get most recent commit where this file was updated\n",
    "            commits_url = f\"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "            params = {\"path\": file_path, \"per_page\": 1}\n",
    "            commits_resp = requests.get(commits_url, params=params)\n",
    "            last_update_sha = commits_resp.json()[0][\"sha\"]\n",
    "            print(\"Most recent commit where file was changed:\", last_update_sha)\n",
    "            # Download file at that SHA\n",
    "            content_url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{file_path}\"\n",
    "            params = {\"ref\": last_update_sha}\n",
    "            file_resp = requests.get(content_url, params=params)\n",
    "            file_data = file_resp.json()\n",
    "            \n",
    "            # Check for 'content' key (base64-encoded)\n",
    "            if \"content\" in file_data:\n",
    "                nb_json = base64.b64decode(file_data[\"content\"]).decode(\"utf-8\")\n",
    "                notebook_dict = json.loads(nb_json)\n",
    "                return notebook_dict\n",
    "            else:\n",
    "                raise Exception(\"Notebook not found or could not fetch content. Details: \" + str(file_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5518dac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sha_pair = get_sha_pair_from_push_id(owner, repo, push_id)\n",
    "\n",
    "sha_pair = [sha_pair]\n",
    "fetch_latest_file_for_sha \n",
    "# --- Usage example ---\n",
    "notebook_contents = fetch_latest_file_for_sha(owner, repo, file_path, sha_pair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befb97bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, cell in enumerate(notebook_contents['cells']):\n",
    "    if cell['cell_type'] == 'code' and cell.get('outputs'):\n",
    "        print(f\"\\nCell #{i+1}:\")\n",
    "        print(\"Code:\")\n",
    "        print(\"\".join(cell['source']))\n",
    "        print(\"\\nOutputs:\")\n",
    "        for output in cell['outputs']:\n",
    "            # Print text output (if any)\n",
    "            if 'text' in output:\n",
    "                print(\"\".join(output['text']))\n",
    "            # Print stream output\n",
    "            if output.get('output_type') == 'stream':\n",
    "                print(\"\".join(output.get('text', '')))\n",
    "            # Print execution result (display_data or execute_result)\n",
    "            if output.get('output_type') in ['execute_result', 'display_data']:\n",
    "                data = output.get('data', {})\n",
    "                # Print text/plain or html if present\n",
    "                if 'text/plain' in data:\n",
    "                    print(data['text/plain'])\n",
    "                if 'text/html' in data:\n",
    "                    print(data['text/html'])\n",
    "            # Print errors if any\n",
    "            if output.get('output_type') == 'error':\n",
    "                print(f\"Error: {output.get('ename')} - {output.get('evalue')}\")\n",
    "                print(\"Traceback:\")\n",
    "                print(\"\\n\".join(output.get('traceback', [])))\n",
    "        print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ef8bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cells_text = \"\"\n",
    "\n",
    "for i, cell in enumerate(notebook_contents['cells']):\n",
    "    if cell['cell_type'] == 'code' and cell.get('outputs'):\n",
    "        # Add cell number and code\n",
    "        all_cells_text += f\"\\nCell #{i+1}\\n\"\n",
    "        all_cells_text += \"Code:\\n\"\n",
    "        all_cells_text += \"\".join(cell['source']).strip() + \"\\n\"\n",
    "        all_cells_text += \"Output(s):\\n\"\n",
    "        # Add outputs\n",
    "        for output in cell['outputs']:\n",
    "            output_text = \"\"\n",
    "            if output.get('output_type') == 'stream':\n",
    "                text = output.get('text', '')\n",
    "                if isinstance(text, list):\n",
    "                    text = \"\".join(text)\n",
    "                output_text += text.strip()\n",
    "            elif output.get('output_type') in ['execute_result', 'display_data']:\n",
    "                data = output.get('data', {})\n",
    "                text = data.get('text/plain', '')\n",
    "                if isinstance(text, list):\n",
    "                    text = \"\".join(text)\n",
    "                output_text += text.strip()\n",
    "            # Skipping errors\n",
    "            if output_text:\n",
    "                all_cells_text += output_text + \"\\n\"\n",
    "        all_cells_text += \"-\" * 30 + \"\\n\"\n",
    "\n",
    "# Optional: remove leading/trailing whitespace\n",
    "all_cells_text = all_cells_text.strip()\n",
    "\n",
    "# Print or use as needed\n",
    "print(all_cells_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d289160",
   "metadata": {},
   "source": [
    "# Chroma DB - Prompt Outpout for white paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a759c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, chromadb\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "DATA_DIR = os.path.abspath(\"./chroma_openai1\")  # <-- match the folder that actually has your DB\n",
    "client = chromadb.PersistentClient(path=DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d54e0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for db in client.list_collections():\n",
    "    print(\"Name:\",db.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ce7392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the collection\n",
    "client.delete_collection(name=\"whitepaper_02958f41437b5bbcf9490a38b0edb5d41a365101ce7979d2822648a320dfdc73\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abff0010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the collection\n",
    "client.delete_collection(name=\"whitepaper_80155473120ea4dcf824fec347b00809b601f5a71c2ed64892a6c178903ec71b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a4e3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the collection\n",
    "client.delete_collection(name=\"whitepaper_803eb93c087768f8959427cf4ede1d1af37a2717b8a7d2b7952e58ea79b8a4ed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fef5b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_collection(name=\"notebook_0a2bc68bfeed8d271ecb43f5600b98dbb99b06c04ea769c3a6bc655a0363154a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c28a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_collection(name=\"notebook_74b8a7ff23f322cce20437b9a56a9c32681a0bab37a51ce5999bad4bb0cf0431\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355189b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_collection(name=\"notebook_f887cf79f48bf8b101631e9ebdb3ca7220bd2c6e47a6b82041ca192aa98cf16b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3644731a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.delete_collection(name=\"notebook_927f65957e11b760abafa86522bf21a1bec1308a8bc4e7df619022846279eefc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50282fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for db in client.list_collections():\n",
    "    print(\"Name:\",db.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c53d416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "\n",
    "DATA_DIR = os.path.abspath(\"./chroma_openai1\")  # same path you're using\n",
    "shutil.rmtree(DATA_DIR)  # Permanently deletes everything\n",
    "os.makedirs(DATA_DIR, exist_ok=True)  # Recreate the directory if needed\n",
    "\n",
    "print(\"ChromaDB data directory reset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced91c7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99339c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8c1eea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443ad0b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427aceaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f566a83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import os\n",
    "\n",
    "# Path to Chroma's persistent storage\n",
    "persist_dir = \"./chroma_openai1\"  # Change this if yours is different\n",
    "db_path = os.path.join(persist_dir, \"chroma.sqlite3\")\n",
    "\n",
    "if not os.path.exists(db_path):\n",
    "    raise FileNotFoundError(f\"Chroma SQLite DB not found at {db_path}\")\n",
    "\n",
    "# Connect to SQLite DB\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0d9d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb\n",
    "\n",
    "# Path to your actual persistent Chroma storage\n",
    "DATA_DIR = os.path.abspath(\"./chroma_openai1\")  # adjust if needed\n",
    "\n",
    "# Use PersistentClient to read from persistent DB\n",
    "client = chromadb.PersistentClient(path=DATA_DIR)\n",
    "\n",
    "# Get the specific collection\n",
    "collection_name = \"whitepaper_02958f41437b5bbcf9490a38b0edb5d41a365101ce7979d2822648a320dfdc73\"\n",
    "collection = client.get_collection(name=collection_name)\n",
    "\n",
    "# Retrieve all records (you can also filter with where / where_document)\n",
    "results = collection.get(\n",
    "    include=[\"documents\", \"metadatas\", \"embeddings\"],  # choose what you need\n",
    "    limit=5  # remove or increase for more\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f63f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print sample\n",
    "for doc, meta,embeddings in zip(results[\"documents\"], results[\"metadatas\"], results['embeddings']):\n",
    "    print(\"Document:\", doc)\n",
    "    print(\"Metadata:\", meta)\n",
    "    print(\"Embeddings\",embeddings)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc3cac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nbformat\n",
    "from openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import streamlit as st\n",
    "import fitz\n",
    "import tempfile\n",
    "import hashlib\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66266726",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc6cfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430d1ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryFun(query, embedding_model, collection):\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=5)\n",
    "    l_docs = [doc for doc in results[\"documents\"][0]]\n",
    "    return l_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855b5b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Summary/Objective of white paper \",\n",
    "    \"Select All features or Features name. Do not include any of the following preprocessing steps, model details, data types, or any explanations.\",\n",
    "    \"Feature names from preprocessing steps â€” list only the features on which preprocessing was applied. excluding any training methodology, model details, bias mitigation, evaluation\",\n",
    "    \"Name the Model selected for classification\",\n",
    "    \"Extract the list of hyperparameters/ performance scores/ Best hyperparameters along with their  values.\",\n",
    "    \"List of validation metrics and respective scores only and hyperparameter scores\",\n",
    "    \"Ethical considerations\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b402bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = queryFun(queries[6], embedding_model, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ca36df",
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in out:\n",
    "    print(o)\n",
    "    print(\"++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b635ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_list = [\n",
    "    \"Overview\",\n",
    "    \"Do not include descriptions, preprocessing details, training methodology, target variable explanations, or any other text.\",\n",
    "    \"Include only the train/test percentages and their purposes if mentioned. Do not include hyperparameter tuning, validation strategy, retraining details, evaluation metrics, or deployment strategy.\",\n",
    "    \"Exclude summary and Laon details. Do not consider any training methodology, model details, bias mitigation, evaluation, or future work, and output in the format <Feature_Name>: <Operation_Name> preserving the exact feature names.\",\n",
    "    \"Keep Model detail only, exclude Traning methodology, resluts, analysis, feature engineering and data handling\",\n",
    "    \"Keep only validation/perofrmance and best hyperparameter scores. Do not include other details\"\n",
    "    \"Keep only ethical considerations. do not include other details\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f8f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_extracted_elements_with_context(similar_elements, query_context):\n",
    "    combined_elements = \"\\n\\n\".join(similar_elements)\n",
    "    prompt = [\n",
    "        SystemMessage(content=\"You are a product analyst.\"),\n",
    "               \n",
    "        HumanMessage(content=f\"\"\"\n",
    "        The following are the top 5 similar elements retrieved from a vector database and  create a structured report in HTML format with the following three sections, using dangerouslySetInnerHTML={{ __html: reportMarkdown }}; html should not affect other elements : \n",
    "        {combined_elements}\n",
    "\n",
    "        The original query context is:\n",
    "        \"{query_context}\"\n",
    "        - Keep only ethical considerations. do not include other details\n",
    "        - Identify and extract only the most relevant elements or functionalities.\n",
    "        - Do not recommend, only extract \n",
    "        - Avoid verbose explanations; focus on clarity and precision.\n",
    "        - Provide concise, bullet-pointed outputs or insights based on retrieved data.\n",
    "        \"\"\")\n",
    "\n",
    "        # HumanMessage(content=f\"\"\"\n",
    "        # The following are the top 5 similar elements retrieved from a vector database : \n",
    "\n",
    "        # {combined_elements}\n",
    "\n",
    "        # The original query context is:\n",
    "        # \"{query_context}\"\n",
    "\n",
    "        # - Identify and extract only the most relevant elements or functionalities.\n",
    "        # - Avoid verbose explanations; focus on clarity and precision.\n",
    "        # - Extract details from given context keep length short in summary format \n",
    "        # - Do not recommend, only extract\n",
    "        # - Extract metrics score/values, model name and and hyperpapramter values if available in context \n",
    "        # - Provide concise, bullet-pointed outputs or insights based on retrieved data.\n",
    "        # - Format the response using IPython Markdown style for readability\n",
    "\n",
    "        # \"\"\")\n",
    "    ]\n",
    "    return llm(prompt).content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4ed16d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b476f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_output= refine_extracted_elements_with_context(out,queries[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2388d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(ref_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c028b0",
   "metadata": {},
   "source": [
    "# Chroma DB for Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cfe8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your actual persistent Chroma storage\n",
    "DATA_DIR = os.path.abspath(\"./chroma_openai1\")  # adjust if needed\n",
    "\n",
    "# Use PersistentClient to read from persistent DB\n",
    "client = chromadb.PersistentClient(path=DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51560aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for coll in client.list_collections():\n",
    "    print(\"Coll name:\", coll.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1942d7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6350d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the specific collection\n",
    "collection_name = \"notebook_74b8a7ff23f322cce20437b9a56a9c32681a0bab37a51ce5999bad4bb0cf0431\"\n",
    "collection = client.get_collection(name=collection_name)\n",
    "\n",
    "# Retrieve all records (you can also filter with where / where_document)\n",
    "results = collection.get(\n",
    "    include=[\"documents\", \"metadatas\", \"embeddings\"],  # choose what you need\n",
    "    limit=5  # remove or increase for more\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4f194b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print sample\n",
    "for doc, meta,embeddings in zip(results[\"documents\"], results[\"metadatas\"], results['embeddings']):\n",
    "    print(\"Document:\", doc)\n",
    "    print(\"Metadata:\", meta)\n",
    "    print(\"Embeddings\",embeddings)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8f9c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nbformat\n",
    "from openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import streamlit as st\n",
    "import fitz\n",
    "import tempfile\n",
    "import hashlib\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37deb9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b91343",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd31e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryFun(query, embedding_model, collection):\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=5)\n",
    "    l_docs = [doc for doc in results[\"documents\"][0]]\n",
    "    return l_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3834e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Summary/Objective of white paper.\",\n",
    "    \"Select All features or Features name. Do not include any of the following preprocessing steps, model details, data types, or any explanations.\",\n",
    "    \"Feature names from preprocessing steps â€” list only the features on which preprocessing was applied. excluding any training methodology, model details, bias mitigation, evaluation.\",\n",
    "    \"Get only Model name used for classification, exclude other information.\",\n",
    "    # \"Extract the list of hyperparameters/ performance scores/ Best hyperparameters along with their  values.\",\n",
    "    \"Extract only the list of validation metrics with their respective scores and the list of hyperparameters with their respective values, excluding all other information.\",\n",
    "    \"Ethical considerations\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebac5e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = queryFun(queries[5], embedding_model, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d02a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in out:\n",
    "    print(o)\n",
    "    print(\"++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702827f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_list = [\n",
    "    \"From the provided HTML or text, extract summary overview in 2 lines only, excluding all other sections or details.\",\n",
    "    \"Keep only features name. Do not include descriptions, preprocessing details, training methodology, target variable explanations, or any other text.\",\n",
    "    \"Include only the train/test percentages and their purposes if mentioned. Do not include hyperparameter tuning, validation strategy, retraining details, evaluation metrics, or deployment strategy.\",\n",
    "    # \"Exclude summary and Laon details. Do not consider any training methodology, model details, bias mitigation, evaluation, or future work, and output in the format <Feature_Name>: <Operation_Name> preserving the exact feature names.\",\n",
    "    \"Strictly Keep Model name only. exclude other details/information.\",\n",
    "    \"Extract only the validation/performance metrics with their scores and the best hyperparameter scores, excluding all other details.\"\n",
    "    \"Keep only ethical considerations. do not include other details.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6649ebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_extracted_elements_with_context(similar_elements, query_context,context_list):\n",
    "    combined_elements = \"\\n\\n\".join(similar_elements)\n",
    "    prompt = [\n",
    "        SystemMessage(content=\"You are a product analyst.\"),\n",
    "               \n",
    "        HumanMessage(content=f\"\"\"\n",
    "        The following are the top 5 similar elements retrieved from a vector database and  create a structured report in HTML format with the following three sections, using dangerouslySetInnerHTML={{ __html: reportMarkdown }}; html should not affect other elements : \n",
    "        {combined_elements}\n",
    "\n",
    "        The original query context is:\n",
    "        \"{query_context}\"\n",
    "        - {context_list}\n",
    "        - Identify and extract only the most relevant elements or functionalities.\n",
    "        - Do not recommend, only extract.\n",
    "        - Avoid verbose explanations; focus on clarity and precision.\n",
    "        - Provide concise, bullet-pointed outputs or insights based on retrieved data.\n",
    "        \"\"\")\n",
    "\n",
    "        # HumanMessage(content=f\"\"\"\n",
    "        # The following are the top 5 similar elements retrieved from a vector database : \n",
    "\n",
    "        # {combined_elements}\n",
    "\n",
    "        # The original query context is:\n",
    "        # \"{query_context}\"\n",
    "\n",
    "        # - Identify and extract only the most relevant elements or functionalities.\n",
    "        # - Avoid verbose explanations; focus on clarity and precision.\n",
    "        # - Extract details from given context keep length short in summary format \n",
    "        # - Do not recommend, only extract\n",
    "        # - Extract metrics score/values, model name and and hyperpapramter values if available in context \n",
    "        # - Provide concise, bullet-pointed outputs or insights based on retrieved data.\n",
    "        # - Format the response using IPython Markdown style for readability\n",
    "\n",
    "        # \"\"\")\n",
    "    ]\n",
    "    return llm(prompt).content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f00885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_output= refine_extracted_elements_with_context(out,queries[4],context_list[4] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243666e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryFun_parallel(queries, embedding_model, collection):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        futures = [executor.submit(queryFun, query, embedding_model, collection) for query in queries]\n",
    "        return [future.result() for future in as_completed(futures)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b9105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Parallel vector queries ---\n",
    "list_pdf_docs = queryFun_parallel(queries, embedding_model, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16716a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pdf_docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37de636",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ThreadPoolExecutor() as executor:\n",
    "    list_refine_context_from_extracted_element_from_pdf = list(\n",
    "        executor.map(\n",
    "            refine_extracted_elements_with_context,\n",
    "            list_pdf_docs, queries , context_list\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a99ef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(ref_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6318a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c58d91",
   "metadata": {},
   "source": [
    "# Read file from GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f2a4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pymupdf requests\n",
    "\n",
    "import fitz  # pymupdf\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from urllib.parse import quote\n",
    "\n",
    "\n",
    "\n",
    "def read_white_paper_from_gcp(filename, base_url, version_number):\n",
    "    # Safely encode filename for a URL\n",
    "\n",
    "    filename = \"Load Prediction Whitepaper.pdf\"\n",
    "    filename.split(\".\")\n",
    "    encoded_filename = quote(filename.split(\".\")[0])\n",
    "    file_type = filename.split(\".\")[1]\n",
    "    url = f\"{base_url}{encoded_filename}_v{version_number}.{file_type}\"\n",
    "\n",
    "    # Download the PDF into memory\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    # Open PDF from bytes\n",
    "    pdf_stream = BytesIO(response.content)\n",
    "    doc = fitz.open(stream=pdf_stream, filetype=\"pdf\")\n",
    "    text_block = \"\"\n",
    "    # Iterate through pages\n",
    "    for page_num in range(len(doc)):\n",
    "        page = doc[page_num]\n",
    "        text = page.get_text(\"text\")  # Extract as plain text\n",
    "        text_block += f\"--- Page {page_num + 1} ---\\n{text}\\n\\n\"\n",
    "        print(f\"--- Page {page_num + 1} ---\")\n",
    "        print(text)\n",
    "        # print()\n",
    "    return text_block \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09277d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Load Prediction Whitepaper.pdf\"\n",
    "base_url = \"https://storage.googleapis.com/whitepaper_test/\"\n",
    "version_number = 1\n",
    "\n",
    "text = read_white_paper_from_gcp(filename, base_url,version_number)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e95e86",
   "metadata": {},
   "source": [
    "# Validate White paper with version code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1d96b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Step 3: Send to GPT-4o via LangChain =====\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key= os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "system_message = (\n",
    "    \"You are an assistant that extracts document headings in 3 levels: \"\n",
    "    \"Level 1 (Main Heading), Level 2 (Subheading), and Level 3 (Sub-subheading). \"\n",
    "    \"Level 3 items can be bullet points, numbered pointers, or descriptive labels under a subheading.\"\n",
    ")\n",
    "\n",
    "user_message = f\"\"\"\n",
    "From the following PDF text, extract a complete hierarchy of headings with up to 3 levels.\n",
    "\n",
    "Output in valid JSON format like:\n",
    "{{\n",
    "  \"headings\": [\n",
    "    {{\n",
    "      \"heading\": \"Main Heading 1\",\n",
    "      \"subheadings\": [\n",
    "        {{\n",
    "          \"subheading\": \"Subheading 1.1\",\n",
    "          \"subsubheadings\": [\"Sub-subheading 1.1.1\", \"Sub-subheading 1.1.2\"]\n",
    "        }},\n",
    "        {{\n",
    "          \"subheading\": \"Subheading 1.2\",\n",
    "          \"subsubheadings\": []\n",
    "        }}\n",
    "      ]\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Rules:\n",
    "- Ignore body text that is not a heading or bullet point under a heading.\n",
    "- Preserve numbering and bullet symbols if they appear in the extracted text.\n",
    "- Bullet points can be treated as sub-subheadings if they belong to a subheading.\n",
    "\n",
    "\n",
    "PDF text:\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "response = llm([\n",
    "    SystemMessage(content=system_message),\n",
    "    HumanMessage(content=user_message)\n",
    "])\n",
    "\n",
    "# ===== Step 4: Output result =====\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e637041f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "1. Read white paper name and version \n",
    "    - Extract Heading/ Sub Heading/ Sub sub Heading only (to compare with diff (v2-v1))\n",
    "\n",
    "2. Read V1 code \n",
    "3. Read V2 Code \n",
    "    - Get code diff b/w v1 and v2 \n",
    "    - Compare code diff aganist white paper (Only for given details in white paper, exclude other details for main summary) \n",
    "    - what if critical info addedd in v2 but not mentioned oin white paper \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45a01e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, List\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def get_sha_pair_from_push_id(owner: str, repo: str, push_id: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}/events\"\n",
    "    resp = requests.get(url)\n",
    "    if resp.status_code != 200:\n",
    "        raise RuntimeError(f\"GitHub events fetch failed: {resp.status_code} {resp.text}\")\n",
    "    events = resp.json()\n",
    "    for event in events:\n",
    "        if event.get(\"type\") == \"PushEvent\" and event.get(\"id\") == str(push_id):\n",
    "            before_sha = event[\"payload\"][\"before\"]\n",
    "            head_sha = event[\"payload\"][\"head\"]\n",
    "            return before_sha, head_sha \n",
    "    return None, None\n",
    "\n",
    "\n",
    "def fetch_latest_file_for_sha(owner: str, repo: str, notebook_file_path: str, sha_pairs: List[Tuple[str, str]]) -> dict:\n",
    "    for (sha_old, sha_new) in sha_pairs:\n",
    "        compare_url = f\"https://api.github.com/repos/{owner}/{repo}/compare/{sha_old}...{sha_new}\"\n",
    "        compare_resp = requests.get(compare_url)\n",
    "        if compare_resp.status_code != 200:\n",
    "            raise RuntimeError(f\"GitHub compare failed: {compare_resp.status_code} {compare_resp.text}\")\n",
    "        compare_data = compare_resp.json()\n",
    "\n",
    "        file_changed = any(f.get(\"filename\") == notebook_file_path for f in compare_data.get(\"files\", []))\n",
    "\n",
    "        if file_changed:\n",
    "            content_url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{notebook_file_path}\"\n",
    "            print(\"File changed URL: \",content_url)\n",
    "            params = {\"ref\": sha_new}\n",
    "        else:\n",
    "            commits_url = f\"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "            params = {\"path\": notebook_file_path, \"per_page\": 1}\n",
    "            commits_resp = requests.get(commits_url, params=params)\n",
    "            if commits_resp.status_code != 200:\n",
    "                raise RuntimeError(f\"GitHub commits fetch failed: {commits_resp.status_code} {commits_resp.text}\")\n",
    "            last_update_sha = commits_resp.json()[0][\"sha\"]\n",
    "            content_url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{notebook_file_path}\"\n",
    "            print(\"File not changed URL: \",content_url)\n",
    "            params = {\"ref\": last_update_sha}\n",
    "\n",
    "        file_resp = requests.get(content_url, params=params)\n",
    "        if file_resp.status_code != 200:\n",
    "            raise RuntimeError(f\"GitHub content fetch failed: {file_resp.status_code} {file_resp.text}\")\n",
    "\n",
    "        file_data = file_resp.json()\n",
    "        if \"content\" not in file_data:\n",
    "            raise RuntimeError(\"Notebook not found or could not fetch content. Details: \" + str(file_data))\n",
    "        nb_json = base64.b64decode(file_data[\"content\"]).decode(\"utf-8\")\n",
    "        return json.loads(nb_json)\n",
    "\n",
    "    raise RuntimeError(\"No SHA pairs yielded a notebook.\")\n",
    "\n",
    "\n",
    "def read_notebook_with_outputs(owner: str, repo: str, push_id: str, notebook_file_path: str) -> str:\n",
    "    sha_pair = get_sha_pair_from_push_id(owner=owner, repo=repo, push_id=push_id)\n",
    "    if not sha_pair or not sha_pair[0] or not sha_pair[1]:\n",
    "        raise RuntimeError(f\"Push ID {push_id} not found in recent events.\")\n",
    "    notebook_contents = fetch_latest_file_for_sha(owner=owner, repo=repo, notebook_file_path=notebook_file_path, sha_pairs=[sha_pair])\n",
    "\n",
    "    all_cells_text = \"\"\n",
    "    for i, cell in enumerate(notebook_contents.get('cells', [])):\n",
    "        if cell.get('cell_type') == 'code' and cell.get('outputs'):\n",
    "            all_cells_text += f\"\\nCell #{i+1}\\n\"\n",
    "            all_cells_text += \"Code:\\n\"\n",
    "            all_cells_text += \"\".join(cell.get('source', [])).strip() + \"\\n\"\n",
    "            all_cells_text += \"Output(s):\\n\"\n",
    "            for output in cell['outputs']:\n",
    "                output_text = \"\"\n",
    "                if output.get('output_type') == 'stream':\n",
    "                    text = output.get('text', '')\n",
    "                    if isinstance(text, list):\n",
    "                        text = \"\".join(text)\n",
    "                    output_text += (text or \"\").strip()\n",
    "                elif output.get('output_type') in ['execute_result', 'display_data']:\n",
    "                    data = output.get('data', {})\n",
    "                    text = data.get('text/plain', '')\n",
    "                    if isinstance(text, list):\n",
    "                        text = \"\".join(text)\n",
    "                    output_text += (text or \"\").strip()\n",
    "                if output_text:\n",
    "                    all_cells_text += output_text + \"\\n\"\n",
    "            all_cells_text += \"-\" * 30 + \"\\n\"\n",
    "\n",
    "    return all_cells_text.strip()\n",
    "\n",
    "\n",
    "def get_first_two_push_ids(path: str = \"push_events.json\") -> list:\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"{path} not found.\")\n",
    "    \n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # If the JSON is a list of dicts\n",
    "    if isinstance(data, list):\n",
    "        for item in data[:2]:  # first two items\n",
    "            if isinstance(item, dict):\n",
    "                first_key = list(item.keys())[0]\n",
    "                results.append(str(item[first_key]))\n",
    "            else:\n",
    "                results.append(str(item))\n",
    "    \n",
    "    # If the JSON is a dict of lists\n",
    "    elif isinstance(data, dict):\n",
    "        first_key = next(iter(data))\n",
    "        results = [str(val) for val in data[first_key][:2]]\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported JSON structure.\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "800ff9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['52949273211', '53215273440']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import base64\n",
    "import requests\n",
    "\n",
    "first_two = get_first_two_push_ids(\"push_events.json\")\n",
    "print(first_two)\n",
    "first_push_id = first_two[1]\n",
    "second_push_id = first_two[0]\n",
    "\n",
    "GITHUB_OWNER = \"arunkenwal02\"\n",
    "GITHUB_REPO = \"code-validator\"\n",
    "# NOTEBOOK_FILE_PATH = \"loan-approval-prediction_v1.ipynb\"\n",
    "NOTEBOOK_FILE_PATH = \"main.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92e5458c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ee534eb29525f1c7b7febe4941cb73f87bf17264',\n",
       " '249882444dc790db55b48469bb34c891418934bd')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_push_id_sha_pair = get_sha_pair_from_push_id(owner = GITHUB_OWNER, repo =  GITHUB_REPO, push_id = first_push_id)\n",
    "first_push_id_sha_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af9e26fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not changed URL:  https://api.github.com/repos/arunkenwal02/code-validator/contents/main.ipynb\n"
     ]
    }
   ],
   "source": [
    "l_sha = fetch_latest_file_for_sha(owner = GITHUB_OWNER, repo = GITHUB_REPO, notebook_file_path = NOTEBOOK_FILE_PATH, sha_pairs = [first_push_id_sha_pair])\n",
    "# l_sha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18d08dc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('5ef4b5b48eca8cad8d86e8d88904fcb2384e8dbb',\n",
       " 'c536e703bf0e78761d6374044ef7d9c2bb482131')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_push_id_sha_pair = get_sha_pair_from_push_id(owner = GITHUB_OWNER, repo =  GITHUB_REPO, push_id = second_push_id)\n",
    "second_push_id_sha_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a513fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not changed URL:  https://api.github.com/repos/arunkenwal02/code-validator/contents/main.ipynb\n"
     ]
    }
   ],
   "source": [
    "l_sha_second_push_id = fetch_latest_file_for_sha(owner = GITHUB_OWNER, repo = GITHUB_REPO, notebook_file_path = NOTEBOOK_FILE_PATH, sha_pairs = [second_push_id_sha_pair])\n",
    "# l_sha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8a41dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- OLD_VERSION\n",
      "+++ NEW_VERSION\n",
      "@@ -1,7 +1,4 @@\n",
      "-### CELL 1 [markdown]\n",
      "-## Check Docs Validation \n",
      "-\n",
      "-### CELL 2 [code]\n",
      "+### CELL 1 [code]\n",
      " from langchain.chat_models import ChatOpenAI\n",
      " from langchain.schema import SystemMessage, HumanMessage\n",
      " from langchain.prompts import ChatPromptTemplate\n",
      "@@ -13,7 +10,7 @@\n",
      " import streamlit as st\n",
      " import tempfile\n",
      " \n",
      "-### CELL 3 [code]\n",
      "+### CELL 2 [code]\n",
      " '''\n",
      " 1. Load White Paper  (PDF)  \n",
      " 2. Vectors Embeddings - Text and tables; later include images\n",
      "@@ -38,7 +35,7 @@\n",
      " 8. Outout should be in structured format (This will be input for summary block with affitional 2 inputs)\n",
      " '''\n",
      " \n",
      "-### CELL 4 [code]\n",
      "+### CELL 3 [code]\n",
      " openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
      " \n",
      " # Initialize LLM\n",
      "@@ -48,7 +45,7 @@\n",
      " \n",
      " \n",
      " \n",
      "-### CELL 5 [code]\n",
      "+### CELL 4 [code]\n",
      " \n",
      " \n",
      " def read_notebook(file_path):\n",
      "@@ -166,7 +163,7 @@\n",
      "         return nbformat.read(f, as_version=4)\n",
      " \n",
      " \n",
      "-### CELL 6 [code]\n",
      "+### CELL 5 [code]\n",
      " \n",
      " \n",
      " def main():\n",
      "@@ -209,952 +206,3 @@\n",
      "     \n",
      " \n",
      " \n",
      "-### CELL 7 [code]\n",
      "-(empty source)\n",
      "-\n",
      "-### CELL 8 [code]\n",
      "-(empty source)\n",
      "-\n",
      "-### CELL 9 [code]\n",
      "-(empty source)\n",
      "-\n",
      "-### CELL 10 [markdown]\n",
      "-(empty source)\n",
      "-\n",
      "-### CELL 11 [code]\n",
      "-# import os\n",
      "-# import nbformat\n",
      "-# from openai import OpenAI\n",
      "-# from langchain.chat_models import ChatOpenAI\n",
      "-# from langchain.schema import SystemMessage, HumanMessage\n",
      "-# from langchain.chat_models import ChatOpenAI\n",
      "-# from langchain.schema import SystemMessage, HumanMessage\n",
      "-# from langchain_openai import OpenAIEmbeddings\n",
      "-# from dotenv import load_dotenv\n",
      "-# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
      "-# import chromadb\n",
      "-# from chromadb.config import Settings\n",
      "-\n",
      "-# import tempfile\n",
      "-\n",
      "-### CELL 12 [code]\n",
      "-# queries = [\n",
      "-#             \"Summary/Objective of white paper \",\n",
      "-#             \"Features mentioned\",\n",
      "-#             \"Preprocessing steps and data transformation steps\",\n",
      "-#             \"Model selected for classification\",\n",
      "-#             \"Training and resting methodology\",\n",
      "-#             \"List of Hyper parameters and respective values\",\n",
      "-#             \"What are list of validation scores and the performance scores?\",\n",
      "-#             \"Ethical considerations\" ]\n",
      "-\n",
      "-### CELL 13 [code]\n",
      "-def queryFun(query, embedding_model,collection):\n",
      "-    query_embedding = embedding_model.embed_query(query)\n",
      "-    l_docs = []\n",
      "-    results = collection.query(query_embeddings=[query_embedding], n_results=5)\n",
      "-    for doc in results[\"documents\"][0]:\n",
      "-        l_docs.append(doc)\n",
      "-        # print(\"ðŸ”Ž Match:\", l_docs.append(doc))\n",
      "-    return l_docs\n",
      "-\n",
      "-### CELL 14 [markdown]\n",
      "-##  Check Embeddings \n",
      "-\n",
      "-### CELL 15 [code]\n",
      "-import chromadb\n",
      "-from chromadb.config import Settings\n",
      "-from chromadb import PersistentClient\n",
      "-\n",
      "-### CELL 16 [code]\n",
      "-path = \"./chroma_openai1\"\n",
      "-\n",
      "-# Step 1: Load the persistent client\n",
      "-chroma_client = PersistentClient(path=path)\n",
      "-\n",
      "-\n",
      "-\n",
      "-### CELL 17 [code]\n",
      "-collections = chroma_client.list_collections()\n",
      "-for col in collections:\n",
      "-    print(col.name)\n",
      "-    # print(col.metadata)\n",
      "-\n",
      "-### CELL 18 [code]\n",
      "-collection_name = \"notebook_f887cf79f48bf8b101631e9ebdb3ca7220bd2c6e47a6b82041ca192aa98cf16b\"\n",
      "-# Step 2: Access the existing collection\n",
      "-collection = chroma_client.get_collection(name=collection_name)\n",
      "-data = collection.get()\n",
      "-\n",
      "-# Optional: View details\n",
      "-print(\"IDs:\", data['ids'])\n",
      "-print(\"Documents:\", data['documents'][:2])  # print only first 2 docs\n",
      "-print(\"Metadata:\", data.get('metadatas'))  # only if metadata was stored\n",
      "-\n",
      "-### CELL 19 [code]\n",
      "-data.keys()\n",
      "-\n",
      "-### CELL 20 [code]\n",
      "-# Optional: View details\n",
      "-print(\"IDs:\", data['ids'])\n",
      "-print(\"Documents:\", data['documents'][:2])  # print only first 2 docs\n",
      "-print(\"Metadata:\", data.get('metadatas'))  # only if metadata was stored\n",
      "-print(\"Embeddings:\", data['embeddings'])\n",
      "-\n",
      "-### CELL 21 [code]\n",
      "-(empty source)\n",
      "-\n",
      "-### CELL 22 [code]\n",
      "-(empty source)\n",
      "-\n",
      "-### CELL 23 [code]\n",
      "-collection = client.get_collection(\"whitepaper_02958f41437b5bbcf9490a38b0edb5d41a365101ce7979d2822648a320dfdc73\")\n",
      "-\n",
      "-results = collections[6].get(\n",
      "-    ids=[\"id1\", \"id2\"],    # optional\n",
      "-    where={\"type\": \"pdf\"}, # optional\n",
      "-    include=[\"documents\", \"metadatas\"]\n",
      "-)\n",
      "-\n",
      "-### CELL 24 [code]\n",
      "-results\n",
      "-\n",
      "-### CELL 25 [code]\n",
      "- # Embedding stored or not \n",
      "-import os\n",
      "-import nbformat\n",
      "-from openai import OpenAI\n",
      "-from langchain.chat_models import ChatOpenAI\n",
      "-from langchain.schema import SystemMessage, HumanMessage\n",
      "-from langchain_openai import OpenAIEmbeddings\n",
      "-from dotenv import load_dotenv\n",
      "-from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
      "-import chromadb\n",
      "-from chromadb.config import Settings\n",
      "-\n",
      "-import streamlit as st\n",
      "-import fitz\n",
      "-import tempfile\n",
      "-import hashlib\n",
      "-from concurrent.futures import ThreadPoolExecutor, as_completed\n",
      "-\n",
      "-\n",
      "-load_dotenv()\n",
      "-openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
      "-llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n",
      "-embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
      "-\n",
      "-### CELL 26 [code]\n",
      "-def get_file_hash(uploaded_file):\n",
      "-    uploaded_file.seek(0)\n",
      "-    hash_val = hashlib.sha256(uploaded_file.read()).hexdigest()\n",
      "-    uploaded_file.seek(0)\n",
      "-    return hash_val\n",
      "-\n",
      "-### CELL 27 [code]\n",
      "-def extract_from_pdf(uploaded_file):\n",
      "-    doc = fitz.open(stream=uploaded_file.read(), filetype=\"pdf\")\n",
      "-    extracted_text = \"\"\n",
      "-    for page_num, page in enumerate(doc):\n",
      "-        text = page.get_text()\n",
      "-        extracted_text += f\"\\n\\n--- Page {page_num + 1} ---\\n{text}\"\n",
      "-    return extracted_text\n",
      "-\n",
      "-### CELL 28 [code]\n",
      "-def collection_exists(collection_name):\n",
      "-    try:\n",
      "-        chroma_client.get_collection(collection_name)\n",
      "-        return True\n",
      "-    except Exception:\n",
      "-        return False\n",
      "-\n",
      "-### CELL 29 [code]\n",
      "-def create_chunks(text):\n",
      "-    text_splitter = RecursiveCharacterTextSplitter(\n",
      "-        chunk_size=750,     # faster, smaller chunk\n",
      "-        chunk_overlap=100   # reduced overlap\n",
      "-    )\n",
      "-    return text_splitter.split_text(text)\n",
      "-\n",
      "-### CELL 30 [code]\n",
      "-\n",
      "-\n",
      "-def get_or_create_embeddings(uploaded_file, text, _embedding_model, collection_name):\n",
      "-    chunks = create_chunks(text)\n",
      "-\n",
      "-    if collection_exists(collection_name):\n",
      "-        collection = chroma_client.get_collection(collection_name)\n",
      "-    else:\n",
      "-        embeddings = _embedding_model.embed_documents(chunks)\n",
      "-        print(embeddings)\n",
      "-        collection = store_in_chromaDB(chunks, embeddings, collection_name)\n",
      "-\n",
      "-    return collection, chunks\n",
      "-\n",
      "-\n",
      "-### CELL 31 [code]\n",
      "-import hashlib\n",
      "-import fitz\n",
      "-\n",
      "-def get_file_hash(file_obj):\n",
      "-    file_obj.seek(0)\n",
      "-    hash_val = hashlib.sha256(file_obj.read()).hexdigest()\n",
      "-    file_obj.seek(0)\n",
      "-    return hash_val\n",
      "-\n",
      "-def extract_from_pdf(file_obj):\n",
      "-    file_obj.seek(0)\n",
      "-    doc = fitz.open(stream=file_obj.read(), filetype=\"pdf\")\n",
      "-    extracted_text = \"\"\n",
      "-    for page_num, page in enumerate(doc):\n",
      "-        text = page.get_text()\n",
      "-        extracted_text += f\"\\n\\n--- Page {page_num + 1} ---\\n{text}\"\n",
      "-    return extracted_text\n",
      "-\n",
      "-file_path = \"Load Prediction Whitepaper.pdf\"\n",
      "-\n",
      "-with open(file_path, \"rb\") as f:\n",
      "-    # 1. Compute hash\n",
      "-    file_hash = get_file_hash(f)\n",
      "-    # 2. Extract text\n",
      "-    text = extract_from_pdf(f)\n",
      "-    # 3. If needed, reset pointer for further use (not always needed)\n",
      "-    f.seek(0)\n",
      "-    # 4. Pass to embedding function (if needed)\n",
      "-    collection, chunks = get_or_create_embeddings(\n",
      "-        uploaded_file=f,              # If function needs file object\n",
      "-        text=text,                    # If function needs text\n",
      "-        _embedding_model=embedding_model,\n",
      "-        collection_name=f\"whitepaper_{file_hash}\"\n",
      "-    )\n",
      "-\n",
      "-### CELL 32 [code]\n",
      "-import requests\n",
      "-\n",
      "-### CELL 33 [code]\n",
      "-def create_summary_of_events():\n",
      "-    # with open(\"push_events.json\", \"r\") as file:\n",
      "-    #     data = json.load(file)\n",
      "-    # owner = data[2]\n",
      "-    # repo_name = data[3]\n",
      "-    api_url = f\"https://api.github.com/repos/arunkenwal02/code-validator/events\"\n",
      "-    response = requests.get(api_url)\n",
      "-    events = response.json()\n",
      "-    push_ids = [id['id'] for id in events]\n",
      "-    print(push_ids)\n",
      "-    data = ['52949273211','52821120274']\n",
      "-    push_events = [e for e in events if e['type'] == 'PushEvent']\n",
      "-\n",
      "-    ids = [e['id'] for e in push_events]\n",
      "-    try:\n",
      "-        idx1 = ids.index(data[0])\n",
      "-        idx2 = ids.index(data[1])\n",
      "-    except ValueError:\n",
      "-        return {\"error\": \"One or both push IDs not found in recent events.\"}\n",
      "-\n",
      "-    start = min(idx1, idx2)\n",
      "-    end = max(idx1, idx2)\n",
      "-\n",
      "-    history_between = push_events[start:end+1]  \n",
      "-    grouped_push_events = []\n",
      "-    commits_list = []\n",
      "-    for event in history_between:\n",
      "-        push_id = event['id']\n",
      "-        created_at = event['created_at']\n",
      "-        repo = event['repo']['name']\n",
      "-        commits_list = []\n",
      "-\n",
      "-        for commit in event[\"payload\"][\"commits\"]:\n",
      "-            sha = commit['sha']\n",
      "-            author = commit['author']['name']\n",
      "-            message = commit['message']\n",
      "-\n",
      "-            commit_detail_url = f\"https://api.github.com/repos/arunkenwal02/code-validator/commits/{sha}\"\n",
      "-            commit_detail_response = requests.get(commit_detail_url)\n",
      "-\n",
      "-            if commit_detail_response.status_code != 200:\n",
      "-                diff = \"âŒ Failed to fetch diff\"\n",
      "-            else:\n",
      "-                commit_detail = commit_detail_response.json()\n",
      "-                diffs = []\n",
      "-                for file in commit_detail.get('files', []):\n",
      "-                    patch = file.get('patch')\n",
      "-                    if patch:\n",
      "-                        diffs.append(f\"File: {file['filename']}\\n{patch}\")\n",
      "-                diff = \"\\n\\n\".join(diffs) if diffs else \"No diff available\"\n",
      "-\n",
      "-            commits_list.append({\n",
      "-                \"sha\": sha,\n",
      "-                \"author\": author,\n",
      "-                \"commit_message\": message,\n",
      "-                \"code_diff\": diff\n",
      "-            })\n",
      "-\n",
      "-        grouped_push_events.append({\n",
      "-            \"push_id\": push_id,\n",
      "-            \"repo\": repo,\n",
      "-            \"created_at\": created_at,\n",
      "-            \"commits\": commits_list\n",
      "-        })\n",
      "-    return grouped_push_events\n",
      "-\n",
      "-### CELL 34 [code]\n",
      "-test = create_summary_of_events()\n",
      "-test\n",
      "-push id , commit summary, \n",
      "-\n",
      "-### CELL 35 [code]\n",
      "-test[5]\n",
      "-\n",
      "-### CELL 36 [code]\n",
      "-len(test)\n",
      "-\n",
      "-### CELL 37 [markdown]\n",
      "-## One drive file access\n",
      "-\n",
      "-### CELL 38 [code]\n",
      "-import msal\n",
      "-import requests\n",
      "-import time\n",
      "-import fitz\n",
      "-import os \n",
      "-from dotenv import load_dotenv\n",
      "-from datetime import datetime\n",
      "-\n",
      "-\n",
      "-\n",
      "-load_dotenv() \n",
      "-Permission_ID =\"6a94cb3a-9869-4b54-ae0b-f4f523df2614\"\n",
      "-client_id = Permission_ID\n",
      "-authority = \"https://login.microsoftonline.com/consumers\"\n",
      "-scopes = [\"Files.Read\"]\n",
      "-source_folder = \"Documents/GitHub/code-validator/\"\n",
      "-file_name = \"Load Prediction Whitepaper.pdf\"\n",
      "-version_id = int(7)\n",
      "-file_path = source_folder+file_name\n",
      "-\n",
      "-\n",
      "-### CELL 39 [code]\n",
      "-# Extract updated version \n",
      "-\n",
      "-def access_token_key(client_id, authority):\n",
      "-    scopes = [\"Files.Read\"]\n",
      "-    app = msal.PublicClientApplication(client_id=client_id, authority=authority)\n",
      "-    result = None\n",
      "-\n",
      "-    accounts = app.get_accounts()\n",
      "-    if accounts:\n",
      "-        result = app.acquire_token_silent(scopes, account=accounts[0])\n",
      "-    if not result:\n",
      "-        result = app.acquire_token_interactive(scopes=scopes)\n",
      "-    if not result or \"access_token\" not in result:\n",
      "-        print(\"MSAL Error:\", result)\n",
      "-    access_token = result[\"access_token\"]\n",
      "-\n",
      "-    return access_token\n",
      "-\n",
      "-def get_raw_data(client_id, authority ,file_path ):\n",
      "-    access_token= access_token_key(client_id=client_id, authority=authority)\n",
      "-    url = f\"https://graph.microsoft.com/v1.0/me/drive/root:/{file_path}:/content\"\n",
      "-    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
      "-    time.sleep(2)\n",
      "-    response = requests.get(url, headers=headers)\n",
      "-    print(f\"Response code: {response.status_code}\")\n",
      "-    if response.status_code == 200:\n",
      "-        file_bytes = response.content\n",
      "-        print(\"File read into memory!\")\n",
      "-        return file_bytes\n",
      "-    else:\n",
      "-        print(\"Failed:\", response.status_code, response.text)\n",
      "-        return None\n",
      "-\n",
      "-\n",
      "-def get_onedrive_whitepaper(file_bytes):\n",
      "-    \n",
      "-    # file_bytes is from above\n",
      "-    doc = fitz.open(stream=file_bytes, filetype=\"pdf\")\n",
      "-    text = \"\"\n",
      "-    for page_num, page in enumerate(doc):\n",
      "-        text += f\"\\n\\n--- Page {page_num + 1} ---\\n{page.get_text()}\"\n",
      "-\n",
      "-    print(\"First 1000 chars of PDF text:\", text)\n",
      "-    \n",
      "-    return text\n",
      "- \n",
      "-   \n",
      "-def prev_version( client_id, authority, file_path, version_id):\n",
      "-    access_token= access_token_key(client_id=client_id, authority=authority)\n",
      "-\n",
      "-    versions_url = f\"https://graph.microsoft.com/v1.0/me/drive/root:/{file_path}:/versions\"\n",
      "-    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
      "-    response = requests.get(versions_url, headers=headers)\n",
      "-\n",
      "-    if response.status_code == 200:\n",
      "-        versions = response.json()[\"value\"]\n",
      "-        if len(versions) >= int(version_id):\n",
      "-            # 3. Get the 2nd version (index 1)\n",
      "-            version_id = versions[1]['id']\n",
      "-            print(f\"2nd Version ID: {version_id}, Last Modified: {versions[1]['lastModifiedDateTime']}\")\n",
      "-            \n",
      "-            # 4. Fetch 2nd version's PDF bytes\n",
      "-            download_url = f\"https://graph.microsoft.com/v1.0/me/drive/root:/{file_path}:/versions/{version_id}/content\"\n",
      "-            version_response = requests.get(download_url, headers=headers)\n",
      "-            if version_response.status_code == 200:\n",
      "-                pdf_bytes = version_response.content  # This is your PDF in memory\n",
      "-                \n",
      "-                # 5. Extract text from the PDF (in memory, no save)\n",
      "-                doc = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
      "-                all_text = \"\"\n",
      "-                for page_num, page in enumerate(doc):\n",
      "-                    all_text += f\"\\n--- Page {page_num+1} ---\\n{page.get_text()}\"\n",
      "-                \n",
      "-                print(\"Extracted PDF text (first 1000 chars):\")\n",
      "-                print(all_text[:1000])\n",
      "-                return all_text\n",
      "-                # You can use `all_text` as needed (search, LLM input, etc)\n",
      "-            else:\n",
      "-                print(\"Failed to download 2nd version:\", version_response.status_code, version_response.text)\n",
      "-        else:\n",
      "-            print(\"Less than 2 versions available!\")\n",
      "-    else:\n",
      "-        print(\"Failed to fetch versions:\", response.status_code, response.text)\n",
      "-\n",
      "-### CELL 40 [code]\n",
      "-def prev_version(client_id, authority, file_path, version_number):\n",
      "-    \"\"\"\n",
      "-    Fetch and parse the N-th version of a OneDrive file via Microsoft Graph.\n",
      "-    version_number is 1-based: 1 = latest, 2 = previous, etc.\n",
      "-    Returns the extracted PDF text.\n",
      "-    \"\"\"\n",
      "-    access_token = access_token_key(client_id=client_id, authority=authority)\n",
      "-    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
      "-\n",
      "-    # 1) List versions\n",
      "-    versions_url = f\"https://graph.microsoft.com/v1.0/me/drive/root:/{file_path}:/versions\"\n",
      "-    response = requests.get(versions_url, headers=headers)\n",
      "-    if response.status_code != 200:\n",
      "-        raise RuntimeError(f\"Failed to fetch versions: {response.status_code} {response.text}\")\n",
      "-\n",
      "-    versions = response.json().get(\"value\", [])\n",
      "-    if not versions:\n",
      "-        raise RuntimeError(\"No versions found for this file.\")\n",
      "-\n",
      "-    # Sort DESC by lastModifiedDateTime so index 0 is latest, 1 is previous, etc.\n",
      "-    def _parse_dt(v):\n",
      "-        ts = v.get(\"lastModifiedDateTime\")\n",
      "-        return datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\")) if ts else datetime.min\n",
      "-    versions.sort(key=_parse_dt, reverse=True)\n",
      "-\n",
      "-    # 2) Show total + quick overview\n",
      "-    total = len(versions)\n",
      "-    print(f\"Total versions available: {total}\")\n",
      "-    for i, v in enumerate(versions, start=1):\n",
      "-        print(f\"{i}. id={v.get('id')} | lastModified={v.get('lastModifiedDateTime')} | size={v.get('size', 'NA')}\")\n",
      "-\n",
      "-    # 3) Validate requested version and pick it\n",
      "-    if not (1 <= int(version_number) <= total):\n",
      "-        raise ValueError(f\"Invalid version_number {version_number}. Only {total} versions exist.\")\n",
      "-\n",
      "-    selected = versions[int(version_number) - 1]   # 1-based â†’ 0-based\n",
      "-    internal_id = selected[\"id\"]\n",
      "-    print(f\"\\nSelected version #{version_number}: id={internal_id}, lastModified={selected.get('lastModifiedDateTime')}\")\n",
      "-\n",
      "-    # 4) Download that specific versionâ€™s content\n",
      "-    download_url = f\"https://graph.microsoft.com/v1.0/me/drive/root:/{file_path}:/versions/{internal_id}/content\"\n",
      "-    version_response = requests.get(download_url, headers=headers)\n",
      "-    if version_response.status_code != 200:\n",
      "-        raise RuntimeError(f\"Failed to download version #{version_number}: {version_response.status_code} {version_response.text}\")\n",
      "-\n",
      "-    pdf_bytes = version_response.content\n",
      "-\n",
      "-    # 5) Extract text from the PDF (in memory, no save)\n",
      "-    doc = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
      "-    all_text = \"\"\n",
      "-    for page_num, page in enumerate(doc):\n",
      "-        all_text += f\"\\n--- Page {page_num+1} ---\\n{page.get_text()}\"\n",
      "-\n",
      "-    print(\"\\nExtracted PDF text (first 1000 chars):\")\n",
      "-    # print(all_text[:1000])\n",
      "-    return all_text\n",
      "-\n",
      "-### CELL 41 [code]\n",
      "-version_number = 7\n",
      "-prev_version(client_id, authority, file_path, version_number)\n",
      "-\n",
      "-### CELL 42 [code]\n",
      "-file_bytes  = get_raw_data(client_id=client_id, authority=authority,file_path = file_path)\n",
      "-pdf_content = get_onedrive_whitepaper(file_bytes)  \n",
      "-\n",
      "-### CELL 43 [code]\n",
      "-\n",
      "-version_id = 1\n",
      "-prev_version(client_id=client_id, authority=authority,  file_path= file_path, version_id = version_id)\n",
      "-\n",
      "-### CELL 44 [markdown]\n",
      "-## Get updated file from github\n",
      "-\n",
      "-### CELL 45 [code]\n",
      "-import json \n",
      "-import pandas as pd \n",
      "-import requests\n",
      "-import requests\n",
      "-import base64\n",
      "-\n",
      "-### CELL 46 [code]\n",
      "-push_event= pd.read_json('push_events.json', )\n",
      "-latest_push_id = push_event[0].tolist()[0]\n",
      "-latest_push_id\n",
      "-\n",
      "-# --- Usage ---\n",
      "-owner = \"arunkenwal02\"\n",
      "-repo = \"code-validator\"\n",
      "-push_id = latest_push_id\n",
      "-file_path = \"loan-approval-prediction_v2.ipynb\"\n",
      "-\n",
      "-### CELL 47 [code]\n",
      "-def get_sha_pair_from_push_id(owner, repo, push_id):\n",
      "-    \"\"\"\n",
      "-    Returns (before_sha, head_sha) for the given push_id.\n",
      "-    If not found, returns (None, None).\n",
      "-    \"\"\"\n",
      "-    url = f\"https://api.github.com/repos/{owner}/{repo}/events\"\n",
      "-    resp = requests.get(url)\n",
      "-    events = resp.json()\n",
      "-    for event in events:\n",
      "-        if event[\"type\"] == \"PushEvent\" and event[\"id\"] == str(push_id):\n",
      "-            before_sha = event[\"payload\"][\"before\"]\n",
      "-            head_sha = event[\"payload\"][\"head\"]\n",
      "-            print(f\"Push ID: {push_id}\\nbefore: {before_sha}\\nhead: {head_sha}\")\n",
      "-            return before_sha, head_sha\n",
      "-    print(f\"Push ID {push_id} not found in recent events.\")\n",
      "-    return None, None\n",
      "-\n",
      "-def fetch_latest_file_for_sha(owner, repo, file_path, sha_pairs):\n",
      "-    \"\"\"\n",
      "-    For each (sha_old, sha_new) in sha_pairs, check if file_path was updated.\n",
      "-    If yes, download file from sha_new. Else, download most recently updated version.\n",
      "-    \"\"\"\n",
      "-    for i, (sha_old, sha_new) in enumerate(sha_pairs):\n",
      "-        print(f\"\\nProcessing pair {i+1}: {sha_old} â†’ {sha_new}\")\n",
      "-\n",
      "-        # 1. Compare the two SHAs\n",
      "-        compare_url = f\"https://api.github.com/repos/{owner}/{repo}/compare/{sha_old}...{sha_new}\"\n",
      "-        compare_resp = requests.get(compare_url)\n",
      "-        compare_data = compare_resp.json()\n",
      "-\n",
      "-        file_changed = False\n",
      "-        for f in compare_data.get(\"files\", []):\n",
      "-            if f[\"filename\"] == file_path:\n",
      "-                file_changed = True\n",
      "-                print(f\"File {file_path} was changed in this push.\")\n",
      "-                break\n",
      "-\n",
      "-        if file_changed:\n",
      "-            # Download updated file from sha_new\n",
      "-            content_url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{file_path}\"\n",
      "-            params = {\"ref\": sha_new}\n",
      "-            file_resp = requests.get(content_url, params=params)\n",
      "-            file_data = file_resp.json()\n",
      "-            \n",
      "-        # Check for 'content' key (base64-encoded)\n",
      "-            if \"content\" in file_data:\n",
      "-                nb_json = base64.b64decode(file_data[\"content\"]).decode(\"utf-8\")\n",
      "-                notebook_dict = json.loads(nb_json)\n",
      "-                return notebook_dict\n",
      "-            else:\n",
      "-                raise Exception(\"Notebook not found or could not fetch content. Details: \" + str(file_data))\n",
      "-\n",
      "-        else:\n",
      "-            print(f\"File {file_path} was NOT changed between {sha_old} and {sha_new}.\")\n",
      "-            # Get most recent commit where this file was updated\n",
      "-            commits_url = f\"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
      "-            params = {\"path\": file_path, \"per_page\": 1}\n",
      "-            commits_resp = requests.get(commits_url, params=params)\n",
      "-            last_update_sha = commits_resp.json()[0][\"sha\"]\n",
      "-            print(\"Most recent commit where file was changed:\", last_update_sha)\n",
      "-            # Download file at that SHA\n",
      "-            content_url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{file_path}\"\n",
      "-            params = {\"ref\": last_update_sha}\n",
      "-            file_resp = requests.get(content_url, params=params)\n",
      "-            file_data = file_resp.json()\n",
      "-            \n",
      "-            # Check for 'content' key (base64-encoded)\n",
      "-            if \"content\" in file_data:\n",
      "-                nb_json = base64.b64decode(file_data[\"content\"]).decode(\"utf-8\")\n",
      "-                notebook_dict = json.loads(nb_json)\n",
      "-                return notebook_dict\n",
      "-            else:\n",
      "-                raise Exception(\"Notebook not found or could not fetch content. Details: \" + str(file_data))\n",
      "-\n",
      "-\n",
      "-### CELL 48 [code]\n",
      "-sha_pair = get_sha_pair_from_push_id(owner, repo, push_id)\n",
      "-\n",
      "-sha_pair = [sha_pair]\n",
      "-fetch_latest_file_for_sha \n",
      "-# --- Usage example ---\n",
      "-notebook_contents = fetch_latest_file_for_sha(owner, repo, file_path, sha_pair)\n",
      "-\n",
      "-### CELL 49 [code]\n",
      "-\n",
      "-for i, cell in enumerate(notebook_contents['cells']):\n",
      "-    if cell['cell_type'] == 'code' and cell.get('outputs'):\n",
      "-        print(f\"\\nCell #{i+1}:\")\n",
      "-        print(\"Code:\")\n",
      "-        print(\"\".join(cell['source']))\n",
      "-        print(\"\\nOutputs:\")\n",
      "-        for output in cell['outputs']:\n",
      "-            # Print text output (if any)\n",
      "-            if 'text' in output:\n",
      "-                print(\"\".join(output['text']))\n",
      "-            # Print stream output\n",
      "-            if output.get('output_type') == 'stream':\n",
      "-                print(\"\".join(output.get('text', '')))\n",
      "-            # Print execution result (display_data or execute_result)\n",
      "-            if output.get('output_type') in ['execute_result', 'display_data']:\n",
      "-                data = output.get('data', {})\n",
      "-                # Print text/plain or html if present\n",
      "-                if 'text/plain' in data:\n",
      "-                    print(data['text/plain'])\n",
      "-                if 'text/html' in data:\n",
      "-                    print(data['text/html'])\n",
      "-            # Print errors if any\n",
      "-            if output.get('output_type') == 'error':\n",
      "-                print(f\"Error: {output.get('ename')} - {output.get('evalue')}\")\n",
      "-                print(\"Traceback:\")\n",
      "-                print(\"\\n\".join(output.get('traceback', [])))\n",
      "-        print(\"-\" * 40)\n",
      "-\n",
      "-### CELL 50 [code]\n",
      "-all_cells_text = \"\"\n",
      "-\n",
      "-for i, cell in enumerate(notebook_contents['cells']):\n",
      "-    if cell['cell_type'] == 'code' and cell.get('outputs'):\n",
      "-        # Add cell number and code\n",
      "-        all_cells_text += f\"\\nCell #{i+1}\\n\"\n",
      "-        all_cells_text += \"Code:\\n\"\n",
      "-        all_cells_text += \"\".join(cell['source']).strip() + \"\\n\"\n",
      "-        all_cells_text += \"Output(s):\\n\"\n",
      "-        # Add outputs\n",
      "-        for output in cell['outputs']:\n",
      "-            output_text = \"\"\n",
      "-            if output.get('output_type') == 'stream':\n",
      "-                text = output.get('text', '')\n",
      "-                if isinstance(text, list):\n",
      "-                    text = \"\".join(text)\n",
      "-                output_text += text.strip()\n",
      "-            elif output.get('output_type') in ['execute_result', 'display_data']:\n",
      "-                data = output.get('data', {})\n",
      "-                text = data.get('text/plain', '')\n",
      "-                if isinstance(text, list):\n",
      "-                    text = \"\".join(text)\n",
      "-                output_text += text.strip()\n",
      "-            # Skipping errors\n",
      "-            if output_text:\n",
      "-                all_cells_text += output_text + \"\\n\"\n",
      "-        all_cells_text += \"-\" * 30 + \"\\n\"\n",
      "-\n",
      "-# Optional: remove leading/trailing whitespace\n",
      "-all_cells_text = all_cells_text.strip()\n",
      "-\n",
      "-# Print or use as needed\n",
      "-print(all_cells_text)\n",
      "-\n",
      "-### CELL 51 [markdown]\n",
      "-# Chroma DB - Prompt Outpout for white paper\n",
      "-\n",
      "-### CELL 52 [code]\n",
      "-import os, chromadb\n",
      "-from IPython.display import Markdown, display\n",
      "-\n",
      "-\n",
      "-DATA_DIR = os.path.abspath(\"./chroma_openai1\")  # <-- match the folder that actually has your DB\n",
      "-client = chromadb.PersistentClient(path=DATA_DIR)\n",
      "-\n",
      "-### CELL 53 [code]\n",
      "-for db in client.list_collections():\n",
      "-    print(\"Name:\",db.name)\n",
      "-\n",
      "-### CELL 54 [code]\n",
      "-import os\n",
      "-import chromadb\n",
      "-\n",
      "-# Path to your actual persistent Chroma storage\n",
      "-DATA_DIR = os.path.abspath(\"./chroma_openai1\")  # adjust if needed\n",
      "-\n",
      "-# Use PersistentClient to read from persistent DB\n",
      "-client = chromadb.PersistentClient(path=DATA_DIR)\n",
      "-\n",
      "-# Get the specific collection\n",
      "-collection_name = \"whitepaper_02958f41437b5bbcf9490a38b0edb5d41a365101ce7979d2822648a320dfdc73\"\n",
      "-collection = client.get_collection(name=collection_name)\n",
      "-\n",
      "-# Retrieve all records (you can also filter with where / where_document)\n",
      "-results = collection.get(\n",
      "-    include=[\"documents\", \"metadatas\", \"embeddings\"],  # choose what you need\n",
      "-    limit=5  # remove or increase for more\n",
      "-)\n",
      "-\n",
      "-\n",
      "-\n",
      "-### CELL 55 [code]\n",
      "-# Print sample\n",
      "-for doc, meta,embeddings in zip(results[\"documents\"], results[\"metadatas\"], results['embeddings']):\n",
      "-    print(\"Document:\", doc)\n",
      "-    print(\"Metadata:\", meta)\n",
      "-    print(\"Embeddings\",embeddings)\n",
      "-    print(\"-\" * 40)\n",
      "-\n",
      "-### CELL 56 [code]\n",
      "-import os\n",
      "-import nbformat\n",
      "-from openai import OpenAI\n",
      "-from langchain.chat_models import ChatOpenAI\n",
      "-from langchain.schema import SystemMessage, HumanMessage\n",
      "-from langchain_openai import OpenAIEmbeddings\n",
      "-from dotenv import load_dotenv\n",
      "-from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
      "-import chromadb\n",
      "-from chromadb.config import Settings\n",
      "-import streamlit as st\n",
      "-import fitz\n",
      "-import tempfile\n",
      "-import hashlib\n",
      "-from concurrent.futures import ThreadPoolExecutor, as_completed\n",
      "-\n",
      "-### CELL 57 [code]\n",
      "-load_dotenv()\n",
      "-openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
      "-\n",
      "-### CELL 58 [code]\n",
      "-embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
      "-llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n",
      "-\n",
      "-### CELL 59 [code]\n",
      "-def queryFun(query, embedding_model, collection):\n",
      "-    query_embedding = embedding_model.embed_query(query)\n",
      "-    results = collection.query(query_embeddings=[query_embedding], n_results=5)\n",
      "-    l_docs = [doc for doc in results[\"documents\"][0]]\n",
      "-    return l_docs\n",
      "-\n",
      "-### CELL 60 [code]\n",
      "-queries = [\n",
      "-    \"Summary/Objective of white paper \",\n",
      "-    \"Select All features or Features name. Do not include any of the following preprocessing steps, model details, data types, or any explanations.\",\n",
      "-    \"Feature names from preprocessing steps â€” list only the features on which preprocessing was applied. excluding any training methodology, model details, bias mitigation, evaluation\",\n",
      "-    \"Name the Model selected for classification\",\n",
      "-    \"Extract the list of hyperparameters/ performance scores/ Best hyperparameters along with their  values.\",\n",
      "-    \"List of validation metrics and respective scores only and hyperparameter scores\",\n",
      "-    \"Ethical considerations\"\n",
      "-]\n",
      "-\n",
      "-### CELL 61 [code]\n",
      "-out = queryFun(queries[6], embedding_model, collection)\n",
      "-\n",
      "-### CELL 62 [code]\n",
      "-for o in out:\n",
      "-    print(o)\n",
      "-    print(\"++++++++++++++++++++++++++++++++\")\n",
      "-\n",
      "-### CELL 63 [code]\n",
      "-context_list = [\n",
      "-    \"Overview\",\n",
      "-    \"Do not include descriptions, preprocessing details, training methodology, target variable explanations, or any other text.\",\n",
      "-    \"Include only the train/test percentages and their purposes if mentioned. Do not include hyperparameter tuning, validation strategy, retraining details, evaluation metrics, or deployment strategy.\",\n",
      "-    \"Exclude summary and Laon details. Do not consider any training methodology, model details, bias mitigation, evaluation, or future work, and output in the format <Feature_Name>: <Operation_Name> preserving the exact feature names.\",\n",
      "-    \"Keep Model detail only, exclude Traning methodology, resluts, analysis, feature engineering and data handling\",\n",
      "-    \"Keep only validation/perofrmance and best hyperparameter scores. Do not include other details\"\n",
      "-    \"Keep only ethical considerations. do not include other details\"\n",
      "-]\n",
      "-\n",
      "-### CELL 64 [code]\n",
      "-def refine_extracted_elements_with_context(similar_elements, query_context):\n",
      "-    combined_elements = \"\\n\\n\".join(similar_elements)\n",
      "-    prompt = [\n",
      "-        SystemMessage(content=\"You are a product analyst.\"),\n",
      "-               \n",
      "-        HumanMessage(content=f\"\"\"\n",
      "-        The following are the top 5 similar elements retrieved from a vector database and  create a structured report in HTML format with the following three sections, using dangerouslySetInnerHTML={{ __html: reportMarkdown }}; html should not affect other elements : \n",
      "-        {combined_elements}\n",
      "-\n",
      "-        The original query context is:\n",
      "-        \"{query_context}\"\n",
      "-        - Keep only ethical considerations. do not include other details\n",
      "-        - Identify and extract only the most relevant elements or functionalities.\n",
      "-        - Do not recommend, only extract \n",
      "-        - Avoid verbose explanations; focus on clarity and precision.\n",
      "-        - Provide concise, bullet-pointed outputs or insights based on retrieved data.\n",
      "-        \"\"\")\n",
      "-\n",
      "-        # HumanMessage(content=f\"\"\"\n",
      "-        # The following are the top 5 similar elements retrieved from a vector database : \n",
      "-\n",
      "-        # {combined_elements}\n",
      "-\n",
      "-        # The original query context is:\n",
      "-        # \"{query_context}\"\n",
      "-\n",
      "-        # - Identify and extract only the most relevant elements or functionalities.\n",
      "-        # - Avoid verbose explanations; focus on clarity and precision.\n",
      "-        # - Extract details from given context keep length short in summary format \n",
      "-        # - Do not recommend, only extract\n",
      "-        # - Extract metrics score/values, model name and and hyperpapramter values if available in context \n",
      "-        # - Provide concise, bullet-pointed outputs or insights based on retrieved data.\n",
      "-        # - Format the response using IPython Markdown style for readability\n",
      "-\n",
      "-        # \"\"\")\n",
      "-    ]\n",
      "-    return llm(prompt).content.strip()\n",
      "-\n",
      "-### CELL 65 [markdown]\n",
      "-(empty source)\n",
      "-\n",
      "-### CELL 66 [code]\n",
      "-ref_output= refine_extracted_elements_with_context(out,queries[6])\n",
      "-\n",
      "-### CELL 67 [code]\n",
      "-Markdown(ref_output)\n",
      "-\n",
      "-### CELL 68 [markdown]\n",
      "-## Chroma DB for Notebook\n",
      "-\n",
      "-### CELL 69 [code]\n",
      "-# Path to your actual persistent Chroma storage\n",
      "-DATA_DIR = os.path.abspath(\"./chroma_openai1\")  # adjust if needed\n",
      "-\n",
      "-# Use PersistentClient to read from persistent DB\n",
      "-client = chromadb.PersistentClient(path=DATA_DIR)\n",
      "-\n",
      "-### CELL 70 [code]\n",
      "-for coll in client.list_collections():\n",
      "-    print(\"Coll name:\", coll.name)\n",
      "-\n",
      "-### CELL 71 [code]\n",
      "-# Get the specific collection\n",
      "-collection_name = \"notebook_f887cf79f48bf8b101631e9ebdb3ca7220bd2c6e47a6b82041ca192aa98cf16b\"\n",
      "-collection = client.get_collection(name=collection_name)\n",
      "-\n",
      "-# Retrieve all records (you can also filter with where / where_document)\n",
      "-results = collection.get(\n",
      "-    include=[\"documents\", \"metadatas\", \"embeddings\"],  # choose what you need\n",
      "-    limit=5  # remove or increase for more\n",
      "-)\n",
      "-\n",
      "-### CELL 72 [code]\n",
      "-# Print sample\n",
      "-for doc, meta,embeddings in zip(results[\"documents\"], results[\"metadatas\"], results['embeddings']):\n",
      "-    print(\"Document:\", doc)\n",
      "-    print(\"Metadata:\", meta)\n",
      "-    print(\"Embeddings\",embeddings)\n",
      "-    print(\"-\" * 40)\n",
      "-\n",
      "-### CELL 73 [code]\n",
      "-import os\n",
      "-import nbformat\n",
      "-from openai import OpenAI\n",
      "-from langchain.chat_models import ChatOpenAI\n",
      "-from langchain.schema import SystemMessage, HumanMessage\n",
      "-from langchain_openai import OpenAIEmbeddings\n",
      "-from dotenv import load_dotenv\n",
      "-from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
      "-import chromadb\n",
      "-from chromadb.config import Settings\n",
      "-import streamlit as st\n",
      "-import fitz\n",
      "-import tempfile\n",
      "-import hashlib\n",
      "-from concurrent.futures import ThreadPoolExecutor, as_completed\n",
      "-\n",
      "-### CELL 74 [code]\n",
      "-load_dotenv()\n",
      "-openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
      "-\n",
      "-### CELL 75 [code]\n",
      "-embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
      "-llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n",
      "-\n",
      "-### CELL 76 [code]\n",
      "-def queryFun(query, embedding_model, collection):\n",
      "-    query_embedding = embedding_model.embed_query(query)\n",
      "-    results = collection.query(query_embeddings=[query_embedding], n_results=5)\n",
      "-    l_docs = [doc for doc in results[\"documents\"][0]]\n",
      "-    return l_docs\n",
      "-\n",
      "-### CELL 77 [code]\n",
      "-queries = [\n",
      "-    \"Summary/Objective of white paper \",\n",
      "-    \"Select All features or Features name. Do not include any of the following preprocessing steps, model details, data types, or any explanations.\",\n",
      "-    \"Feature names from preprocessing steps â€” list only the features on which preprocessing was applied. excluding any training methodology, model details, bias mitigation, evaluation\",\n",
      "-    \"Name the Model selected for classification\",\n",
      "-    \"Extract the list of hyperparameters/ performance scores/ Best hyperparameters along with their  values.\",\n",
      "-    \"List of validation metrics and respective scores only and hyperparameter scores\",\n",
      "-    \"Ethical considerations\"\n",
      "-]\n",
      "-\n",
      "-### CELL 78 [code]\n",
      "-out = queryFun(queries[0], embedding_model, collection)\n",
      "-\n",
      "-### CELL 79 [code]\n",
      "-for o in out:\n",
      "-    print(o)\n",
      "-    print(\"++++++++++++++++++++++++++++++++\")\n",
      "-\n",
      "-### CELL 80 [code]\n",
      "-context_list = [\n",
      "-    \"Overview\",\n",
      "-    \"Do not include descriptions, preprocessing details, training methodology, target variable explanations, or any other text.\",\n",
      "-    \"Include only the train/test percentages and their purposes if mentioned. Do not include hyperparameter tuning, validation strategy, retraining details, evaluation metrics, or deployment strategy.\",\n",
      "-    \"Exclude summary and Laon details. Do not consider any training methodology, model details, bias mitigation, evaluation, or future work, and output in the format <Feature_Name>: <Operation_Name> preserving the exact feature names.\",\n",
      "-    \"Keep Model detail only, exclude Traning methodology, resluts, analysis, feature engineering and data handling\",\n",
      "-    \"Keep only validation/perofrmance and best hyperparameter scores. Do not include other details\"\n",
      "-    \"Keep only ethical considerations. do not include other details\"\n",
      "-]\n",
      "-\n",
      "-### CELL 81 [code]\n",
      "-def refine_extracted_elements_with_context(similar_elements, query_context):\n",
      "-    combined_elements = \"\\n\\n\".join(similar_elements)\n",
      "-    prompt = [\n",
      "-        SystemMessage(content=\"You are a product analyst.\"),\n",
      "-               \n",
      "-        HumanMessage(content=f\"\"\"\n",
      "-        The following are the top 5 similar elements retrieved from a vector database and  create a structured report in HTML format with the following three sections, using dangerouslySetInnerHTML={{ __html: reportMarkdown }}; html should not affect other elements : \n",
      "-        {combined_elements}\n",
      "-\n",
      "-        The original query context is:\n",
      "-        \"{query_context}\"\n",
      "-        - From the provided HTML or text, extract and main pointer summarize only the Overview section, excluding all other sections or details, keeping the summary concise.\n",
      "-        - Identify and extract only the most relevant elements or functionalities.\n",
      "-        - Do not recommend, only extract \n",
      "-        - Avoid verbose explanations; focus on clarity and precision.\n",
      "-        - Provide concise, bullet-pointed outputs or insights based on retrieved data.\n",
      "-        \"\"\")\n",
      "-\n",
      "-        # HumanMessage(content=f\"\"\"\n",
      "-        # The following are the top 5 similar elements retrieved from a vector database : \n",
      "-\n",
      "-        # {combined_elements}\n",
      "-\n",
      "-        # The original query context is:\n",
      "-        # \"{query_context}\"\n",
      "-\n",
      "-        # - Identify and extract only the most relevant elements or functionalities.\n",
      "-        # - Avoid verbose explanations; focus on clarity and precision.\n",
      "-        # - Extract details from given context keep length short in summary format \n",
      "-        # - Do not recommend, only extract\n",
      "-        # - Extract metrics score/values, model name and and hyperpapramter values if available in context \n",
      "-        # - Provide concise, bullet-pointed outputs or insights based on retrieved data.\n",
      "-        # - Format the response using IPython Markdown style for readability\n",
      "-\n",
      "-        # \"\"\")\n",
      "-    ]\n",
      "-    return llm(prompt).content.strip()\n",
      "-\n",
      "-### CELL 82 [code]\n",
      "-ref_output= refine_extracted_elements_with_context(out,queries[0])\n",
      "-\n",
      "-### CELL 83 [code]\n",
      "-Markdown(ref_output)\n",
      "-\n",
      "-### CELL 84 [code]\n",
      "-(empty source)\n",
      "-\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import difflib\n",
    "\n",
    "def fetch_text(url: str) -> str:\n",
    "    \"\"\"Download file text from a raw GitHub URL.\"\"\"\n",
    "    r = requests.get(url, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    # Raw endpoints serve text/bytes directly\n",
    "    return r.text\n",
    "\n",
    "def normalize_for_diff(text: str, url_hint: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Return a list of lines ready for diff.\n",
    "    - If it's a .ipynb (by URL or detectable JSON with 'cells'), compare cell sources.\n",
    "    - Otherwise, return plain lines.\n",
    "    \"\"\"\n",
    "    # Quick path for non-notebooks\n",
    "    if not url_hint.endswith(\".ipynb\"):\n",
    "        return text.splitlines()\n",
    "\n",
    "    # Try to parse as a notebook; if parsing fails, fallback to plain text.\n",
    "    try:\n",
    "        nb = json.loads(text)\n",
    "        if not isinstance(nb, dict) or \"cells\" not in nb:\n",
    "            return text.splitlines()\n",
    "        lines = []\n",
    "        for i, cell in enumerate(nb.get(\"cells\", []), start=1):\n",
    "            ctype = cell.get(\"cell_type\", \"unknown\")\n",
    "            src = \"\".join(cell.get(\"source\", []))\n",
    "            lines.append(f\"### CELL {i} [{ctype}]\")\n",
    "            lines.extend(src.splitlines() if src else [\"(empty source)\"])\n",
    "            lines.append(\"\")  # separator\n",
    "        return lines\n",
    "    except Exception:\n",
    "        return text.splitlines()\n",
    "\n",
    "def diff_urls(url_old: str, url_new: str) -> str:\n",
    "    old_text = fetch_text(url_old)\n",
    "    new_text = fetch_text(url_new)\n",
    "\n",
    "    old_lines = normalize_for_diff(old_text, url_old)\n",
    "    new_lines = normalize_for_diff(new_text, url_new)\n",
    "\n",
    "    diff = difflib.unified_diff(\n",
    "        old_lines, new_lines,\n",
    "        fromfile=\"OLD_VERSION\",\n",
    "        tofile=\"NEW_VERSION\",\n",
    "        lineterm=\"\"\n",
    "    )\n",
    "    out = \"\\n\".join(diff)\n",
    "    return out if out else \"No differences found.\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Replace with your two RAW URLs (must be raw.githubusercontent.com or ?raw=1)\n",
    "    url_v1 = \"https://raw.githubusercontent.com/arunkenwal02/code-validator/249882444dc790db55b48469bb34c891418934bd/main.ipynb\"\n",
    "    url_v2 = \"https://raw.githubusercontent.com/arunkenwal02/code-validator/c536e703bf0e78761d6374044ef7d9c2bb482131/main.ipynb\"\n",
    "\n",
    "    print(diff_urls(url_v1, url_v2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784c4e81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5cd50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for db in client.list_collections():\n",
    "    print(\"Name:\",db.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bd75cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
