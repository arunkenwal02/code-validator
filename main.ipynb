{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ac416dc",
   "metadata": {},
   "source": [
    "## Check Docs Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e18532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from IPython.display import Markdown, display\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "import nbformat\n",
    "import streamlit as st\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913e4307",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. Load White Paper  (PDF)  \n",
    "2. Vectors Embeddings - Text and tables; later include images\n",
    "3. Chroma db\n",
    "4. Retrival - (Accuracy)\n",
    "5. Generation - \n",
    "    Validation-\n",
    "        Data sources - List all sources and metadata\n",
    "        Features - detect any change in features\n",
    "        Changes in Transformation steps\n",
    "        Model Details \n",
    "        Hyperparameter\n",
    "        List of Validation Metrics and resepctive scores\n",
    "        Compare Validation scores of white paper with model's validation scores\n",
    "        Brief of comparision of scores\n",
    "\n",
    "        List and Track of critical metrics - these should not be lower than mentioned (in white paper)\n",
    "\n",
    "6. Respective scores for tracked metrics (confidence on generation)\n",
    "7. If required update prompt and go back to step 4 and reiterate step 4 and 5. reason (geneation have \n",
    "   heiger confidence )\n",
    "8. Outout should be in structured format (This will be input for summary block with affitional 2 inputs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1358c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\",  \n",
    "                 temperature=0,\n",
    "                 openai_api_key= openai_api_key)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f30bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_notebook(file_path):\n",
    "    \"\"\"Read .ipynb notebook and extract content.\"\"\"\n",
    "    nb = nbformat.read(file_path, as_version=4)\n",
    "    content = []\n",
    "    for cell in nb.cells:\n",
    "        if cell.cell_type == 'markdown':\n",
    "            content.append(\"## Markdown Cell:\\n\" + cell.source)\n",
    "        elif cell.cell_type == 'code':\n",
    "            content.append(\"## Code Cell:\\n```python\\n\" + cell.source + \"\\n```\")\n",
    "    return \"\\n\\n\".join(content)\n",
    "\n",
    "\n",
    "def read_file(file_path):\n",
    "    \"\"\"Reads the content of a file.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def extract_functionalities_from_code(notebook_content):\n",
    "\n",
    "    \"\"\"Uses LLM to extract functionalities from Python code.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert Python code reviewer. Here is a Jupyter notebook:\n",
    "    {notebook_content}\n",
    "\n",
    "    The following is a Jupyter notebook content (code and markdown). \n",
    "    Please extract the following:\n",
    "    Analyze the notebook and answer:\n",
    "\n",
    "    1. List of features used in the model.\n",
    "    2. Name/type of ML model used, only name of model\n",
    "    3. Accuracy metrics (e.g., accuracy, F1, precision, recall, AUC, etc.), only metrics name. \n",
    "    4. What is the purpose of this notebook?\n",
    "    5. What are the main operations and their results?\n",
    "    6. Are there any errors or anomalies in outputs?\n",
    "    7. What conclusions can be drawn from the outputs?\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke([SystemMessage(content=\"You are a helpful assistant.\"), HumanMessage(content=prompt)])\n",
    "\n",
    "    return response.content.strip()\n",
    "\n",
    "\n",
    "def extract_functionalities_from_whitepaper(whitepaper_text):\n",
    "    \"\"\"Uses LLM to extract functionalities from whitepaper.\"\"\"\n",
    "    prompt = [\n",
    "        SystemMessage(content=\"You are a product analyst.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Here is the whitepaper or product requirement document:\n",
    "\n",
    "        {whitepaper_text}\n",
    "\n",
    "        List all functionalities or features the whitepaper mentions. Use bullet points.\n",
    "        \"\"\")\n",
    "            ]\n",
    "    return llm(prompt).content.strip()\n",
    "\n",
    "\n",
    "def compare_functionalities(whitepaper_funcs, code_funcs):\n",
    "    \"\"\"Compares two sets of functionalities using the LLM.\"\"\"\n",
    "    prompt = [\n",
    "        SystemMessage(content=\"You are a software QA expert.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Whitepaper Functionalities:\n",
    "        {whitepaper_funcs}\n",
    "\n",
    "        Code Functionalities:\n",
    "        {code_funcs}\n",
    "        Extract validation metrics from code funcs eg, precision, recall and other validation are in output cell.\n",
    "        Compare the two lists and identify which functionalities from the whitepape, if functionality is implemented in code but not available in white paper, print: white paper is not updated please update the document. and show details of each section \n",
    "        listmissing sections like feature and if model varies according to white paper and same for validation metrics.\n",
    "        compare validation scores : Compare scores of code function with white paper.\n",
    "        Also compare critical validation metrics: make sure critical metrics of code should be grater then white paper critical metrics\n",
    "        if thereis no changhe in metrics of docuemt and code_funcs: keep output 'white paper is updated please proceed to next steps. no other information is required'  \n",
    "        \n",
    "        \"\"\")\n",
    "            ]\n",
    "    return llm(prompt).content.strip()\n",
    "\n",
    "\n",
    "def read_notebook_with_outputs(file_path):\n",
    "    \"\"\"Read .ipynb notebook and include both code and output.\"\"\"\n",
    "    nb = nbformat.read(file_path, as_version=4)\n",
    "    cells_content = []\n",
    "\n",
    "    for cell in nb.cells:\n",
    "        if cell.cell_type == 'markdown':\n",
    "            cells_content.append(f\"## Markdown Cell:\\n{cell.source}\")\n",
    "        elif cell.cell_type == 'code':\n",
    "            code = f\"## Code Cell:\\n```python\\n{cell.source}\\n```\"\n",
    "            outputs = []\n",
    "\n",
    "            for output in cell.get(\"outputs\", []):\n",
    "                if output.output_type == \"stream\":\n",
    "                    outputs.append(f\"Output (stream):\\n{output.text}\")\n",
    "                elif output.output_type == \"execute_result\":\n",
    "                    # Display the result of the cell (e.g., print(2+2))\n",
    "                    result = output.get(\"data\", {}).get(\"text/plain\", \"\")\n",
    "                    outputs.append(f\"Output (execute_result):\\n{result}\")\n",
    "                elif output.output_type == \"error\":\n",
    "                    outputs.append(\"Error:\\n\" + \"\\n\".join(output.get(\"traceback\", [])))\n",
    "\n",
    "            full_output = \"\\n\".join(outputs)\n",
    "            if full_output:\n",
    "                code += f\"\\n\\n### Output:\\n```\\n{full_output}\\n```\"\n",
    "            cells_content.append(code)\n",
    "\n",
    "    return \"\\n\\n\".join(cells_content)\n",
    "\n",
    "def read_notebook(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return nbformat.read(f, as_version=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcfe8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    st.set_page_config(page_title=\"Functionality Coverage Checker\", layout=\"wide\")\n",
    "    \n",
    "    st.title(\"🧠 AI Feature Mapping Validator\")\n",
    "    st.subheader(\"Compare functionalities between a Whitepaper and its Codebase\")\n",
    "\n",
    "    uploaded_whitepaper = st.file_uploader(\"📄 Upload Whitepaper File\", type=[\"txt\", \"md\", \"pdf\"])\n",
    "    uploaded_code = st.file_uploader(\"💻 Upload Code File\", type=[\"py\", \"txt\", \"ipynb\"])\n",
    "\n",
    "    if uploaded_whitepaper and uploaded_code:\n",
    "        if st.button(\"Click to Process Files\"):\n",
    "            # Read whitepaper content\n",
    "            whitepaper = uploaded_whitepaper.read().decode(\"utf-8\")\n",
    "\n",
    "            # Handle .ipynb or other code files\n",
    "            if uploaded_code.name.endswith(\".ipynb\"):\n",
    "                # Write the raw content to a temp file\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix=\".ipynb\", mode='wb') as tmp_file:\n",
    "                    tmp_file.write(uploaded_code.read())\n",
    "                    temp_file_path = tmp_file.name\n",
    "\n",
    "                # notebook_contents = read_notebook(temp_file_path)\n",
    "                notebook_contents = read_notebook_with_outputs(temp_file_path)\n",
    "                code_funcs = extract_functionalities_from_code(notebook_contents)\n",
    "            else:\n",
    "                code = uploaded_code.read().decode(\"utf-8\")\n",
    "                code_funcs = extract_functionalities_from_code(code)\n",
    "\n",
    "            whitepaper_funcs = extract_functionalities_from_whitepaper(whitepaper)\n",
    "\n",
    "            st.markdown(\"### ⚖️ Comparing Functionalities\")\n",
    "            missing_funcs = compare_functionalities(whitepaper_funcs, code_funcs)\n",
    "            st.markdown(missing_funcs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c540a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29f7a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f1b071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c25394f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a51519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import nbformat\n",
    "# from openai import OpenAI\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.schema import SystemMessage, HumanMessage\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.schema import SystemMessage, HumanMessage\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# from dotenv import load_dotenv\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# import chromadb\n",
    "# from chromadb.config import Settings\n",
    "\n",
    "# import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3769bc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries = [\n",
    "#             \"Summary/Objective of white paper \",\n",
    "#             \"Features mentioned\",\n",
    "#             \"Preprocessing steps and data transformation steps\",\n",
    "#             \"Model selected for classification\",\n",
    "#             \"Training and resting methodology\",\n",
    "#             \"List of Hyper parameters and respective values\",\n",
    "#             \"What are list of validation scores and the performance scores?\",\n",
    "#             \"Ethical considerations\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b706715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryFun(query, embedding_model,collection):\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    l_docs = []\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=5)\n",
    "    for doc in results[\"documents\"][0]:\n",
    "        l_docs.append(doc)\n",
    "        # print(\"🔎 Match:\", l_docs.append(doc))\n",
    "    return l_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81786eca",
   "metadata": {},
   "source": [
    "##  Check Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4fc45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb import PersistentClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723ab5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./chroma_openai1\"\n",
    "\n",
    "# Step 1: Load the persistent client\n",
    "chroma_client = PersistentClient(path=path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935ed8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "collections = chroma_client.list_collections()\n",
    "for col in collections:\n",
    "    print(col.name)\n",
    "    # print(col.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e06b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"notebook_f887cf79f48bf8b101631e9ebdb3ca7220bd2c6e47a6b82041ca192aa98cf16b\"\n",
    "# Step 2: Access the existing collection\n",
    "collection = chroma_client.get_collection(name=collection_name)\n",
    "data = collection.get()\n",
    "\n",
    "# Optional: View details\n",
    "print(\"IDs:\", data['ids'])\n",
    "print(\"Documents:\", data['documents'][:2])  # print only first 2 docs\n",
    "print(\"Metadata:\", data.get('metadatas'))  # only if metadata was stored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418da824",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7484104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: View details\n",
    "print(\"IDs:\", data['ids'])\n",
    "print(\"Documents:\", data['documents'][:2])  # print only first 2 docs\n",
    "print(\"Metadata:\", data.get('metadatas'))  # only if metadata was stored\n",
    "print(\"Embeddings:\", data['embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211070c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6279493b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253ac37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.get_collection(\"whitepaper_02958f41437b5bbcf9490a38b0edb5d41a365101ce7979d2822648a320dfdc73\")\n",
    "\n",
    "results = collections[6].get(\n",
    "    ids=[\"id1\", \"id2\"],    # optional\n",
    "    where={\"type\": \"pdf\"}, # optional\n",
    "    include=[\"documents\", \"metadatas\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1181a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d63925d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Embedding stored or not \n",
    "import os\n",
    "import nbformat\n",
    "from openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "import streamlit as st\n",
    "import fitz\n",
    "import tempfile\n",
    "import hashlib\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b40a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_hash(uploaded_file):\n",
    "    uploaded_file.seek(0)\n",
    "    hash_val = hashlib.sha256(uploaded_file.read()).hexdigest()\n",
    "    uploaded_file.seek(0)\n",
    "    return hash_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85886d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_pdf(uploaded_file):\n",
    "    doc = fitz.open(stream=uploaded_file.read(), filetype=\"pdf\")\n",
    "    extracted_text = \"\"\n",
    "    for page_num, page in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "        extracted_text += f\"\\n\\n--- Page {page_num + 1} ---\\n{text}\"\n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537bf1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collection_exists(collection_name):\n",
    "    try:\n",
    "        chroma_client.get_collection(collection_name)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60a085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=750,     # faster, smaller chunk\n",
    "        chunk_overlap=100   # reduced overlap\n",
    "    )\n",
    "    return text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b051a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_or_create_embeddings(uploaded_file, text, _embedding_model, collection_name):\n",
    "    chunks = create_chunks(text)\n",
    "\n",
    "    if collection_exists(collection_name):\n",
    "        collection = chroma_client.get_collection(collection_name)\n",
    "    else:\n",
    "        embeddings = _embedding_model.embed_documents(chunks)\n",
    "        print(embeddings)\n",
    "        collection = store_in_chromaDB(chunks, embeddings, collection_name)\n",
    "\n",
    "    return collection, chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad320aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import fitz\n",
    "\n",
    "def get_file_hash(file_obj):\n",
    "    file_obj.seek(0)\n",
    "    hash_val = hashlib.sha256(file_obj.read()).hexdigest()\n",
    "    file_obj.seek(0)\n",
    "    return hash_val\n",
    "\n",
    "def extract_from_pdf(file_obj):\n",
    "    file_obj.seek(0)\n",
    "    doc = fitz.open(stream=file_obj.read(), filetype=\"pdf\")\n",
    "    extracted_text = \"\"\n",
    "    for page_num, page in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "        extracted_text += f\"\\n\\n--- Page {page_num + 1} ---\\n{text}\"\n",
    "    return extracted_text\n",
    "\n",
    "file_path = \"Load Prediction Whitepaper.pdf\"\n",
    "\n",
    "with open(file_path, \"rb\") as f:\n",
    "    # 1. Compute hash\n",
    "    file_hash = get_file_hash(f)\n",
    "    # 2. Extract text\n",
    "    text = extract_from_pdf(f)\n",
    "    # 3. If needed, reset pointer for further use (not always needed)\n",
    "    f.seek(0)\n",
    "    # 4. Pass to embedding function (if needed)\n",
    "    collection, chunks = get_or_create_embeddings(\n",
    "        uploaded_file=f,              # If function needs file object\n",
    "        text=text,                    # If function needs text\n",
    "        _embedding_model=embedding_model,\n",
    "        collection_name=f\"whitepaper_{file_hash}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec64fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb95ef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_of_events():\n",
    "    # with open(\"push_events.json\", \"r\") as file:\n",
    "    #     data = json.load(file)\n",
    "    # owner = data[2]\n",
    "    # repo_name = data[3]\n",
    "    api_url = f\"https://api.github.com/repos/arunkenwal02/code-validator/events\"\n",
    "    response = requests.get(api_url)\n",
    "    events = response.json()\n",
    "    push_ids = [id['id'] for id in events]\n",
    "    print(push_ids)\n",
    "    data = ['52949273211','52821120274']\n",
    "    push_events = [e for e in events if e['type'] == 'PushEvent']\n",
    "\n",
    "    ids = [e['id'] for e in push_events]\n",
    "    try:\n",
    "        idx1 = ids.index(data[0])\n",
    "        idx2 = ids.index(data[1])\n",
    "    except ValueError:\n",
    "        return {\"error\": \"One or both push IDs not found in recent events.\"}\n",
    "\n",
    "    start = min(idx1, idx2)\n",
    "    end = max(idx1, idx2)\n",
    "\n",
    "    history_between = push_events[start:end+1]  \n",
    "    grouped_push_events = []\n",
    "    commits_list = []\n",
    "    for event in history_between:\n",
    "        push_id = event['id']\n",
    "        created_at = event['created_at']\n",
    "        repo = event['repo']['name']\n",
    "        commits_list = []\n",
    "\n",
    "        for commit in event[\"payload\"][\"commits\"]:\n",
    "            sha = commit['sha']\n",
    "            author = commit['author']['name']\n",
    "            message = commit['message']\n",
    "\n",
    "            commit_detail_url = f\"https://api.github.com/repos/arunkenwal02/code-validator/commits/{sha}\"\n",
    "            commit_detail_response = requests.get(commit_detail_url)\n",
    "\n",
    "            if commit_detail_response.status_code != 200:\n",
    "                diff = \"❌ Failed to fetch diff\"\n",
    "            else:\n",
    "                commit_detail = commit_detail_response.json()\n",
    "                diffs = []\n",
    "                for file in commit_detail.get('files', []):\n",
    "                    patch = file.get('patch')\n",
    "                    if patch:\n",
    "                        diffs.append(f\"File: {file['filename']}\\n{patch}\")\n",
    "                diff = \"\\n\\n\".join(diffs) if diffs else \"No diff available\"\n",
    "\n",
    "            commits_list.append({\n",
    "                \"sha\": sha,\n",
    "                \"author\": author,\n",
    "                \"commit_message\": message,\n",
    "                \"code_diff\": diff\n",
    "            })\n",
    "\n",
    "        grouped_push_events.append({\n",
    "            \"push_id\": push_id,\n",
    "            \"repo\": repo,\n",
    "            \"created_at\": created_at,\n",
    "            \"commits\": commits_list\n",
    "        })\n",
    "    return grouped_push_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdfe6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = create_summary_of_events()\n",
    "test\n",
    "push id , commit summary, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a64e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499acb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc3313b",
   "metadata": {},
   "source": [
    "## One drive file access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacde4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import msal\n",
    "import requests\n",
    "import time\n",
    "import fitz\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv() \n",
    "Permission_ID =\"6a94cb3a-9869-4b54-ae0b-f4f523df2614\"\n",
    "client_id = Permission_ID\n",
    "authority = \"https://login.microsoftonline.com/consumers\"\n",
    "scopes = [\"Files.Read\"]\n",
    "source_folder = \"Documents/GitHub/code-validator/\"\n",
    "file_name = \"Load Prediction Whitepaper.pdf\"\n",
    "version_id = int(7)\n",
    "file_path = source_folder+file_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b81431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract updated version \n",
    "\n",
    "def access_token_key(client_id, authority):\n",
    "    scopes = [\"Files.Read\"]\n",
    "    app = msal.PublicClientApplication(client_id=client_id, authority=authority)\n",
    "    result = None\n",
    "\n",
    "    accounts = app.get_accounts()\n",
    "    if accounts:\n",
    "        result = app.acquire_token_silent(scopes, account=accounts[0])\n",
    "    if not result:\n",
    "        result = app.acquire_token_interactive(scopes=scopes)\n",
    "    if not result or \"access_token\" not in result:\n",
    "        print(\"MSAL Error:\", result)\n",
    "    access_token = result[\"access_token\"]\n",
    "\n",
    "    return access_token\n",
    "\n",
    "def get_raw_data(client_id, authority ,file_path ):\n",
    "    access_token= access_token_key(client_id=client_id, authority=authority)\n",
    "    url = f\"https://graph.microsoft.com/v1.0/me/drive/root:/{file_path}:/content\"\n",
    "    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "    time.sleep(2)\n",
    "    response = requests.get(url, headers=headers)\n",
    "    print(f\"Response code: {response.status_code}\")\n",
    "    if response.status_code == 200:\n",
    "        file_bytes = response.content\n",
    "        print(\"File read into memory!\")\n",
    "        return file_bytes\n",
    "    else:\n",
    "        print(\"Failed:\", response.status_code, response.text)\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_onedrive_whitepaper(file_bytes):\n",
    "    \n",
    "    # file_bytes is from above\n",
    "    doc = fitz.open(stream=file_bytes, filetype=\"pdf\")\n",
    "    text = \"\"\n",
    "    for page_num, page in enumerate(doc):\n",
    "        text += f\"\\n\\n--- Page {page_num + 1} ---\\n{page.get_text()}\"\n",
    "\n",
    "    print(\"First 1000 chars of PDF text:\", text)\n",
    "    \n",
    "    return text\n",
    " \n",
    "   \n",
    "def prev_version( client_id, authority, file_path, version_id):\n",
    "    access_token= access_token_key(client_id=client_id, authority=authority)\n",
    "\n",
    "    versions_url = f\"https://graph.microsoft.com/v1.0/me/drive/root:/{file_path}:/versions\"\n",
    "    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "    response = requests.get(versions_url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        versions = response.json()[\"value\"]\n",
    "        if len(versions) >= int(version_id):\n",
    "            # 3. Get the 2nd version (index 1)\n",
    "            version_id = versions[1]['id']\n",
    "            print(f\"2nd Version ID: {version_id}, Last Modified: {versions[1]['lastModifiedDateTime']}\")\n",
    "            \n",
    "            # 4. Fetch 2nd version's PDF bytes\n",
    "            download_url = f\"https://graph.microsoft.com/v1.0/me/drive/root:/{file_path}:/versions/{version_id}/content\"\n",
    "            version_response = requests.get(download_url, headers=headers)\n",
    "            if version_response.status_code == 200:\n",
    "                pdf_bytes = version_response.content  # This is your PDF in memory\n",
    "                \n",
    "                # 5. Extract text from the PDF (in memory, no save)\n",
    "                doc = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
    "                all_text = \"\"\n",
    "                for page_num, page in enumerate(doc):\n",
    "                    all_text += f\"\\n--- Page {page_num+1} ---\\n{page.get_text()}\"\n",
    "                \n",
    "                print(\"Extracted PDF text (first 1000 chars):\")\n",
    "                print(all_text[:1000])\n",
    "                return all_text\n",
    "                # You can use `all_text` as needed (search, LLM input, etc)\n",
    "            else:\n",
    "                print(\"Failed to download 2nd version:\", version_response.status_code, version_response.text)\n",
    "        else:\n",
    "            print(\"Less than 2 versions available!\")\n",
    "    else:\n",
    "        print(\"Failed to fetch versions:\", response.status_code, response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bde66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prev_version(client_id, authority, file_path, version_number):\n",
    "    \"\"\"\n",
    "    Fetch and parse the N-th version of a OneDrive file via Microsoft Graph.\n",
    "    version_number is 1-based: 1 = latest, 2 = previous, etc.\n",
    "    Returns the extracted PDF text.\n",
    "    \"\"\"\n",
    "    access_token = access_token_key(client_id=client_id, authority=authority)\n",
    "    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "\n",
    "    # 1) List versions\n",
    "    versions_url = f\"https://graph.microsoft.com/v1.0/me/drive/root:/{file_path}:/versions\"\n",
    "    response = requests.get(versions_url, headers=headers)\n",
    "    if response.status_code != 200:\n",
    "        raise RuntimeError(f\"Failed to fetch versions: {response.status_code} {response.text}\")\n",
    "\n",
    "    versions = response.json().get(\"value\", [])\n",
    "    if not versions:\n",
    "        raise RuntimeError(\"No versions found for this file.\")\n",
    "\n",
    "    # Sort DESC by lastModifiedDateTime so index 0 is latest, 1 is previous, etc.\n",
    "    def _parse_dt(v):\n",
    "        ts = v.get(\"lastModifiedDateTime\")\n",
    "        return datetime.fromisoformat(ts.replace(\"Z\", \"+00:00\")) if ts else datetime.min\n",
    "    versions.sort(key=_parse_dt, reverse=True)\n",
    "\n",
    "    # 2) Show total + quick overview\n",
    "    total = len(versions)\n",
    "    print(f\"Total versions available: {total}\")\n",
    "    for i, v in enumerate(versions, start=1):\n",
    "        print(f\"{i}. id={v.get('id')} | lastModified={v.get('lastModifiedDateTime')} | size={v.get('size', 'NA')}\")\n",
    "\n",
    "    # 3) Validate requested version and pick it\n",
    "    if not (1 <= int(version_number) <= total):\n",
    "        raise ValueError(f\"Invalid version_number {version_number}. Only {total} versions exist.\")\n",
    "\n",
    "    selected = versions[int(version_number) - 1]   # 1-based → 0-based\n",
    "    internal_id = selected[\"id\"]\n",
    "    print(f\"\\nSelected version #{version_number}: id={internal_id}, lastModified={selected.get('lastModifiedDateTime')}\")\n",
    "\n",
    "    # 4) Download that specific version’s content\n",
    "    download_url = f\"https://graph.microsoft.com/v1.0/me/drive/root:/{file_path}:/versions/{internal_id}/content\"\n",
    "    version_response = requests.get(download_url, headers=headers)\n",
    "    if version_response.status_code != 200:\n",
    "        raise RuntimeError(f\"Failed to download version #{version_number}: {version_response.status_code} {version_response.text}\")\n",
    "\n",
    "    pdf_bytes = version_response.content\n",
    "\n",
    "    # 5) Extract text from the PDF (in memory, no save)\n",
    "    doc = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
    "    all_text = \"\"\n",
    "    for page_num, page in enumerate(doc):\n",
    "        all_text += f\"\\n--- Page {page_num+1} ---\\n{page.get_text()}\"\n",
    "\n",
    "    print(\"\\nExtracted PDF text (first 1000 chars):\")\n",
    "    # print(all_text[:1000])\n",
    "    return all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae9c5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "version_number = 7\n",
    "prev_version(client_id, authority, file_path, version_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4bc5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_bytes  = get_raw_data(client_id=client_id, authority=authority,file_path = file_path)\n",
    "pdf_content = get_onedrive_whitepaper(file_bytes)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "version_id = 1\n",
    "prev_version(client_id=client_id, authority=authority,  file_path= file_path, version_id = version_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87ed6a9",
   "metadata": {},
   "source": [
    "## Get updated file from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e5ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd \n",
    "import requests\n",
    "import requests\n",
    "import base64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb8bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "push_event= pd.read_json('push_events.json', )\n",
    "latest_push_id = push_event[0].tolist()[0]\n",
    "latest_push_id\n",
    "\n",
    "# --- Usage ---\n",
    "owner = \"arunkenwal02\"\n",
    "repo = \"code-validator\"\n",
    "push_id = latest_push_id\n",
    "file_path = \"loan-approval-prediction_v2.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a0dcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sha_pair_from_push_id(owner, repo, push_id):\n",
    "    \"\"\"\n",
    "    Returns (before_sha, head_sha) for the given push_id.\n",
    "    If not found, returns (None, None).\n",
    "    \"\"\"\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}/events\"\n",
    "    resp = requests.get(url)\n",
    "    events = resp.json()\n",
    "    for event in events:\n",
    "        if event[\"type\"] == \"PushEvent\" and event[\"id\"] == str(push_id):\n",
    "            before_sha = event[\"payload\"][\"before\"]\n",
    "            head_sha = event[\"payload\"][\"head\"]\n",
    "            print(f\"Push ID: {push_id}\\nbefore: {before_sha}\\nhead: {head_sha}\")\n",
    "            return before_sha, head_sha\n",
    "    print(f\"Push ID {push_id} not found in recent events.\")\n",
    "    return None, None\n",
    "\n",
    "def fetch_latest_file_for_sha(owner, repo, file_path, sha_pairs):\n",
    "    \"\"\"\n",
    "    For each (sha_old, sha_new) in sha_pairs, check if file_path was updated.\n",
    "    If yes, download file from sha_new. Else, download most recently updated version.\n",
    "    \"\"\"\n",
    "    for i, (sha_old, sha_new) in enumerate(sha_pairs):\n",
    "        print(f\"\\nProcessing pair {i+1}: {sha_old} → {sha_new}\")\n",
    "\n",
    "        # 1. Compare the two SHAs\n",
    "        compare_url = f\"https://api.github.com/repos/{owner}/{repo}/compare/{sha_old}...{sha_new}\"\n",
    "        compare_resp = requests.get(compare_url)\n",
    "        compare_data = compare_resp.json()\n",
    "\n",
    "        file_changed = False\n",
    "        for f in compare_data.get(\"files\", []):\n",
    "            if f[\"filename\"] == file_path:\n",
    "                file_changed = True\n",
    "                print(f\"File {file_path} was changed in this push.\")\n",
    "                break\n",
    "\n",
    "        if file_changed:\n",
    "            # Download updated file from sha_new\n",
    "            content_url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{file_path}\"\n",
    "            params = {\"ref\": sha_new}\n",
    "            file_resp = requests.get(content_url, params=params)\n",
    "            file_data = file_resp.json()\n",
    "            \n",
    "        # Check for 'content' key (base64-encoded)\n",
    "            if \"content\" in file_data:\n",
    "                nb_json = base64.b64decode(file_data[\"content\"]).decode(\"utf-8\")\n",
    "                notebook_dict = json.loads(nb_json)\n",
    "                return notebook_dict\n",
    "            else:\n",
    "                raise Exception(\"Notebook not found or could not fetch content. Details: \" + str(file_data))\n",
    "\n",
    "        else:\n",
    "            print(f\"File {file_path} was NOT changed between {sha_old} and {sha_new}.\")\n",
    "            # Get most recent commit where this file was updated\n",
    "            commits_url = f\"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "            params = {\"path\": file_path, \"per_page\": 1}\n",
    "            commits_resp = requests.get(commits_url, params=params)\n",
    "            last_update_sha = commits_resp.json()[0][\"sha\"]\n",
    "            print(\"Most recent commit where file was changed:\", last_update_sha)\n",
    "            # Download file at that SHA\n",
    "            content_url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{file_path}\"\n",
    "            params = {\"ref\": last_update_sha}\n",
    "            file_resp = requests.get(content_url, params=params)\n",
    "            file_data = file_resp.json()\n",
    "            \n",
    "            # Check for 'content' key (base64-encoded)\n",
    "            if \"content\" in file_data:\n",
    "                nb_json = base64.b64decode(file_data[\"content\"]).decode(\"utf-8\")\n",
    "                notebook_dict = json.loads(nb_json)\n",
    "                return notebook_dict\n",
    "            else:\n",
    "                raise Exception(\"Notebook not found or could not fetch content. Details: \" + str(file_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5518dac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sha_pair = get_sha_pair_from_push_id(owner, repo, push_id)\n",
    "\n",
    "sha_pair = [sha_pair]\n",
    "fetch_latest_file_for_sha \n",
    "# --- Usage example ---\n",
    "notebook_contents = fetch_latest_file_for_sha(owner, repo, file_path, sha_pair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befb97bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, cell in enumerate(notebook_contents['cells']):\n",
    "    if cell['cell_type'] == 'code' and cell.get('outputs'):\n",
    "        print(f\"\\nCell #{i+1}:\")\n",
    "        print(\"Code:\")\n",
    "        print(\"\".join(cell['source']))\n",
    "        print(\"\\nOutputs:\")\n",
    "        for output in cell['outputs']:\n",
    "            # Print text output (if any)\n",
    "            if 'text' in output:\n",
    "                print(\"\".join(output['text']))\n",
    "            # Print stream output\n",
    "            if output.get('output_type') == 'stream':\n",
    "                print(\"\".join(output.get('text', '')))\n",
    "            # Print execution result (display_data or execute_result)\n",
    "            if output.get('output_type') in ['execute_result', 'display_data']:\n",
    "                data = output.get('data', {})\n",
    "                # Print text/plain or html if present\n",
    "                if 'text/plain' in data:\n",
    "                    print(data['text/plain'])\n",
    "                if 'text/html' in data:\n",
    "                    print(data['text/html'])\n",
    "            # Print errors if any\n",
    "            if output.get('output_type') == 'error':\n",
    "                print(f\"Error: {output.get('ename')} - {output.get('evalue')}\")\n",
    "                print(\"Traceback:\")\n",
    "                print(\"\\n\".join(output.get('traceback', [])))\n",
    "        print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ef8bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cells_text = \"\"\n",
    "\n",
    "for i, cell in enumerate(notebook_contents['cells']):\n",
    "    if cell['cell_type'] == 'code' and cell.get('outputs'):\n",
    "        # Add cell number and code\n",
    "        all_cells_text += f\"\\nCell #{i+1}\\n\"\n",
    "        all_cells_text += \"Code:\\n\"\n",
    "        all_cells_text += \"\".join(cell['source']).strip() + \"\\n\"\n",
    "        all_cells_text += \"Output(s):\\n\"\n",
    "        # Add outputs\n",
    "        for output in cell['outputs']:\n",
    "            output_text = \"\"\n",
    "            if output.get('output_type') == 'stream':\n",
    "                text = output.get('text', '')\n",
    "                if isinstance(text, list):\n",
    "                    text = \"\".join(text)\n",
    "                output_text += text.strip()\n",
    "            elif output.get('output_type') in ['execute_result', 'display_data']:\n",
    "                data = output.get('data', {})\n",
    "                text = data.get('text/plain', '')\n",
    "                if isinstance(text, list):\n",
    "                    text = \"\".join(text)\n",
    "                output_text += text.strip()\n",
    "            # Skipping errors\n",
    "            if output_text:\n",
    "                all_cells_text += output_text + \"\\n\"\n",
    "        all_cells_text += \"-\" * 30 + \"\\n\"\n",
    "\n",
    "# Optional: remove leading/trailing whitespace\n",
    "all_cells_text = all_cells_text.strip()\n",
    "\n",
    "# Print or use as needed\n",
    "print(all_cells_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d289160",
   "metadata": {},
   "source": [
    "# Chroma DB - Prompt Outpout for white paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24a759c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, chromadb\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "DATA_DIR = os.path.abspath(\"./chroma_openai1\")  # <-- match the folder that actually has your DB\n",
    "client = chromadb.PersistentClient(path=DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d54e0c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: notebook_74b8a7ff23f322cce20437b9a56a9c32681a0bab37a51ce5999bad4bb0cf0431\n",
      "Name: whitepaper_02958f41437b5bbcf9490a38b0edb5d41a365101ce7979d2822648a320dfdc73\n",
      "Name: whitepaper_80155473120ea4dcf824fec347b00809b601f5a71c2ed64892a6c178903ec71b\n",
      "Name: notebook_f887cf79f48bf8b101631e9ebdb3ca7220bd2c6e47a6b82041ca192aa98cf16b\n",
      "Name: whitepaper_803eb93c087768f8959427cf4ede1d1af37a2717b8a7d2b7952e58ea79b8a4ed\n"
     ]
    }
   ],
   "source": [
    "for db in client.list_collections():\n",
    "    print(\"Name:\",db.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a0d9d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import chromadb\n",
    "\n",
    "# Path to your actual persistent Chroma storage\n",
    "DATA_DIR = os.path.abspath(\"./chroma_openai1\")  # adjust if needed\n",
    "\n",
    "# Use PersistentClient to read from persistent DB\n",
    "client = chromadb.PersistentClient(path=DATA_DIR)\n",
    "\n",
    "# Get the specific collection\n",
    "collection_name = \"whitepaper_02958f41437b5bbcf9490a38b0edb5d41a365101ce7979d2822648a320dfdc73\"\n",
    "collection = client.get_collection(name=collection_name)\n",
    "\n",
    "# Retrieve all records (you can also filter with where / where_document)\n",
    "results = collection.get(\n",
    "    include=[\"documents\", \"metadatas\", \"embeddings\"],  # choose what you need\n",
    "    limit=5  # remove or increase for more\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35f63f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: --- Page 1 ---\n",
      "Machine Learning-Based Loan Approval Prediction System\n",
      "for Financial Institutions\n",
      "1. Executive Summary\n",
      "The financial services industry faces a critical challenge in automating and de-risking\n",
      "the loan approval process. Traditional methods, often relying on manual review and\n",
      "static rule-based systems, are prone to human error, inconsistency, and significant\n",
      "processing delays. These limitations result in missed opportunities, elevated credit risk,\n",
      "and suboptimal customer experiences. To address these issues, we have developed a\n",
      "robust machine learning-based Loan Approval Classification System. This model\n",
      "leverages a comprehensive set of applicant data to predict the likelihood of loan\n",
      "Metadata: None\n",
      "Embeddings [ 0.00842538  0.00844858  0.01349917 ...  0.02191875 -0.00681337\n",
      "  0.00152359]\n",
      "----------------------------------------\n",
      "Document: leverages a comprehensive set of applicant data to predict the likelihood of loan\n",
      "repayment, classifying applications as either approved or rejected.\n",
      "Our system is designed to provide a high-level overview of an applicant's\n",
      "creditworthiness, offering a data-driven, objective, and transparent decision-making tool.\n",
      "By integrating advanced machine learning techniques, our model achieves superior\n",
      "predictive accuracy compared to traditional methods. It significantly reduces the time\n",
      "from application to decision, minimizes the risk of default, and ensures a consistent, fair\n",
      "evaluation process. This strategic asset not only enhances operational efficiency but\n",
      "also provides a competitive advantage by enabling faster, more confident lending\n",
      "Metadata: None\n",
      "Embeddings [-0.00768665 -0.00180809  0.03975978 ...  0.02178186 -0.00130231\n",
      " -0.02467723]\n",
      "----------------------------------------\n",
      "Document: also provides a competitive advantage by enabling faster, more confident lending\n",
      "decisions.\n",
      "2. Introduction\n",
      "The process of loan approval is a cornerstone of the financial industry. It involves\n",
      "evaluating a multitude of factors to determine an applicant's creditworthiness and ability\n",
      "to repay a loan. Historically, this process has been labor-intensive, relying on credit\n",
      "officers to manually review application forms, financial statements, and credit reports.\n",
      "This manual approach is slow, expensive, and susceptible to biases. The rise of digital\n",
      "banking and the demand for instant financial services have made this traditional model\n",
      "increasingly unsustainable.\n",
      "This white paper details a machine learning-based solution designed to modernize and\n",
      "Metadata: None\n",
      "Embeddings [-0.00970097 -0.00116368  0.04154433 ...  0.0092116  -0.02381199\n",
      " -0.00088806]\n",
      "----------------------------------------\n",
      "Document: This white paper details a machine learning-based solution designed to modernize and\n",
      "optimize the loan approval workflow. By building a classification model, our system can\n",
      "accurately predict the Loan_Status (Approved or Rejected) for new applications. The\n",
      "primary motivation for this project is to create a scalable, efficient, and fair system that\n",
      "can process thousands of applications in real-time, reducing operational costs while\n",
      "simultaneously improving the quality of lending decisions. Our model is intended to\n",
      "serve as a decision-support tool for loan officers, enabling them to focus on complex\n",
      "cases and customer relationships rather than routine data analysis.\n",
      "Metadata: None\n",
      "Embeddings [-0.01066026  0.02007049  0.00140409 ...  0.02936973 -0.01320119\n",
      " -0.00089152]\n",
      "----------------------------------------\n",
      "Document: --- Page 2 ---\n",
      "3. Related Work / Literature Review\n",
      "The field of credit scoring and loan prediction has seen extensive research and\n",
      "application of various machine learning models. A number of algorithms have been\n",
      "employed to analyze applicant data and forecast loan outcomes:\n",
      "·\n",
      "Logistic Regression: A statistical method used for binary classification, which is\n",
      "well-suited for predicting a 'yes' or 'no' outcome for loan approval. It is valued for\n",
      "its simplicity and the interpretability of its results, as it shows how different factors\n",
      "influence the final decision.\n",
      "·\n",
      "Decision Trees: These models use a tree-like structure of decisions and their\n",
      "possible consequences. They are easy to understand and visualize, as they\n",
      "Metadata: None\n",
      "Embeddings [-0.01123868  0.0081664   0.01467241 ...  0.0407981   0.00485409\n",
      " -0.00456889]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print sample\n",
    "for doc, meta,embeddings in zip(results[\"documents\"], results[\"metadatas\"], results['embeddings']):\n",
    "    print(\"Document:\", doc)\n",
    "    print(\"Metadata:\", meta)\n",
    "    print(\"Embeddings\",embeddings)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bc3cac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nbformat\n",
    "from openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import streamlit as st\n",
    "import fitz\n",
    "import tempfile\n",
    "import hashlib\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66266726",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcc6cfd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6y/6bdr99fj10b8r3lfc1hyq1_40000gn/T/ipykernel_1618/3768337022.py:2: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n"
     ]
    }
   ],
   "source": [
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "430d1ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryFun(query, embedding_model, collection):\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=5)\n",
    "    l_docs = [doc for doc in results[\"documents\"][0]]\n",
    "    return l_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "855b5b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Summary/Objective of white paper \",\n",
    "    \"Select All features or Features name. Do not include any of the following preprocessing steps, model details, data types, or any explanations.\",\n",
    "    \"Feature names from preprocessing steps — list only the features on which preprocessing was applied. excluding any training methodology, model details, bias mitigation, evaluation\",\n",
    "    \"Name the Model selected for classification\",\n",
    "    \"Extract the list of hyperparameters/ performance scores/ Best hyperparameters along with their  values.\",\n",
    "    \"List of validation metrics and respective scores only and hyperparameter scores\",\n",
    "    \"Ethical considerations\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "3b402bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = queryFun(queries[6], embedding_model, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "c9ca36df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "++++++++++++++++++++++++++++++++\n",
      "```\n",
      "\n",
      "## Markdown Cell:\n",
      "## Machine learning model development\n",
      "\n",
      "## Code Cell:\n",
      "```python\n",
      "\n",
      "# XGBoost model\n",
      "xgb_model = xgb.XGBClassifier(\n",
      "    objective = 'binary:logistic',\n",
      "    use_label_encoder = False\n",
      ")\n",
      "\n",
      "# Define parameter range \n",
      "params = {\n",
      "    'eta': np.arange(0.1, 0.26, 0.05),\n",
      "    'min_child_weight': np.arange(1, 5, 0.5).tolist(),\n",
      "    'gamma': [5],\n",
      "    'subsample': np.arange(0.5, 1.0, 0.11).tolist(),\n",
      "    'colsample_bytree': np.arange(0.5, 1.0, 0.11).tolist()\n",
      "}\n",
      "\n",
      "# Make a scorer from a performance metric or loss function\n",
      "scorers = {\n",
      "    'f1_score': make_scorer(f1_score),\n",
      "    'precision_score': make_scorer(precision_score),\n",
      "    'recall_score': make_scorer(recall_score),\n",
      "    'accuracy_score': make_scorer(accuracy_score)\n",
      "}\n",
      "++++++++++++++++++++++++++++++++\n",
      "Dependents_3+  Education_Not Graduate  Self_Employed_Yes  \\\n",
      "0          False                   False              False   \n",
      "1           True                    True               True   \n",
      "2          False                   False              False   \n",
      "3          False                    True               True   \n",
      "4          False                   False              False\n",
      "++++++++++++++++++++++++++++++++\n",
      "Dependents_3+  Education_Not Graduate  Self_Employed_Yes  \\\n",
      "0          False                   False              False   \n",
      "1           True                    True               True   \n",
      "2          False                   False              False   \n",
      "3          False                    True               True   \n",
      "4          False                   False              False\n",
      "++++++++++++++++++++++++++++++++\n",
      "## Markdown Cell:\n",
      "# Machine Learning Model Dev\n",
      "\n",
      "## Markdown Cell:\n",
      "---\n",
      "\n",
      "## Markdown Cell:\n",
      "## Import packages\n",
      "\n",
      "## Markdown Cell:\n",
      "There are some packages for doing descriptive analytics as follows:\n",
      "- **pandas**: for data manipulation\n",
      "- **numpy**: for linear algebra calculation\n",
      "- **matplotlib**: for data visualization\n",
      "- **seaborn**: for data manipulation\n",
      "- **plotnine**: for data manipulation\n",
      "\n",
      "**Note**: that there are more than one package used for making a data visualization. The `plotnine` can be your choice if you are familiar with `ggplot2` on R programming. It will create your visualization beautifully\n",
      "\n",
      "## Code Cell:\n",
      "```python\n",
      "# Dataframe manipulation\n",
      "import pandas as pd\n",
      "\n",
      "# Linear algebra\n",
      "import numpy as np\n",
      "++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "for o in out:\n",
    "    print(o)\n",
    "    print(\"++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "55b635ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_list = [\n",
    "    \"Overview\",\n",
    "    \"Do not include descriptions, preprocessing details, training methodology, target variable explanations, or any other text.\",\n",
    "    \"Include only the train/test percentages and their purposes if mentioned. Do not include hyperparameter tuning, validation strategy, retraining details, evaluation metrics, or deployment strategy.\",\n",
    "    \"Exclude summary and Laon details. Do not consider any training methodology, model details, bias mitigation, evaluation, or future work, and output in the format <Feature_Name>: <Operation_Name> preserving the exact feature names.\",\n",
    "    \"Keep Model detail only, exclude Traning methodology, resluts, analysis, feature engineering and data handling\",\n",
    "    \"Keep only validation/perofrmance and best hyperparameter scores. Do not include other details\"\n",
    "    \"Keep only ethical considerations. do not include other details\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "a6f8f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_extracted_elements_with_context(similar_elements, query_context):\n",
    "    combined_elements = \"\\n\\n\".join(similar_elements)\n",
    "    prompt = [\n",
    "        SystemMessage(content=\"You are a product analyst.\"),\n",
    "               \n",
    "        HumanMessage(content=f\"\"\"\n",
    "        The following are the top 5 similar elements retrieved from a vector database and  create a structured report in HTML format with the following three sections, using dangerouslySetInnerHTML={{ __html: reportMarkdown }}; html should not affect other elements : \n",
    "        {combined_elements}\n",
    "\n",
    "        The original query context is:\n",
    "        \"{query_context}\"\n",
    "        - Keep only ethical considerations. do not include other details\n",
    "        - Identify and extract only the most relevant elements or functionalities.\n",
    "        - Do not recommend, only extract \n",
    "        - Avoid verbose explanations; focus on clarity and precision.\n",
    "        - Provide concise, bullet-pointed outputs or insights based on retrieved data.\n",
    "        \"\"\")\n",
    "\n",
    "        # HumanMessage(content=f\"\"\"\n",
    "        # The following are the top 5 similar elements retrieved from a vector database : \n",
    "\n",
    "        # {combined_elements}\n",
    "\n",
    "        # The original query context is:\n",
    "        # \"{query_context}\"\n",
    "\n",
    "        # - Identify and extract only the most relevant elements or functionalities.\n",
    "        # - Avoid verbose explanations; focus on clarity and precision.\n",
    "        # - Extract details from given context keep length short in summary format \n",
    "        # - Do not recommend, only extract\n",
    "        # - Extract metrics score/values, model name and and hyperpapramter values if available in context \n",
    "        # - Provide concise, bullet-pointed outputs or insights based on retrieved data.\n",
    "        # - Format the response using IPython Markdown style for readability\n",
    "\n",
    "        # \"\"\")\n",
    "    ]\n",
    "    return llm(prompt).content.strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4ed16d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "b4b476f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_output= refine_extracted_elements_with_context(out,queries[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "2388d98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```html\n",
       "<div dangerouslySetInnerHTML={{ __html: reportMarkdown }}>\n",
       "  <h2>Ethical Considerations in Machine Learning Model Development</h2>\n",
       "  <ul>\n",
       "    <li>Ensure transparency in model development and decision-making processes.</li>\n",
       "    <li>Maintain data privacy and protect sensitive information used in training models.</li>\n",
       "    <li>Avoid bias in model training by using diverse and representative datasets.</li>\n",
       "    <li>Implement fairness checks to ensure equitable outcomes across different demographic groups.</li>\n",
       "    <li>Regularly audit models for unintended consequences and rectify any ethical issues.</li>\n",
       "  </ul>\n",
       "</div>\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(ref_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c028b0",
   "metadata": {},
   "source": [
    "## Chroma DB for Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "b0cfe8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your actual persistent Chroma storage\n",
    "DATA_DIR = os.path.abspath(\"./chroma_openai1\")  # adjust if needed\n",
    "\n",
    "# Use PersistentClient to read from persistent DB\n",
    "client = chromadb.PersistentClient(path=DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "51560aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coll name: notebook_74b8a7ff23f322cce20437b9a56a9c32681a0bab37a51ce5999bad4bb0cf0431\n",
      "Coll name: whitepaper_02958f41437b5bbcf9490a38b0edb5d41a365101ce7979d2822648a320dfdc73\n",
      "Coll name: whitepaper_80155473120ea4dcf824fec347b00809b601f5a71c2ed64892a6c178903ec71b\n",
      "Coll name: notebook_f887cf79f48bf8b101631e9ebdb3ca7220bd2c6e47a6b82041ca192aa98cf16b\n",
      "Coll name: whitepaper_803eb93c087768f8959427cf4ede1d1af37a2717b8a7d2b7952e58ea79b8a4ed\n"
     ]
    }
   ],
   "source": [
    "for coll in client.list_collections():\n",
    "    print(\"Coll name:\", coll.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "6350d83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the specific collection\n",
    "collection_name = \"notebook_f887cf79f48bf8b101631e9ebdb3ca7220bd2c6e47a6b82041ca192aa98cf16b\"\n",
    "collection = client.get_collection(name=collection_name)\n",
    "\n",
    "# Retrieve all records (you can also filter with where / where_document)\n",
    "results = collection.get(\n",
    "    include=[\"documents\", \"metadatas\", \"embeddings\"],  # choose what you need\n",
    "    limit=5  # remove or increase for more\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "fc4f194b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: ## Markdown Cell:\n",
      "# Machine Learning Model Dev\n",
      "\n",
      "## Markdown Cell:\n",
      "---\n",
      "\n",
      "## Markdown Cell:\n",
      "## Import packages\n",
      "\n",
      "## Markdown Cell:\n",
      "There are some packages for doing descriptive analytics as follows:\n",
      "- **pandas**: for data manipulation\n",
      "- **numpy**: for linear algebra calculation\n",
      "- **matplotlib**: for data visualization\n",
      "- **seaborn**: for data manipulation\n",
      "- **plotnine**: for data manipulation\n",
      "\n",
      "**Note**: that there are more than one package used for making a data visualization. The `plotnine` can be your choice if you are familiar with `ggplot2` on R programming. It will create your visualization beautifully\n",
      "\n",
      "## Code Cell:\n",
      "```python\n",
      "# Dataframe manipulation\n",
      "import pandas as pd\n",
      "\n",
      "# Linear algebra\n",
      "import numpy as np\n",
      "Metadata: None\n",
      "Embeddings [-0.02262077  0.00858882  0.05075535 ... -0.01819087 -0.01168151\n",
      " -0.01755466]\n",
      "----------------------------------------\n",
      "Document: # Linear algebra\n",
      "import numpy as np\n",
      "\n",
      "# Data visualization with plotnine\n",
      "from plotnine import *\n",
      "import plotnine\n",
      "\n",
      "# Data visualization with matplotlib\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# Data partitioning\n",
      "from sklearn.model_selection import train_test_split\n",
      "from sklearn.model_selection import KFold\n",
      "\n",
      "# Grid-search\n",
      "from sklearn.model_selection import GridSearchCV\n",
      "\n",
      "# Evaluation metrics\n",
      "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
      "from sklearn.metrics import make_scorer\n",
      "\n",
      "# XGBoost model\n",
      "import xgboost as xgb\n",
      "\n",
      "# Save the model\n",
      "import joblib \n",
      "```\n",
      "\n",
      "## Code Cell:\n",
      "```python\n",
      "# Ignore warnings\n",
      "import warnings\n",
      "warnings.filterwarnings('ignore', category = FutureWarning)\n",
      "```\n",
      "\n",
      "## Markdown Cell:\n",
      "## Import data set\n",
      "Metadata: None\n",
      "Embeddings [-0.03622217  0.0097133   0.0779786  ... -0.01214588 -0.0225453\n",
      " -0.02183083]\n",
      "----------------------------------------\n",
      "Document: ## Markdown Cell:\n",
      "## Import data set\n",
      "\n",
      "## Markdown Cell:\n",
      "After importing the data set into Python, the `df_train` is now our data frame. The data frame has a lot of functions and methods that will create spesific outputs about the characteristic of data frame. The method of `columns` will print out all the column names.\n",
      "\n",
      "## Markdown Cell:\n",
      "### Training set\n",
      "\n",
      "## Code Cell:\n",
      "```python\n",
      "# Import the training set\n",
      "df_train = pd.read_csv(\n",
      "    filepath_or_buffer = 'https://raw.githubusercontent.com/dphi-official/Datasets/master/Loan_Data/loan_train.csv',\n",
      "    usecols = [i for i in range(1, 14)]\n",
      ")\n",
      "```\n",
      "Metadata: None\n",
      "Embeddings [-0.02445893  0.00676845  0.05273957 ... -0.02389573 -0.00858376\n",
      " -0.01461301]\n",
      "----------------------------------------\n",
      "Document: ## Code Cell:\n",
      "```python\n",
      "# Data dimension\n",
      "print('Data dimension: {} rows and {} columns'.format(len(df_train), len(df_train.columns)))\n",
      "df_train.head()\n",
      "```\n",
      "\n",
      "### Output:\n",
      "```\n",
      "Output (stream):\n",
      "Data dimension: 491 rows and 13 columns\n",
      "\n",
      "Output (execute_result):\n",
      "    Loan_ID  Gender Married Dependents     Education Self_Employed  \\\n",
      "0  LP002305  Female      No          0      Graduate            No   \n",
      "1  LP001715    Male     Yes         3+  Not Graduate           Yes   \n",
      "2  LP002086  Female     Yes          0      Graduate            No   \n",
      "3  LP001136    Male     Yes          0  Not Graduate           Yes   \n",
      "4  LP002529    Male     Yes          2      Graduate            No\n",
      "Metadata: None\n",
      "Embeddings [0.00616907 0.00636676 0.04874427 ... 0.00169941 0.00422001 0.00493221]\n",
      "----------------------------------------\n",
      "Document: ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
      "0             4547                0.0       115.0             360.0   \n",
      "1             5703                0.0       130.0             360.0   \n",
      "2             4333             2451.0       110.0             360.0   \n",
      "3             4695                0.0        96.0               NaN   \n",
      "4             6700             1750.0       230.0             300.0   \n",
      "\n",
      "   Credit_History Property_Area  Loan_Status  \n",
      "0             1.0     Semiurban            1  \n",
      "1             1.0         Rural            1  \n",
      "2             1.0         Urban            0  \n",
      "3             1.0         Urban            1  \n",
      "4             1.0     Semiurban            1  \n",
      "```\n",
      "\n",
      "## Markdown Cell:\n",
      "### Testing data\n",
      "Metadata: None\n",
      "Embeddings [-0.03801318 -0.02082817  0.08246544 ... -0.01752391 -0.01282168\n",
      " -0.00463162]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print sample\n",
    "for doc, meta,embeddings in zip(results[\"documents\"], results[\"metadatas\"], results['embeddings']):\n",
    "    print(\"Document:\", doc)\n",
    "    print(\"Metadata:\", meta)\n",
    "    print(\"Embeddings\",embeddings)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "1a8f9c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nbformat\n",
    "from openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import streamlit as st\n",
    "import fitz\n",
    "import tempfile\n",
    "import hashlib\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "37deb9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "69b91343",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "9cd31e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryFun(query, embedding_model, collection):\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=5)\n",
    "    l_docs = [doc for doc in results[\"documents\"][0]]\n",
    "    return l_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "9a3834e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Summary/Objective of white paper \",\n",
    "    \"Select All features or Features name. Do not include any of the following preprocessing steps, model details, data types, or any explanations.\",\n",
    "    \"Feature names from preprocessing steps — list only the features on which preprocessing was applied. excluding any training methodology, model details, bias mitigation, evaluation\",\n",
    "    \"Name the Model selected for classification\",\n",
    "    \"Extract the list of hyperparameters/ performance scores/ Best hyperparameters along with their  values.\",\n",
    "    \"List of validation metrics and respective scores only and hyperparameter scores\",\n",
    "    \"Ethical considerations\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "ebac5e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = queryFun(queries[0], embedding_model, collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "c7d02a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```\n",
      "++++++++++++++++++++++++++++++++\n",
      "## Code Cell:\n",
      "```python\n",
      "plotnine.options.figure_size = (8, 4.8)\n",
      "(\n",
      "    ggplot(\n",
      "        data = df_viz_4\n",
      "    )+\n",
      "    geom_density(\n",
      "        aes(\n",
      "            x = 'ApplicantIncome',\n",
      "            fill = 'Loan_Status'\n",
      "        ),\n",
      "        color = 'white',\n",
      "        alpha = 0.85\n",
      "    )+\n",
      "    labs(\n",
      "        title = 'The distribution of applicant incomes by loan status'\n",
      "    )+\n",
      "    scale_fill_manual(\n",
      "        name = 'Loan Status',\n",
      "        values = ['#981220','#80797c'],\n",
      "        labels = ['Default', 'Not Default']\n",
      "    )+\n",
      "    xlab(\n",
      "        'Applicant income'\n",
      "    )+\n",
      "    ylab(\n",
      "        'Density'\n",
      "    )+\n",
      "    theme_minimal()\n",
      ")\n",
      "```\n",
      "\n",
      "## Markdown Cell:\n",
      "### The distribution of loan amount by loan status\n",
      "++++++++++++++++++++++++++++++++\n",
      "## Code Cell:\n",
      "```python\n",
      "plotnine.options.figure_size = (8, 4.8)\n",
      "(\n",
      "    ggplot(\n",
      "        data = df_viz_5\n",
      "    )+\n",
      "    geom_density(\n",
      "        aes(\n",
      "            x = 'LoanAmount',\n",
      "            fill = 'Loan_Status'\n",
      "        ),\n",
      "        color = 'white',\n",
      "        alpha = 0.85\n",
      "    )+\n",
      "    labs(\n",
      "        title = 'The distribution of loan amount by loan status'\n",
      "    )+\n",
      "    scale_fill_manual(\n",
      "        name = 'Loan Status',\n",
      "        values = ['#981220','#80797c'],\n",
      "        labels = ['Default', 'Not Default']\n",
      "    )+\n",
      "    xlab(\n",
      "        'Loan amount'\n",
      "    )+\n",
      "    ylab(\n",
      "        'Density'\n",
      "    )+\n",
      "    theme_minimal()\n",
      ")\n",
      "```\n",
      "\n",
      "## Markdown Cell:\n",
      "## One-hot encoder\n",
      "++++++++++++++++++++++++++++++++\n",
      "## Code Cell:\n",
      "```python\n",
      "plotnine.options.figure_size = (8, 4.8)\n",
      "(\n",
      "    ggplot(\n",
      "        data = df_viz_3\n",
      "    )+\n",
      "    geom_bar(\n",
      "        aes(\n",
      "            x = 'Education',\n",
      "            y = 'Total',\n",
      "            fill = 'Loan_Status'\n",
      "        ),\n",
      "        stat = 'identity',\n",
      "        position = 'fill',\n",
      "        width = 0.5\n",
      "    )+\n",
      "    labs(\n",
      "        title = 'The composition of loan status by the education',\n",
      "        fill = 'Loan status'\n",
      "    )+\n",
      "    xlab(\n",
      "        'Educations'\n",
      "    )+\n",
      "    ylab(\n",
      "        'Frequency'\n",
      "    )+\n",
      "    scale_x_discrete(\n",
      "        limits = ['Graduate', 'Not Graduate']\n",
      "    )+\n",
      "    scale_fill_manual(\n",
      "        values = ['#981220','#80797c'],\n",
      "        labels = ['Default', 'Not Default']\n",
      "    )+\n",
      "    theme_minimal()\n",
      ")\n",
      "```\n",
      "++++++++++++++++++++++++++++++++\n",
      "## Code Cell:\n",
      "```python\n",
      "plotnine.options.figure_size = (8, 4.8)\n",
      "(\n",
      "    ggplot(\n",
      "        data = df_viz_2\n",
      "    )+\n",
      "    geom_bar(\n",
      "        aes(\n",
      "            x = 'Dependents',\n",
      "            y = 'Total',\n",
      "            fill = 'Loan_Status'\n",
      "        ),\n",
      "        stat = 'identity',\n",
      "        position = 'fill',\n",
      "        width = 0.5\n",
      "    )+\n",
      "    labs(\n",
      "        title = 'The composition of loan status by the dependents',\n",
      "        fill = 'Loan status'\n",
      "    )+\n",
      "    xlab(\n",
      "        'Dependents'\n",
      "    )+\n",
      "    ylab(\n",
      "        'Frequency'\n",
      "    )+\n",
      "    scale_x_discrete(\n",
      "        limits = ['0', '1', '2', '3+']\n",
      "    )+\n",
      "    scale_fill_manual(\n",
      "        values = ['#981220','#80797c'],\n",
      "        labels = ['Default', 'Not Default']\n",
      "    )+\n",
      "    theme_minimal()\n",
      ")\n",
      "```\n",
      "++++++++++++++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "for o in out:\n",
    "    print(o)\n",
    "    print(\"++++++++++++++++++++++++++++++++\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "702827f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_list = [\n",
    "    \"Overview\",\n",
    "    \"Do not include descriptions, preprocessing details, training methodology, target variable explanations, or any other text.\",\n",
    "    \"Include only the train/test percentages and their purposes if mentioned. Do not include hyperparameter tuning, validation strategy, retraining details, evaluation metrics, or deployment strategy.\",\n",
    "    \"Exclude summary and Laon details. Do not consider any training methodology, model details, bias mitigation, evaluation, or future work, and output in the format <Feature_Name>: <Operation_Name> preserving the exact feature names.\",\n",
    "    \"Keep Model detail only, exclude Traning methodology, resluts, analysis, feature engineering and data handling\",\n",
    "    \"Keep only validation/perofrmance and best hyperparameter scores. Do not include other details\"\n",
    "    \"Keep only ethical considerations. do not include other details\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6649ebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_extracted_elements_with_context(similar_elements, query_context):\n",
    "    combined_elements = \"\\n\\n\".join(similar_elements)\n",
    "    prompt = [\n",
    "        SystemMessage(content=\"You are a product analyst.\"),\n",
    "               \n",
    "        HumanMessage(content=f\"\"\"\n",
    "        The following are the top 5 similar elements retrieved from a vector database and  create a structured report in HTML format with the following three sections, using dangerouslySetInnerHTML={{ __html: reportMarkdown }}; html should not affect other elements : \n",
    "        {combined_elements}\n",
    "\n",
    "        The original query context is:\n",
    "        \"{query_context}\"\n",
    "        - From the provided HTML or text, extract and main pointer summarize only the Overview section, excluding all other sections or details, keeping the summary concise.\n",
    "        - Identify and extract only the most relevant elements or functionalities.\n",
    "        - Do not recommend, only extract \n",
    "        - Avoid verbose explanations; focus on clarity and precision.\n",
    "        - Provide concise, bullet-pointed outputs or insights based on retrieved data.\n",
    "        \"\"\")\n",
    "\n",
    "        # HumanMessage(content=f\"\"\"\n",
    "        # The following are the top 5 similar elements retrieved from a vector database : \n",
    "\n",
    "        # {combined_elements}\n",
    "\n",
    "        # The original query context is:\n",
    "        # \"{query_context}\"\n",
    "\n",
    "        # - Identify and extract only the most relevant elements or functionalities.\n",
    "        # - Avoid verbose explanations; focus on clarity and precision.\n",
    "        # - Extract details from given context keep length short in summary format \n",
    "        # - Do not recommend, only extract\n",
    "        # - Extract metrics score/values, model name and and hyperpapramter values if available in context \n",
    "        # - Provide concise, bullet-pointed outputs or insights based on retrieved data.\n",
    "        # - Format the response using IPython Markdown style for readability\n",
    "\n",
    "        # \"\"\")\n",
    "    ]\n",
    "    return llm(prompt).content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "4f00885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_output= refine_extracted_elements_with_context(out,queries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "9a99ef7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```html\n",
       "<div>\n",
       "    <h2>Overview</h2>\n",
       "    <ul>\n",
       "        <li>Analysis of applicant income distribution by loan status using density plots.</li>\n",
       "        <li>Examination of loan amount distribution by loan status through density plots.</li>\n",
       "        <li>Investigation of loan status composition by education level using bar charts.</li>\n",
       "        <li>Study of loan status composition by number of dependents using bar charts.</li>\n",
       "    </ul>\n",
       "</div>\n",
       "\n",
       "<div>\n",
       "    <h2>Relevant Elements</h2>\n",
       "    <ul>\n",
       "        <li>Density plots for visualizing income and loan amount distributions.</li>\n",
       "        <li>Bar charts for analyzing loan status composition by education and dependents.</li>\n",
       "        <li>Use of color coding to differentiate between 'Default' and 'Not Default' loan statuses.</li>\n",
       "    </ul>\n",
       "</div>\n",
       "\n",
       "<div>\n",
       "    <h2>Functionalities</h2>\n",
       "    <ul>\n",
       "        <li>Customizable plot size using <code>plotnine.options.figure_size</code>.</li>\n",
       "        <li>Density and bar plots created using <code>ggplot</code> and <code>geom_density</code>/<code>geom_bar</code>.</li>\n",
       "        <li>Manual color scaling and labeling for clarity in visualizations.</li>\n",
       "        <li>Minimal theme applied for a clean and focused presentation of data.</li>\n",
       "    </ul>\n",
       "</div>\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(ref_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de70af67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
