{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcbf75e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04950344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```html\n",
      "<!DOCTYPE html>\n",
      "<html lang=\"en\">\n",
      "<head>\n",
      "    <meta charset=\"UTF-8\">\n",
      "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
      "    <title>AI Report</title>\n",
      "</head>\n",
      "<body>\n",
      "\n",
      "    <h1>AI Report</h1>\n",
      "\n",
      "    <h2>1. Validation Metrics</h2>\n",
      "    <p>The code implementation provides specific metrics such as F1 Score, Precision, Recall, and Accuracy. However, the white paper emphasizes high precision, recall, and F1-score without specifying exact values. The strengths of the code lie in its detailed metric reporting, but it lacks alignment with the white paper's focus on logistic regression, which may affect the interpretability of these metrics.</p>\n",
      "\n",
      "    <h2>2. Code Comparison Inferences</h2>\n",
      "    <p>The code uses XGBoost (XGBClassifier) instead of the Logistic Regression model specified in the white paper. This represents a significant deviation from the documented goals. The data splitting strategy and feature selection also differ, with the code lacking explicit preprocessing steps and imbalance handling techniques like SMOTE. These discrepancies highlight a need for alignment between the code and the white paper's objectives.</p>\n",
      "\n",
      "    <h2>3. Recommendations</h2>\n",
      "    <p>To improve alignment and performance, the following steps are recommended:</p>\n",
      "    <ul>\n",
      "        <li>Update the white paper to reflect the use of XGBoost if it is the preferred model, or adjust the code to implement Logistic Regression as originally intended.</li>\n",
      "        <li>Ensure preprocessing steps and imbalance handling techniques are clearly defined and implemented in the code.</li>\n",
      "        <li>Align data splitting strategies and feature selection with the white paper's specifications to ensure consistency.</li>\n",
      "        <li>Consider providing fallback mechanisms and detailed hyperparameter configurations for the chosen model to enhance robustness.</li>\n",
      "    </ul>\n",
      "\n",
      "</body>\n",
      "</html>\n",
      "```\n",
      "\n",
      "Saved to FinalReport.txt\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "\n",
    "MODEL = \"gpt-4o\"\n",
    "OUTPUT_FILE = \"FinalReport.txt\"\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Setup LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o\",\n",
    "    temperature=0,\n",
    "    openai_api_key=openai_api_key\n",
    ")\n",
    "\n",
    "def read_file(path: str) -> str:\n",
    "    return Path(path).read_text(encoding=\"utf-8\")\n",
    "\n",
    "\n",
    "def generate_summary(Push_Commit_summary: str, White_paper_comparision: str) -> str:\n",
    "    \"\"\"Takes two text inputs + instruction, returns LLM output.\"\"\"\n",
    "    llm = ChatOpenAI(model=MODEL, temperature=0.0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    # 2) Define instruction\n",
    "    instruction = (\n",
    "\n",
    "    '''\n",
    "        You are an AI report generator. Based on the inputs provided, create a structured report in HTML format with the following three sections, using dangerouslySetInnerHTML={{ __html: reportMarkdown }}; html should not affect other elements:\n",
    "\n",
    "        1. **Validation Metrics**\n",
    "            - Summarize model evaluation metrics (Accuracy, Precision, Recall, F1 Score, AUC, etc.).\n",
    "            - Highlight strengths or weaknesses in these metrics.\n",
    "\n",
    "        2. **Code Comparison Inferences**\n",
    "            - Compare implemented code functionalities with described goals or documentation (e.g., whitepaper).\n",
    "            - Identify alignments, discrepancies, or missing elements.\n",
    "            - Highlight improvements or regressions.\n",
    "\n",
    "        3. **Recommendations**\n",
    "            - Suggest improvements or next steps based on validation results and code comparison.\n",
    "            - Include actionable changes to improve accuracy, consistency, or system robustness.\n",
    "\n",
    "    '''\n",
    "    )\n",
    "\n",
    "    user_content = (\n",
    "        f\"{instruction}\\n\\n\"\n",
    "        f\"## Document A\\n```\\n{Push_Commit_summary}\\n```\\n\\n\"\n",
    "        f\"## Document B\\n```\\n{White_paper_comparision}\\n```\\n\"\n",
    "    )\n",
    "    \n",
    "    return llm.invoke([\n",
    "        SystemMessage(content=\"You are a concise, structured assistant. Use Markdown.\"),\n",
    "        HumanMessage(content=user_content)\n",
    "    ]).content\n",
    "\n",
    "\n",
    "\n",
    "Push_Commit_summary = read_file(\"Push_Commit_summary_outout.txt\")\n",
    "White_paper_comparision = read_file(\"white_paper_comparision.txt\")\n",
    "\n",
    "\n",
    "# 3) Get output\n",
    "output = generate_summary(Push_Commit_summary, White_paper_comparision)\n",
    "\n",
    "# 4) Print + save\n",
    "print(output)\n",
    "Path(OUTPUT_FILE).write_text(output, encoding=\"utf-8\")\n",
    "print(f\"\\nSaved to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ff5f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6099b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6387af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_summary(Push_Commit_summary: str, White_paper_comparision: str) -> str:\n",
    "    \"\"\"Takes two text inputs + instruction, returns LLM output.\"\"\"\n",
    "    llm = ChatOpenAI(model=MODEL, temperature=0.0, openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    \n",
    "    # 2) Define instruction\n",
    "    instruction = (\n",
    "\n",
    "    '''\n",
    "        You are an AI report generator. Based on the inputs provided, create a structured report:\n",
    "\n",
    "        1. **Validation Metrics**\n",
    "            - Summarize model evaluation metrics (Accuracy, Precision, Recall, F1 Score, AUC, etc.).\n",
    "            - Highlight strengths or weaknesses in these metrics.\n",
    "\n",
    "        2. **Code Comparison Inferences**\n",
    "            - Compare implemented code functionalities with described goals or documentation (e.g., whitepaper).\n",
    "            - Identify alignments, discrepancies, or missing elements.\n",
    "            - Highlight improvements or regressions.\n",
    "\n",
    "        3. **Recommendations**\n",
    "            - Suggest improvements or next steps based on validation results and code comparison.\n",
    "            - Include actionable changes to improve accuracy, consistency, or system robustness.\n",
    "\n",
    "    '''\n",
    "    )\n",
    "\n",
    "    user_content = (\n",
    "        f\"{instruction}\\n\\n\"\n",
    "        f\"## Document A\\n```\\n{Push_Commit_summary}\\n```\\n\\n\"\n",
    "        f\"## Document B\\n```\\n{White_paper_comparision}\\n```\\n\"\n",
    "    )\n",
    "    \n",
    "    return llm.invoke([\n",
    "        SystemMessage(content=\"You are a concise, structured assistant. Use Markdown.\"),\n",
    "        HumanMessage(content=user_content)\n",
    "    ]).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffa01a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Structured Report\n",
      "\n",
      "## 1. Validation Metrics\n",
      "\n",
      "### Summary of Model Evaluation Metrics\n",
      "- **Accuracy**: Not explicitly mentioned in the white paper; code provides `accuracy_score`.\n",
      "- **Precision, Recall, F1 Score**: White paper emphasizes high values; code includes these metrics but does not specify target values.\n",
      "- **AUC**: Not mentioned in either document.\n",
      "\n",
      "### Strengths and Weaknesses\n",
      "- **Strengths**: The code provides a comprehensive set of evaluation metrics, allowing for detailed performance analysis.\n",
      "- **Weaknesses**: The white paper lacks specific metric targets, making it difficult to assess alignment with code results.\n",
      "\n",
      "## 2. Code Comparison Inferences\n",
      "\n",
      "### Alignment with Goals or Documentation\n",
      "- **Model Selection**: White paper specifies Logistic Regression; code uses XGBoost, indicating a significant deviation.\n",
      "- **Data Splitting**: Discrepancy in validation and testing data proportions.\n",
      "- **Feature Selection**: Code uses a subset of features mentioned in the white paper.\n",
      "- **Preprocessing and Imbalance Handling**: White paper suggests techniques not implemented in the code.\n",
      "\n",
      "### Discrepancies and Missing Elements\n",
      "- **Model Architecture**: Change from Logistic Regression to XGBoost.\n",
      "- **Data Handling**: Lack of preprocessing and imbalance handling in the code.\n",
      "- **Hyperparameters**: Different sets due to model change.\n",
      "\n",
      "### Improvements or Regressions\n",
      "- **Improvements**: XGBoost may offer better performance but requires justification.\n",
      "- **Regressions**: Lack of preprocessing and imbalance handling could affect model performance.\n",
      "\n",
      "## 3. Recommendations\n",
      "\n",
      "### Suggested Improvements or Next Steps\n",
      "1. **Model Alignment**: Consider aligning the code with the white paper by implementing Logistic Regression or updating the white paper to reflect the use of XGBoost.\n",
      "2. **Data Handling**: Implement preprocessing steps and imbalance handling techniques as suggested in the white paper.\n",
      "3. **Feature Consistency**: Ensure all features mentioned in the white paper are utilized in the code.\n",
      "4. **Documentation Update**: Revise the white paper to include specific metric targets and justify the choice of model and hyperparameters.\n",
      "5. **Validation Strategy**: Align data splitting strategies between the white paper and code for consistency.\n",
      "\n",
      "### Actionable Changes\n",
      "- **Implement SMOTE and class weighting** to handle imbalanced data.\n",
      "- **Update the white paper** to reflect the current model and hyperparameters used in the code.\n",
      "- **Ensure feature selection** in the code matches the white paper's specifications.\n",
      "- **Provide detailed preprocessing steps** in the code to match the white paper's recommendations.\n",
      "\n",
      "Saved to FinalReport.txt\n"
     ]
    }
   ],
   "source": [
    "# 3) Get output\n",
    "output = generate_summary(Push_Commit_summary, White_paper_comparision)\n",
    "\n",
    "# 4) Print + save\n",
    "print(output)\n",
    "Path(OUTPUT_FILE).write_text(output, encoding=\"utf-8\")\n",
    "print(f\"\\nSaved to {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eae0311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1edc9d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a41b9d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9841987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create report\n",
    "def CreateReport(input1, input2, input3):\n",
    "    formatted_prompt = f\"\"\"\n",
    "   You are an AI report generator. Based on the inputs provided, create a structured report with the following three sections:\n",
    "\n",
    "   1. **Validation Metrics**\n",
    "      - Summarize model evaluation metrics (Accuracy, Precision, Recall, F1 Score, AUC, etc.).\n",
    "      - Highlight strengths or weaknesses in these metrics.\n",
    "\n",
    "   2. **Code Comparison Inferences**\n",
    "      - Compare implemented code functionalities with described goals or documentation (e.g., whitepaper).\n",
    "      - Identify alignments, discrepancies, or missing elements.\n",
    "      - Highlight improvements or regressions.\n",
    "\n",
    "   3. **Recommendations**\n",
    "      - Suggest improvements or next steps based on validation results and code comparison.\n",
    "      - Include actionable changes to improve accuracy, consistency, or system robustness.\n",
    "\n",
    "   ---\n",
    "\n",
    "   ### 🔢 Input Block 1 (Enhancements & Changes):\n",
    "   {input1}\n",
    "\n",
    "   ---\n",
    "\n",
    "   ### 🧩 Input Block 2 (Version-wise Comparison):\n",
    "   {input2}\n",
    "\n",
    "   ---\n",
    "\n",
    "   ### 📄 Input Block 3 (Whitepaper vs Code Analysis):\n",
    "   {input3}\n",
    "\n",
    "   Generate the report in markdown format with bullet points for clarity.\n",
    "      \"\"\"\n",
    "\n",
    "    response = llm.invoke([\n",
    "        SystemMessage(content=\"You are a helpful assistant that creates structured reports for ML systems.\"),\n",
    "        HumanMessage(content=formatted_prompt)\n",
    "    ])\n",
    "\n",
    "    return response.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a21e4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Summary ===\n",
      "# AI System Evaluation Report\n",
      "\n",
      "## 1. Validation Metrics\n",
      "\n",
      "- **Accuracy**: Improved from ~78% to 84% after enhancements.\n",
      "- **Precision, Recall, F1 Score, AUC**: Newly introduced metrics provide a comprehensive evaluation of model performance, especially for imbalanced classes.\n",
      "- **Strengths**:\n",
      "  - Significant improvement in accuracy and overall model performance due to feature engineering and model tuning.\n",
      "  - Introduction of multiple evaluation metrics offers deeper insights into model performance.\n",
      "- **Weaknesses**:\n",
      "  - No specific weaknesses identified in the validation metrics; however, continuous monitoring is recommended to ensure consistent performance across different datasets.\n",
      "\n",
      "## 2. Code Comparison Inferences\n",
      "\n",
      "- **Implemented Code vs. Described Goals**:\n",
      "  - **Alignments**:\n",
      "    - The codebase now includes modular scripts and a clean folder structure, aligning with best practices for production-grade systems.\n",
      "    - Feature engineering and preprocessing enhancements align with the goals of improving model input representation.\n",
      "    - The introduction of Random Forest and XGBoost models, along with hyperparameter tuning, aligns with the goal of improving model performance.\n",
      "    - The addition of a Streamlit app for real-time predictions aligns with the goal of creating a user-friendly interface.\n",
      "    - MLflow integration for experiment tracking aligns with the goal of enhancing reproducibility and collaboration.\n",
      "  - **Discrepancies**:\n",
      "    - The whitepaper lacks mention of the \"Sepal length (cm)\" feature used in the code.\n",
      "    - Validation metrics in the code exceed those specified in the whitepaper, indicating a need for documentation updates.\n",
      "- **Improvements/Regressions**:\n",
      "  - The system has improved significantly in terms of modularity, explainability, and user interaction compared to the initial version.\n",
      "  - No regressions identified; the system has evolved from a proof-of-concept to a professional-grade ML system.\n",
      "\n",
      "## 3. Recommendations\n",
      "\n",
      "- **Documentation Updates**:\n",
      "  - Update the whitepaper to include the \"Sepal length (cm)\" feature and revise validation metrics to reflect the improved performance.\n",
      "- **Model and Feature Enhancements**:\n",
      "  - Continue exploring additional feature engineering techniques to further enhance model performance.\n",
      "  - Consider implementing additional models or ensemble methods to further improve accuracy and robustness.\n",
      "- **System Robustness and Consistency**:\n",
      "  - Regularly validate the model on new datasets to ensure consistent performance.\n",
      "  - Implement automated testing for data preprocessing and model inference to catch potential errors early.\n",
      "- **User Interface and Deployment**:\n",
      "  - Gather user feedback on the Streamlit app to identify areas for improvement in user experience.\n",
      "  - Explore deployment options to scale the application for larger user bases if needed.\n",
      "\n",
      "By following these recommendations, the system can maintain its high performance and continue to evolve in response to user needs and technological advancements.\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Run the report generator\n",
    "if __name__ == \"__main__\":\n",
    "    input1 = '''New Features / Enhancements:\n",
    "    Added feature engineering pipeline for income, credit history, and loan term normalization.\n",
    "    Integrated missing value imputation using median/mode strategies.\n",
    "    Introduced XGBoost and Random Forest classifiers alongside logistic regression for improved model performance.\n",
    "    Implemented model selection and hyperparameter tuning using GridSearchCV.\n",
    "    Added streamlit-based frontend for interactive loan approval predictions.\n",
    "    Included model versioning with MLflow for tracking experiments.\n",
    "    🐛 Bug Fixes / Code Refactoring:\n",
    "    Fixed issue with incorrect encoding of categorical variables (replaced LabelEncoder with OneHotEncoder).\n",
    "    Refactored data loading and preprocessing into modular functions (data_utils.py).\n",
    "    Improved error handling and logging across preprocessing and inference scripts.\n",
    "    📁 Repository Structure Updates:\n",
    "    Created notebooks/, src/, and models/ directories for cleaner project organization.\n",
    "    Added requirements.txt and README.md with setup instructions.\n",
    "    Updated .gitignore to exclude model artifacts and environment files.\n",
    "    📈 Performance Changes:\n",
    "    Validation accuracy improved from ~78% to 84% with model tuning and feature engineering.\n",
    "    Reduced training time by 20% after optimizing preprocessing and model pipeline.\n",
    "    📌 Commit Comparison Highlights:\n",
    "    Compared commits: a1c2b3d (old baseline model) → d4e5f6g (latest tuned system).\n",
    "    Major differences:\n",
    "    Introduction of new ML models and evaluation metrics.\n",
    "    UI integration for real-time prediction.\n",
    "    Codebase modularization and documentation improvements.'''\n",
    "\n",
    "    input2 = '''Structure & Organization\n",
    "    V1: Single Jupyter notebook; all logic inline.\n",
    "    V2: Modular scripts (data_utils.py, model.py, app.py); clean folder structure.\n",
    "    Inference: Shift from exploratory to production-grade code.\n",
    "    🧮 Data Preprocessing\n",
    "    V1: Basic null handling and LabelEncoder.\n",
    "    V2: SimpleImputer, OneHotEncoder, scaling, ColumnTransformer.\n",
    "    Inference: More robust and reusable preprocessing pipeline.\n",
    "    🧠 Feature Engineering\n",
    "    V1: Used raw features.\n",
    "    V2: Added domain-driven features (e.g., debt-to-income ratio, loan amount bins).\n",
    "    Inference: Better input representation, likely improved model performance.\n",
    "    🤖 Modeling\n",
    "    V1: Logistic Regression, no tuning.\n",
    "    V2: Added Random Forest, XGBoost, and GridSearchCV.\n",
    "    Inference: More powerful models with hyperparameter optimization.\n",
    "    📊 Evaluation\n",
    "    V1: Accuracy only.\n",
    "    V2: Precision, Recall, F1, AUC, confusion matrix.\n",
    "    Inference: Deeper insight into performance, especially for imbalanced classes.\n",
    "    🌐 Deployment/UI\n",
    "    V1: No interface\n",
    "    V2: Streamlit app for real-time predictions.\n",
    "    Inference: User-friendly and deployable.\n",
    "\n",
    "    📈 Experiment Tracking\n",
    "    V1: None.\n",
    "    V2: MLflow used for tracking metrics and versions.\n",
    "    Inference: Enables reproducibility and team collaboration.\n",
    "\n",
    "    ✅ Final Verdict:\n",
    "    V2 demonstrates a professional-grade ML system — modular, explainable, user-facing, and maintainable — a significant upgrade over the initial proof-of-concept in V1.\n",
    "    '''\n",
    "   \n",
    "    input3 = '''Connecting\n",
    "    🧠 AI Feature Mapping Validator\n",
    "    Compare functionalities between a Whitepaper and its Codebase\n",
    "    📄 Upload Whitepaper File\n",
    "\n",
    "    white paper.txt\n",
    "    Drag and drop file here\n",
    "    Limit 200MB per file • TXT, MD, PDF\n",
    "    white paper.txt\n",
    "    329.0B\n",
    "    💻 Upload Code File\n",
    "\n",
    "    model.ipynb\n",
    "    Drag and drop file here\n",
    "    Limit 200MB per file • PY, TXT, IPYNB\n",
    "    model.ipynb\n",
    "    8.9KB\n",
    "\n",
    "    ⚖️ Comparing Functionalities\n",
    "    To perform a comprehensive comparison between the whitepaper and the code functionalities, let's break down the information provided and identify any discrepancies or updates needed:\n",
    "\n",
    "    Comparison of Features:\n",
    "    Whitepaper Features:\n",
    "\n",
    "    Sepal width (cm)\n",
    "    Petal length (cm)\n",
    "    Petal width (cm)\n",
    "    Code Features:\n",
    "\n",
    "    Sepal length (cm)\n",
    "    Sepal width (cm)\n",
    "    Petal length (cm)\n",
    "    Petal width (cm)\n",
    "    Missing Feature in Whitepaper:\n",
    "\n",
    "    Sepal length (cm) is used in the code but not mentioned in the whitepaper.\n",
    "    Comparison of Model:\n",
    "    Both the whitepaper and the code use Logistic Regression. There is no discrepancy here.\n",
    "    Comparison of Validation Metrics:\n",
    "    Whitepaper Metrics:\n",
    "\n",
    "    Accuracy: 90%\n",
    "    Precision: 90%\n",
    "    Recall: 85%\n",
    "    F1 Score: 88%\n",
    "    Code Metrics:\n",
    "\n",
    "    Accuracy: 96.67%\n",
    "    Precision: 96.67%\n",
    "    Recall: 96.67%\n",
    "    F1 Score: 96.67%\n",
    "    Comparison of Scores:\n",
    "\n",
    "    The code metrics are higher than those specified in the whitepaper. This indicates that the model performs better than the expectations set in the whitepaper.\n",
    "    Critical Validation Metrics:\n",
    "    The critical metrics (Accuracy, Precision, Recall, F1 Score) in the code are all greater than those in the whitepaper.\n",
    "    Conclusion:\n",
    "    Feature Discrepancy: The whitepaper is missing the feature \"Sepal length (cm)\" which is used in the code.\n",
    "    Validation Metrics Discrepancy: The code achieves higher validation metrics than those specified in the whitepaper.\n",
    "    Action Required:\n",
    "\n",
    "    White Paper Update Needed: The whitepaper is not updated. Please update the document to include the missing feature \"Sepal length (cm)\" and revise the validation metrics to reflect the improved performance of the model as demonstrated in the code.\n",
    "    This analysis ensures that the documentation accurately reflects the implementation and performance of the model.\n",
    "    '''\n",
    " \n",
    "    result = CreateReport(input1, input2, input3)\n",
    "    # \n",
    "    # print(\"=== Summary ===\")\n",
    "    # print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6abc4099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# AI System Evaluation Report\n",
       "\n",
       "## 1. Validation Metrics\n",
       "\n",
       "- **Accuracy**: Improved from ~78% to 84% after enhancements.\n",
       "- **Precision, Recall, F1 Score, AUC**: Newly introduced metrics provide a comprehensive evaluation of model performance, especially for imbalanced classes.\n",
       "- **Strengths**:\n",
       "  - Significant improvement in accuracy and overall model performance due to feature engineering and model tuning.\n",
       "  - Introduction of multiple evaluation metrics offers deeper insights into model performance.\n",
       "- **Weaknesses**:\n",
       "  - No specific weaknesses identified in the validation metrics; however, continuous monitoring is recommended to ensure consistent performance across different datasets.\n",
       "\n",
       "## 2. Code Comparison Inferences\n",
       "\n",
       "- **Implemented Code vs. Described Goals**:\n",
       "  - **Alignments**:\n",
       "    - The codebase now includes modular scripts and a clean folder structure, aligning with best practices for production-grade systems.\n",
       "    - Feature engineering and preprocessing enhancements align with the goals of improving model input representation.\n",
       "    - The introduction of Random Forest and XGBoost models, along with hyperparameter tuning, aligns with the goal of improving model performance.\n",
       "    - The addition of a Streamlit app for real-time predictions aligns with the goal of creating a user-friendly interface.\n",
       "    - MLflow integration for experiment tracking aligns with the goal of enhancing reproducibility and collaboration.\n",
       "  - **Discrepancies**:\n",
       "    - The whitepaper lacks mention of the \"Sepal length (cm)\" feature used in the code.\n",
       "    - Validation metrics in the code exceed those specified in the whitepaper, indicating a need for documentation updates.\n",
       "- **Improvements/Regressions**:\n",
       "  - The system has improved significantly in terms of modularity, explainability, and user interaction compared to the initial version.\n",
       "  - No regressions identified; the system has evolved from a proof-of-concept to a professional-grade ML system.\n",
       "\n",
       "## 3. Recommendations\n",
       "\n",
       "- **Documentation Updates**:\n",
       "  - Update the whitepaper to include the \"Sepal length (cm)\" feature and revise validation metrics to reflect the improved performance.\n",
       "- **Model and Feature Enhancements**:\n",
       "  - Continue exploring additional feature engineering techniques to further enhance model performance.\n",
       "  - Consider implementing additional models or ensemble methods to further improve accuracy and robustness.\n",
       "- **System Robustness and Consistency**:\n",
       "  - Regularly validate the model on new datasets to ensure consistent performance.\n",
       "  - Implement automated testing for data preprocessing and model inference to catch potential errors early.\n",
       "- **User Interface and Deployment**:\n",
       "  - Gather user feedback on the Streamlit app to identify areas for improvement in user experience.\n",
       "  - Explore deployment options to scale the application for larger user bases if needed.\n",
       "\n",
       "By following these recommendations, the system can maintain its high performance and continue to evolve in response to user needs and technological advancements."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
