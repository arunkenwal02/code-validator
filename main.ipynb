{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ac416dc",
   "metadata": {},
   "source": [
    "## Check Docs Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e18532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from IPython.display import Markdown, display\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "import nbformat\n",
    "import streamlit as st\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913e4307",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. Load White Paper  (PDF)  \n",
    "2. Vectors Embeddings - Text and tables; later include images\n",
    "3. Chroma db\n",
    "4. Retrival - (Accuracy)\n",
    "5. Generation - \n",
    "    Validation-\n",
    "        Data sources - List all sources and metadata\n",
    "        Features - detect any change in features\n",
    "        Changes in Transformation steps\n",
    "        Model Details \n",
    "        Hyperparameter\n",
    "        List of Validation Metrics and resepctive scores\n",
    "        Compare Validation scores of white paper with model's validation scores\n",
    "        Brief of comparision of scores\n",
    "\n",
    "        List and Track of critical metrics - these should not be lower than mentioned (in white paper)\n",
    "\n",
    "6. Respective scores for tracked metrics (confidence on generation)\n",
    "7. If required update prompt and go back to step 4 and reiterate step 4 and 5. reason (geneation have \n",
    "   heiger confidence )\n",
    "8. Outout should be in structured format (This will be input for summary block with affitional 2 inputs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1358c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\",  \n",
    "                 temperature=0,\n",
    "                 openai_api_key= openai_api_key)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f30bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_notebook(file_path):\n",
    "    \"\"\"Read .ipynb notebook and extract content.\"\"\"\n",
    "    nb = nbformat.read(file_path, as_version=4)\n",
    "    content = []\n",
    "    for cell in nb.cells:\n",
    "        if cell.cell_type == 'markdown':\n",
    "            content.append(\"## Markdown Cell:\\n\" + cell.source)\n",
    "        elif cell.cell_type == 'code':\n",
    "            content.append(\"## Code Cell:\\n```python\\n\" + cell.source + \"\\n```\")\n",
    "    return \"\\n\\n\".join(content)\n",
    "\n",
    "\n",
    "def read_file(file_path):\n",
    "    \"\"\"Reads the content of a file.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def extract_functionalities_from_code(notebook_content):\n",
    "\n",
    "    \"\"\"Uses LLM to extract functionalities from Python code.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert Python code reviewer. Here is a Jupyter notebook:\n",
    "    {notebook_content}\n",
    "\n",
    "    The following is a Jupyter notebook content (code and markdown). \n",
    "    Please extract the following:\n",
    "    Analyze the notebook and answer:\n",
    "\n",
    "    1. List of features used in the model.\n",
    "    2. Name/type of ML model used, only name of model\n",
    "    3. Accuracy metrics (e.g., accuracy, F1, precision, recall, AUC, etc.), only metrics name. \n",
    "    4. What is the purpose of this notebook?\n",
    "    5. What are the main operations and their results?\n",
    "    6. Are there any errors or anomalies in outputs?\n",
    "    7. What conclusions can be drawn from the outputs?\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke([SystemMessage(content=\"You are a helpful assistant.\"), HumanMessage(content=prompt)])\n",
    "\n",
    "    return response.content.strip()\n",
    "\n",
    "\n",
    "def extract_functionalities_from_whitepaper(whitepaper_text):\n",
    "    \"\"\"Uses LLM to extract functionalities from whitepaper.\"\"\"\n",
    "    prompt = [\n",
    "        SystemMessage(content=\"You are a product analyst.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Here is the whitepaper or product requirement document:\n",
    "\n",
    "        {whitepaper_text}\n",
    "\n",
    "        List all functionalities or features the whitepaper mentions. Use bullet points.\n",
    "        \"\"\")\n",
    "            ]\n",
    "    return llm(prompt).content.strip()\n",
    "\n",
    "\n",
    "def compare_functionalities(whitepaper_funcs, code_funcs):\n",
    "    \"\"\"Compares two sets of functionalities using the LLM.\"\"\"\n",
    "    prompt = [\n",
    "        SystemMessage(content=\"You are a software QA expert.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Whitepaper Functionalities:\n",
    "        {whitepaper_funcs}\n",
    "\n",
    "        Code Functionalities:\n",
    "        {code_funcs}\n",
    "        Extract validation metrics from code funcs eg, precision, recall and other validation are in output cell.\n",
    "        Compare the two lists and identify which functionalities from the whitepape, if functionality is implemented in code but not available in white paper, print: white paper is not updated please update the document. and show details of each section \n",
    "        listmissing sections like feature and if model varies according to white paper and same for validation metrics.\n",
    "        compare validation scores : Compare scores of code function with white paper.\n",
    "        Also compare critical validation metrics: make sure critical metrics of code should be grater then white paper critical metrics\n",
    "        if thereis no changhe in metrics of docuemt and code_funcs: keep output 'white paper is updated please proceed to next steps. no other information is required'  \n",
    "        \n",
    "        \"\"\")\n",
    "            ]\n",
    "    return llm(prompt).content.strip()\n",
    "\n",
    "\n",
    "def read_notebook_with_outputs(file_path):\n",
    "    \"\"\"Read .ipynb notebook and include both code and output.\"\"\"\n",
    "    nb = nbformat.read(file_path, as_version=4)\n",
    "    cells_content = []\n",
    "\n",
    "    for cell in nb.cells:\n",
    "        if cell.cell_type == 'markdown':\n",
    "            cells_content.append(f\"## Markdown Cell:\\n{cell.source}\")\n",
    "        elif cell.cell_type == 'code':\n",
    "            code = f\"## Code Cell:\\n```python\\n{cell.source}\\n```\"\n",
    "            outputs = []\n",
    "\n",
    "            for output in cell.get(\"outputs\", []):\n",
    "                if output.output_type == \"stream\":\n",
    "                    outputs.append(f\"Output (stream):\\n{output.text}\")\n",
    "                elif output.output_type == \"execute_result\":\n",
    "                    # Display the result of the cell (e.g., print(2+2))\n",
    "                    result = output.get(\"data\", {}).get(\"text/plain\", \"\")\n",
    "                    outputs.append(f\"Output (execute_result):\\n{result}\")\n",
    "                elif output.output_type == \"error\":\n",
    "                    outputs.append(\"Error:\\n\" + \"\\n\".join(output.get(\"traceback\", [])))\n",
    "\n",
    "            full_output = \"\\n\".join(outputs)\n",
    "            if full_output:\n",
    "                code += f\"\\n\\n### Output:\\n```\\n{full_output}\\n```\"\n",
    "            cells_content.append(code)\n",
    "\n",
    "    return \"\\n\\n\".join(cells_content)\n",
    "\n",
    "def read_notebook(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return nbformat.read(f, as_version=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcfe8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    st.set_page_config(page_title=\"Functionality Coverage Checker\", layout=\"wide\")\n",
    "    \n",
    "    st.title(\"🧠 AI Feature Mapping Validator\")\n",
    "    st.subheader(\"Compare functionalities between a Whitepaper and its Codebase\")\n",
    "\n",
    "    uploaded_whitepaper = st.file_uploader(\"📄 Upload Whitepaper File\", type=[\"txt\", \"md\", \"pdf\"])\n",
    "    uploaded_code = st.file_uploader(\"💻 Upload Code File\", type=[\"py\", \"txt\", \"ipynb\"])\n",
    "\n",
    "    if uploaded_whitepaper and uploaded_code:\n",
    "        if st.button(\"Click to Process Files\"):\n",
    "            # Read whitepaper content\n",
    "            whitepaper = uploaded_whitepaper.read().decode(\"utf-8\")\n",
    "\n",
    "            # Handle .ipynb or other code files\n",
    "            if uploaded_code.name.endswith(\".ipynb\"):\n",
    "                # Write the raw content to a temp file\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix=\".ipynb\", mode='wb') as tmp_file:\n",
    "                    tmp_file.write(uploaded_code.read())\n",
    "                    temp_file_path = tmp_file.name\n",
    "\n",
    "                # notebook_contents = read_notebook(temp_file_path)\n",
    "                notebook_contents = read_notebook_with_outputs(temp_file_path)\n",
    "                code_funcs = extract_functionalities_from_code(notebook_contents)\n",
    "            else:\n",
    "                code = uploaded_code.read().decode(\"utf-8\")\n",
    "                code_funcs = extract_functionalities_from_code(code)\n",
    "\n",
    "            whitepaper_funcs = extract_functionalities_from_whitepaper(whitepaper)\n",
    "\n",
    "            st.markdown(\"### ⚖️ Comparing Functionalities\")\n",
    "            missing_funcs = compare_functionalities(whitepaper_funcs, code_funcs)\n",
    "            st.markdown(missing_funcs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c540a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29f7a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f1b071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c25394f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a51519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import nbformat\n",
    "# from openai import OpenAI\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.schema import SystemMessage, HumanMessage\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.schema import SystemMessage, HumanMessage\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# from dotenv import load_dotenv\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# import chromadb\n",
    "# from chromadb.config import Settings\n",
    "\n",
    "# import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3769bc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries = [\n",
    "#             \"Summary/Objective of white paper \",\n",
    "#             \"Features mentioned\",\n",
    "#             \"Preprocessing steps and data transformation steps\",\n",
    "#             \"Model selected for classification\",\n",
    "#             \"Training and resting methodology\",\n",
    "#             \"List of Hyper parameters and respective values\",\n",
    "#             \"What are list of validation scores and the performance scores?\",\n",
    "#             \"Ethical considerations\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b706715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryFun(query, embedding_model,collection):\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    l_docs = []\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=5)\n",
    "    for doc in results[\"documents\"][0]:\n",
    "        l_docs.append(doc)\n",
    "        # print(\"🔎 Match:\", l_docs.append(doc))\n",
    "    return l_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81786eca",
   "metadata": {},
   "source": [
    "##  Check Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4fc45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb import PersistentClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723ab5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./chroma_openai1\"\n",
    "\n",
    "# Step 1: Load the persistent client\n",
    "chroma_client = PersistentClient(path=path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935ed8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "collections = chroma_client.list_collections()\n",
    "for col in collections:\n",
    "    print(col.name)\n",
    "    # print(col.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e06b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"notebook_f887cf79f48bf8b101631e9ebdb3ca7220bd2c6e47a6b82041ca192aa98cf16b\"\n",
    "# Step 2: Access the existing collection\n",
    "collection = chroma_client.get_collection(name=collection_name)\n",
    "data = collection.get()\n",
    "\n",
    "# Optional: View details\n",
    "print(\"IDs:\", data['ids'])\n",
    "print(\"Documents:\", data['documents'][:2])  # print only first 2 docs\n",
    "print(\"Metadata:\", data.get('metadatas'))  # only if metadata was stored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418da824",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7484104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: View details\n",
    "print(\"IDs:\", data['ids'])\n",
    "print(\"Documents:\", data['documents'][:2])  # print only first 2 docs\n",
    "print(\"Metadata:\", data.get('metadatas'))  # only if metadata was stored\n",
    "print(\"Embeddings:\", data['embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211070c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6279493b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253ac37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.get_collection(\"whitepaper_02958f41437b5bbcf9490a38b0edb5d41a365101ce7979d2822648a320dfdc73\")\n",
    "\n",
    "results = collections[6].get(\n",
    "    ids=[\"id1\", \"id2\"],    # optional\n",
    "    where={\"type\": \"pdf\"}, # optional\n",
    "    include=[\"documents\", \"metadatas\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1181a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d63925d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Embedding stored or not \n",
    "import os\n",
    "import nbformat\n",
    "from openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "import streamlit as st\n",
    "import fitz\n",
    "import tempfile\n",
    "import hashlib\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b40a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_hash(uploaded_file):\n",
    "    uploaded_file.seek(0)\n",
    "    hash_val = hashlib.sha256(uploaded_file.read()).hexdigest()\n",
    "    uploaded_file.seek(0)\n",
    "    return hash_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85886d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_pdf(uploaded_file):\n",
    "    doc = fitz.open(stream=uploaded_file.read(), filetype=\"pdf\")\n",
    "    extracted_text = \"\"\n",
    "    for page_num, page in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "        extracted_text += f\"\\n\\n--- Page {page_num + 1} ---\\n{text}\"\n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537bf1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collection_exists(collection_name):\n",
    "    try:\n",
    "        chroma_client.get_collection(collection_name)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60a085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=750,     # faster, smaller chunk\n",
    "        chunk_overlap=100   # reduced overlap\n",
    "    )\n",
    "    return text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b051a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_or_create_embeddings(uploaded_file, text, _embedding_model, collection_name):\n",
    "    chunks = create_chunks(text)\n",
    "\n",
    "    if collection_exists(collection_name):\n",
    "        collection = chroma_client.get_collection(collection_name)\n",
    "    else:\n",
    "        embeddings = _embedding_model.embed_documents(chunks)\n",
    "        print(embeddings)\n",
    "        collection = store_in_chromaDB(chunks, embeddings, collection_name)\n",
    "\n",
    "    return collection, chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad320aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import fitz\n",
    "\n",
    "def get_file_hash(file_obj):\n",
    "    file_obj.seek(0)\n",
    "    hash_val = hashlib.sha256(file_obj.read()).hexdigest()\n",
    "    file_obj.seek(0)\n",
    "    return hash_val\n",
    "\n",
    "def extract_from_pdf(file_obj):\n",
    "    file_obj.seek(0)\n",
    "    doc = fitz.open(stream=file_obj.read(), filetype=\"pdf\")\n",
    "    extracted_text = \"\"\n",
    "    for page_num, page in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "        extracted_text += f\"\\n\\n--- Page {page_num + 1} ---\\n{text}\"\n",
    "    return extracted_text\n",
    "\n",
    "file_path = \"Load Prediction Whitepaper.pdf\"\n",
    "\n",
    "with open(file_path, \"rb\") as f:\n",
    "    # 1. Compute hash\n",
    "    file_hash = get_file_hash(f)\n",
    "    # 2. Extract text\n",
    "    text = extract_from_pdf(f)\n",
    "    # 3. If needed, reset pointer for further use (not always needed)\n",
    "    f.seek(0)\n",
    "    # 4. Pass to embedding function (if needed)\n",
    "    collection, chunks = get_or_create_embeddings(\n",
    "        uploaded_file=f,              # If function needs file object\n",
    "        text=text,                    # If function needs text\n",
    "        _embedding_model=embedding_model,\n",
    "        collection_name=f\"whitepaper_{file_hash}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec64fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb95ef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_of_events():\n",
    "    # with open(\"push_events.json\", \"r\") as file:\n",
    "    #     data = json.load(file)\n",
    "    # owner = data[2]\n",
    "    # repo_name = data[3]\n",
    "    api_url = f\"https://api.github.com/repos/arunkenwal02/code-validator/events\"\n",
    "    response = requests.get(api_url)\n",
    "    events = response.json()\n",
    "    push_ids = [id['id'] for id in events]\n",
    "    print(push_ids)\n",
    "    data = ['52949273211','52821120274']\n",
    "    push_events = [e for e in events if e['type'] == 'PushEvent']\n",
    "\n",
    "    ids = [e['id'] for e in push_events]\n",
    "    try:\n",
    "        idx1 = ids.index(data[0])\n",
    "        idx2 = ids.index(data[1])\n",
    "    except ValueError:\n",
    "        return {\"error\": \"One or both push IDs not found in recent events.\"}\n",
    "\n",
    "    start = min(idx1, idx2)\n",
    "    end = max(idx1, idx2)\n",
    "\n",
    "    history_between = push_events[start:end+1]  \n",
    "    grouped_push_events = []\n",
    "    commits_list = []\n",
    "    for event in history_between:\n",
    "        push_id = event['id']\n",
    "        created_at = event['created_at']\n",
    "        repo = event['repo']['name']\n",
    "        commits_list = []\n",
    "\n",
    "        for commit in event[\"payload\"][\"commits\"]:\n",
    "            sha = commit['sha']\n",
    "            author = commit['author']['name']\n",
    "            message = commit['message']\n",
    "\n",
    "            commit_detail_url = f\"https://api.github.com/repos/arunkenwal02/code-validator/commits/{sha}\"\n",
    "            commit_detail_response = requests.get(commit_detail_url)\n",
    "\n",
    "            if commit_detail_response.status_code != 200:\n",
    "                diff = \"❌ Failed to fetch diff\"\n",
    "            else:\n",
    "                commit_detail = commit_detail_response.json()\n",
    "                diffs = []\n",
    "                for file in commit_detail.get('files', []):\n",
    "                    patch = file.get('patch')\n",
    "                    if patch:\n",
    "                        diffs.append(f\"File: {file['filename']}\\n{patch}\")\n",
    "                diff = \"\\n\\n\".join(diffs) if diffs else \"No diff available\"\n",
    "\n",
    "            commits_list.append({\n",
    "                \"sha\": sha,\n",
    "                \"author\": author,\n",
    "                \"commit_message\": message,\n",
    "                \"code_diff\": diff\n",
    "            })\n",
    "\n",
    "        grouped_push_events.append({\n",
    "            \"push_id\": push_id,\n",
    "            \"repo\": repo,\n",
    "            \"created_at\": created_at,\n",
    "            \"commits\": commits_list\n",
    "        })\n",
    "    return grouped_push_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdfe6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = create_summary_of_events()\n",
    "test\n",
    "push id , commit summary, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a64e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499acb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc3313b",
   "metadata": {},
   "source": [
    "## One drive file access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bacde4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import msal\n",
    "import requests\n",
    "import time\n",
    "import fitz\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv() \n",
    "Permission_ID =\"6a94cb3a-9869-4b54-ae0b-f4f523df2614\"\n",
    "client_id = Permission_ID\n",
    "authority = \"https://login.microsoftonline.com/consumers\"\n",
    "scopes = [\"Files.Read\"]\n",
    "source_folder = \"Documents/GitHub/code-validator/\"\n",
    "file_name = \"Load Prediction Whitepaper.pdf\"\n",
    "version_id = int(7)\n",
    "file_path = source_folder+file_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b81431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract updated version \n",
    "\n",
    "def access_token_key(client_id, authority):\n",
    "    scopes = [\"Files.Read\"]\n",
    "    app = msal.PublicClientApplication(client_id=client_id, authority=authority)\n",
    "    result = None\n",
    "\n",
    "    accounts = app.get_accounts()\n",
    "    if accounts:\n",
    "        result = app.acquire_token_silent(scopes, account=accounts[0])\n",
    "    if not result:\n",
    "        result = app.acquire_token_interactive(scopes=scopes)\n",
    "    if not result or \"access_token\" not in result:\n",
    "        print(\"MSAL Error:\", result)\n",
    "    access_token = result[\"access_token\"]\n",
    "\n",
    "    return access_token\n",
    "\n",
    "def get_raw_data(client_id, authority ,file_path ):\n",
    "    access_token= access_token_key(client_id=client_id, authority=authority)\n",
    "    url = f\"https://graph.microsoft.com/v1.0/me/drive/root:/{file_path}:/content\"\n",
    "    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "    time.sleep(2)\n",
    "    response = requests.get(url, headers=headers)\n",
    "    print(f\"Response code: {response.status_code}\")\n",
    "    if response.status_code == 200:\n",
    "        file_bytes = response.content\n",
    "        print(\"File read into memory!\")\n",
    "        return file_bytes\n",
    "    else:\n",
    "        print(\"Failed:\", response.status_code, response.text)\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_onedrive_whitepaper(file_bytes):\n",
    "    \n",
    "    # file_bytes is from above\n",
    "    doc = fitz.open(stream=file_bytes, filetype=\"pdf\")\n",
    "    text = \"\"\n",
    "    for page_num, page in enumerate(doc):\n",
    "        text += f\"\\n\\n--- Page {page_num + 1} ---\\n{page.get_text()}\"\n",
    "\n",
    "    print(\"First 1000 chars of PDF text:\", text)\n",
    "    \n",
    "    return text\n",
    " \n",
    "   \n",
    "def prev_version( client_id, authority, file_path, version_id):\n",
    "    access_token= access_token_key(client_id=client_id, authority=authority)\n",
    "\n",
    "    versions_url = f\"https://graph.microsoft.com/v1.0/me/drive/root:/{file_path}:/versions\"\n",
    "    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "    response = requests.get(versions_url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        versions = response.json()[\"value\"]\n",
    "        if len(versions) >= int(version_id):\n",
    "            # 3. Get the 2nd version (index 1)\n",
    "            version_id = versions[1]['id']\n",
    "            print(f\"2nd Version ID: {version_id}, Last Modified: {versions[1]['lastModifiedDateTime']}\")\n",
    "            \n",
    "            # 4. Fetch 2nd version's PDF bytes\n",
    "            download_url = f\"https://graph.microsoft.com/v1.0/me/drive/root:/{file_path}:/versions/{version_id}/content\"\n",
    "            version_response = requests.get(download_url, headers=headers)\n",
    "            if version_response.status_code == 200:\n",
    "                pdf_bytes = version_response.content  # This is your PDF in memory\n",
    "                \n",
    "                # 5. Extract text from the PDF (in memory, no save)\n",
    "                doc = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
    "                all_text = \"\"\n",
    "                for page_num, page in enumerate(doc):\n",
    "                    all_text += f\"\\n--- Page {page_num+1} ---\\n{page.get_text()}\"\n",
    "                \n",
    "                print(\"Extracted PDF text (first 1000 chars):\")\n",
    "                print(all_text[:1000])\n",
    "                return all_text\n",
    "                # You can use `all_text` as needed (search, LLM input, etc)\n",
    "            else:\n",
    "                print(\"Failed to download 2nd version:\", version_response.status_code, version_response.text)\n",
    "        else:\n",
    "            print(\"Less than 2 versions available!\")\n",
    "    else:\n",
    "        print(\"Failed to fetch versions:\", response.status_code, response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c4bc5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response code: 200\n",
      "File read into memory!\n",
      "First 1000 chars of PDF text: \n",
      "\n",
      "--- Page 1 ---\n",
      "Machine Learning-Based Loan Approval Prediction System\n",
      "for Financial Institutions\n",
      "1. Executive Summary\n",
      "The financial services industry faces a critical challenge in automating and de-risking\n",
      "the loan approval process. Traditional methods, often relying on manual review and\n",
      "static rule-based systems, are prone to human error, inconsistency, and significant\n",
      "processing delays. These limitations result in missed opportunities, elevated credit risk,\n",
      "and suboptimal customer experiences. To address these issues, we have developed a\n",
      "robust machine learning-based Loan Approval Classification System. This model\n",
      "leverages a comprehensive set of applicant data to predict the likelihood of loan\n",
      "repayment, classifying applications as either approved or rejected.\n",
      "Our system is designed to provide a high-level overview of an applicant's\n",
      "creditworthiness, offering a data-driven, objective, and transparent decision-making tool.\n",
      "By integrating advanced machine learning techniques, our model achieves superior\n",
      "predictive accuracy compared to traditional methods. It significantly reduces the time\n",
      "from application to decision, minimizes the risk of default, and ensures a consistent, fair\n",
      "evaluation process. This strategic asset not only enhances operational efficiency but\n",
      "also provides a competitive advantage by enabling faster, more confident lending\n",
      "decisions.\n",
      "2. Introduction\n",
      "The process of loan approval is a cornerstone of the financial industry. It involves\n",
      "evaluating a multitude of factors to determine an applicant's creditworthiness and ability\n",
      "to repay a loan. Historically, this process has been labor-intensive, relying on credit\n",
      "officers to manually review application forms, financial statements, and credit reports.\n",
      "This manual approach is slow, expensive, and susceptible to biases. The rise of digital\n",
      "banking and the demand for instant financial services have made this traditional model\n",
      "increasingly unsustainable.\n",
      "This white paper details a machine learning-based solution designed to modernize and\n",
      "optimize the loan approval workflow. By building a classification model, our system can\n",
      "accurately predict the Loan_Status (Approved or Rejected) for new applications. The\n",
      "primary motivation for this project is to create a scalable, efficient, and fair system that\n",
      "can process thousands of applications in real-time, reducing operational costs while\n",
      "simultaneously improving the quality of lending decisions. Our model is intended to\n",
      "serve as a decision-support tool for loan officers, enabling them to focus on complex\n",
      "cases and customer relationships rather than routine data analysis.\n",
      "\n",
      "\n",
      "--- Page 2 ---\n",
      "3. Related Work / Literature Review\n",
      "The field of credit scoring and loan prediction has seen extensive research and\n",
      "application of various machine learning models. A number of algorithms have been\n",
      "employed to analyze applicant data and forecast loan outcomes:\n",
      "·\n",
      "Logistic Regression: A statistical method used for binary classification, which is\n",
      "well-suited for predicting a 'yes' or 'no' outcome for loan approval. It is valued for\n",
      "its simplicity and the interpretability of its results, as it shows how different factors\n",
      "influence the final decision.\n",
      "·\n",
      "Decision Trees: These models use a tree-like structure of decisions and their\n",
      "possible consequences. They are easy to understand and visualize, as they\n",
      "mimic human decision-making processes.\n",
      "\n",
      "\n",
      "--- Page 3 ---\n",
      "·\n",
      "Random Forests: An ensemble method that builds multiple decision trees and\n",
      "combines their predictions to improve accuracy and reduce overfitting.\n",
      "·\n",
      "Steps of ML Algorithms:\n",
      "\n",
      "\n",
      "--- Page 4 ---\n",
      "4. Data Description\n",
      "The model is trained on a comprehensive dataset of past loan applications. The dataset\n",
      "contains a mix of demographic, financial, and behavioral features.\n",
      "Feature Name and Definition:\n",
      "·\n",
      "Loan_ID: A unique identifier for each loan application.\n",
      "·\n",
      "Gender: The applicant's gender (Male/Female).\n",
      "·\n",
      "Married: Marital status of the applicant (Yes/No).\n",
      "·\n",
      "Dependents: Number of dependents the applicant has.\n",
      "·\n",
      "Education: Applicant's education level (Graduate/Not Graduate).\n",
      "·\n",
      "Self_Employed: Whether the applicant is self-employed (Yes/No).\n",
      "·\n",
      "Applicant_Income: The applicant's monthly income.\n",
      "·\n",
      "Coapplicant_Income: The co-applicant's monthly income.\n",
      "·\n",
      "Loan_Amount: The amount of the loan requested.\n",
      "·\n",
      "Loan_Amount_Term: The term of the loan in months.\n",
      "·\n",
      "Credit_History: A binary variable indicating if the applicant has a good credit\n",
      "history (1.0) or not (0.0). This is a critical predictor.\n",
      "·\n",
      "Property_Area: The area where the property is located\n",
      "(Rural/Semiurban/Urban).\n",
      "·\n",
      "Loan_Status: The target variable, indicating if the loan was approved (Y) or\n",
      "rejected (N).\n",
      "Preprocessing Steps:\n",
      "1.\n",
      "Handling Missing Values: Missing values are common in real-world data. We\n",
      "employ different strategies based on the feature type:\n",
      "o\n",
      "Categorical Features: Missing values in Gender, Married, Dependents,\n",
      "Self_Employed, and Credit_History are imputed using the mode (most\n",
      "frequent value) of the respective columns.\n",
      "o\n",
      "Numerical Features: Missing values in Loan_Amount and\n",
      "Loan_Amount_Term are imputed using the mean or median to avoid\n",
      "skewing the distribution.\n",
      "1.\n",
      "Data Type Consistency: All features are checked for consistent data types.\n",
      "Numerical features are stored as integers or floats, while categorical features are\n",
      "stored as strings or object types.\n",
      "2.\n",
      "Data Balancing: An analysis of the Loan_Status target variable revealed an\n",
      "imbalance, with a significantly higher number of approved loans than rejected\n",
      "ones. This imbalance can bias a model to favor the majority class. Imbalance\n",
      "\n",
      "\n",
      "--- Page 5 ---\n",
      "data handled by ‘class_weight’ = ‘balanced’ parameter in Logistic Regression\n",
      "model.\n",
      "𝑤𝑖= 𝑘⋅𝑛𝑖\n",
      "𝑛\n",
      "§\n",
      "wᵢis the weight for class/sample i\n",
      "§\n",
      "k is a constant (e.g., total desired sample size or scaling factor)\n",
      "§\n",
      "nᵢis the count/frequency of class i\n",
      "§\n",
      "n is the total number of samples\n",
      "o\n",
      "Synthetic Minority Over-sampling Technique (SMOTE) during the\n",
      "training phase to create synthetic data points for the minority class,\n",
      "ensuring the model is not biased and can accurately identify both\n",
      "approved and rejected applications.\n",
      "3.\n",
      "Data Transformation:\n",
      "o\n",
      "Numerical Columns: We apply a normalization technique (e.g.,\n",
      "StandardScaler) to numerical columns (Applicant_Income,\n",
      "Coapplicant_Income, Loan_Amount) to ensure they have a zero mean\n",
      "and unit variance. This prevents features with larger magnitudes from\n",
      "dominating the model's training process.\n",
      "o\n",
      "Categorical Features: We use One-Hot Encoding to convert categorical\n",
      "features (Gender, Married, Education, etc.) into a numerical format\n",
      "suitable for the model. This creates new binary columns for each unique\n",
      "category, avoiding the assumption of ordinality that simple label encoding\n",
      "might introduce.\n",
      "5. Model Architecture\n",
      "Why Logistic Regression: Preferred choice\n",
      "We selected Logistic Regression for the loan approval model because it provides high\n",
      "transparency, explainability, and auditability — essential factors in regulated financial\n",
      "environments. While other models like Random Forest marginally outperform it in\n",
      "accuracy, Logistic Regression allows us to clearly communicate how each feature\n",
      "contributes to the final decision, enabling easier compliance with fairness, bias\n",
      "detection, and model governance requirements\n",
      "·\n",
      "Operating in a highly regulated environment (e.g. banking, insurance)\n",
      "·\n",
      "Need to explain decisions to compliance officers or regulators\n",
      "·\n",
      "Wanted easy-to-track fairness or bias metrics\n",
      "·\n",
      "Prioritize transparency over marginal gains in accuracy\n",
      "\n",
      "\n",
      "--- Page 6 ---\n",
      "Feature Name\n",
      "Type\n",
      "Gender\n",
      "Categorical\n",
      "Married\n",
      "Categorical\n",
      "Dependents\n",
      "Categorical\n",
      "Education\n",
      "Categorical\n",
      "Self_Employed\n",
      "Categorical\n",
      "ApplicantIncome\n",
      "Numerical\n",
      "CoapplicantIncome\n",
      "Numerical\n",
      "LoanAmount\n",
      "Numerical\n",
      "Loan_Amount_Term\n",
      "Numerical\n",
      "Credit_History\n",
      "Binary (0 or 1)\n",
      "Property_Area\n",
      "Categorical\n",
      "General Equation of Logistic Regression\n",
      "𝑃𝑌= 1 =\n",
      "1\n",
      "1 + 𝑒−𝑍\n",
      "𝑍= 𝛽0 + 𝛽1𝑋1 + 𝛽2𝑋2 +⋯+ 𝛽𝑛𝑋𝑛\n",
      "𝑃𝑌= 1 = 𝑝𝑟𝑜𝑏𝑎𝑏𝑖𝑙𝑖𝑡𝑦 𝑜𝑓 𝑙𝑜𝑎𝑛 𝑎𝑝𝑝𝑟𝑜𝑣𝑎𝑙\n",
      "𝛽0 = 𝑖𝑛𝑡𝑒𝑟𝑐𝑒𝑝𝑡 (𝑏𝑖𝑎𝑠 𝑡𝑒𝑟𝑚)\n",
      "𝛽𝑖= 𝑐𝑜𝑒𝑓𝑓𝑖𝑐𝑖𝑒𝑛𝑡 𝑓𝑜𝑟 𝑓𝑒𝑎𝑡𝑢𝑟𝑒 𝑋𝑖\n",
      "𝑍= 𝛽0 + 𝛽1 ⋅𝐺𝑒𝑛𝑑𝑒𝑟+ 𝛽2 ⋅𝑀𝑎𝑟𝑟𝑖𝑒𝑑+ 𝛽3 ⋅𝐷𝑒𝑝𝑒𝑛𝑑𝑒𝑛𝑡𝑠+ 𝛽4 ⋅𝐸𝑑𝑢𝑐𝑎𝑡𝑖𝑜𝑛+ 𝛽5 ⋅\n",
      "𝑆𝑒𝑙𝑓_𝐸𝑚𝑝𝑙𝑜𝑦𝑒𝑑+ 𝛽6 ⋅𝐴𝑝𝑝𝑙𝑖𝑐𝑎𝑛𝑡𝐼𝑛𝑐𝑜𝑚𝑒+ 𝛽7 ⋅𝐶𝑜𝑎𝑝𝑝𝑙𝑖𝑐𝑎𝑛𝑡𝐼𝑛𝑐𝑜𝑚𝑒+ 𝛽8 ⋅𝐿𝑜𝑎𝑛𝐴𝑚𝑜𝑢𝑛𝑡+ 𝛽9 ⋅\n",
      "𝐿𝑜𝑎𝑛_𝐴𝑚𝑜𝑢𝑛𝑡_𝑇𝑒𝑟𝑚+ 𝛽10 ⋅𝐶𝑟𝑒𝑑𝑖𝑡_𝐻𝑖𝑠𝑡𝑜𝑟𝑦+ 𝛽11 ⋅𝑃𝑟𝑜𝑝𝑒𝑟𝑡𝑦_𝐴𝑟𝑒𝑎_𝑈𝑟𝑏𝑎𝑛+ 𝛽12 ⋅\n",
      "𝑃𝑟𝑜𝑝𝑒𝑟𝑡𝑦_𝐴𝑟𝑒𝑎_𝑆𝑒𝑚𝑖𝑢𝑟𝑏𝑎𝑛\n",
      "6. Training Methodology\n",
      "The training process is meticulously designed to ensure the model is robust, accurate,\n",
      "and ready for production.\n",
      "1.\n",
      "Data Split:\n",
      "·\n",
      "Dataset split into three parts:\n",
      "o\n",
      "Training set: 70%\n",
      "o\n",
      "Testing set: 30%\n",
      "·\n",
      "Training set purpose: Used to train the model.\n",
      "·\n",
      "Validation set purpose: Used for hyperparameter tuning and model selection.\n",
      "\n",
      "\n",
      "--- Page 7 ---\n",
      "·\n",
      "Testing set purpose: Reserved for final, unbiased evaluation of model\n",
      "performance.\n",
      "2.\n",
      "Hyperparameter Tuning: We use techniques like Grid Search or Bayesian\n",
      "Optimization to find the optimal set of hyperparameters for the Logixtic\n",
      "Regression model. Key parameters tuned include\n",
      "o\n",
      "Penalty: Specifies the type of regularization used to avoid overfitting\n",
      "o\n",
      "C: Inverse of regularization strength (i.e., smaller C means stronger\n",
      "regularization).\n",
      "o\n",
      "Solver: Optimization algorithm used for finding model coefficients.\n",
      "o\n",
      "Max_iter: Maximum number of iterations taken by the solver to converge.\n",
      "3.\n",
      "Validation Strategy: We employ k-fold cross-validation during the training\n",
      "phase. The training data is divided into k folds. The model is trained k times,\n",
      "each time using a different fold as the validation set. This robust strategy ensures\n",
      "the model's performance is not specific to a single data split.\n",
      "4.\n",
      "Retraining Pipelines: The model is not a static artifact. It is part of a continuous\n",
      "learning loop. A retraining pipeline is established to periodically retrain the model\n",
      "on new data, typically on a monthly or quarterly basis. This ensures the model\n",
      "remains relevant and its predictions accurate as consumer behavior and\n",
      "economic conditions change.\n",
      "7. Evaluation Metrics\n",
      "To assess the model's performance, we utilize a suite of metrics tailored to the financial\n",
      "domain. A simple accuracy score is often misleading in imbalanced classification\n",
      "problems, so we rely on a more comprehensive set of metrics.\n",
      "Business-Specific Interpretation\n",
      "Scenario\n",
      "Prioritize\n",
      "You want to maximize profit while reducing risky loans\n",
      "Precision, F1-score\n",
      "You want to not miss good applicants\n",
      "Recall, F1-score\n",
      "You’re building a regulatory-compliant, fair model\n",
      "Balanced Accuracy,\n",
      "Fairness metrics\n",
      "You prioritize avoiding defaults (minimize false approvals)\n",
      "High Precision\n",
      "You prioritize financial inclusion (minimize false rejections)\n",
      "High Recall\n",
      "You want a balanced approval system for early model\n",
      "evaluation\n",
      "F1-score\n",
      "\n",
      "\n",
      "--- Page 8 ---\n",
      "1. Our primary objective is to avoid loan defaults, so we prioritize Precision to\n",
      "reduce false approvals.\n",
      "2. To ensure a balanced and inclusive approval system, we use the F1-score,\n",
      "which helps capture both Precision and Recall, ensuring we do not miss eligible\n",
      "applicants.\n",
      "3. Since the dataset is highly imbalanced, we use PR-AUC (Precision-Recall\n",
      "AUC) for a more reliable evaluation of model performance.\n",
      "·\n",
      "Precision: Prioritize avoiding defaults (minimize false approvals)\n",
      "Precision =\n",
      "TruePositives\n",
      "TruePositives + FalsePositives\n",
      "·\n",
      "Recall (Sensitivity): Prioritize financial inclusion (minimize false rejections)\n",
      "Recall =\n",
      "TruePositives\n",
      "TruePositives + FalseNegatives\n",
      "·\n",
      "F1-Score: Wanted balanced approval system for early model evaluation\n",
      "F1 - Score = 2 × Precision × Recall\n",
      "Precision + Recall\n",
      "·\n",
      "PR-AUC (Precision-Recall AUC): In our imbalanced setting, we're more interested\n",
      "in how well the model identifies truly eligible applicants. PR-AUC gives a more\n",
      "realistic view of our model's performance than ROC-AUC or raw precision alone.\n",
      "Data is imbalance and more informative than ROC-AUC (e.g., 90% loan denials,\n",
      "10%\n",
      "approvals).\n",
      "𝑃𝑅−𝐴𝑈𝐶=\n",
      "1\n",
      "0\n",
      "𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛𝑅𝑒𝑐𝑎𝑙𝑙𝑑𝑅𝑒𝑐𝑎𝑙𝑙\n",
      "Precision =\n",
      "TruePositives\n",
      "TruePositives+FalsePositives\n",
      "Recall =\n",
      "TruePositives\n",
      "TruePositives+FalseNegatives\n",
      "\n",
      "\n",
      "--- Page 9 ---\n",
      "Confusion Matrix\n",
      "\n",
      "\n",
      "--- Page 10 ---\n",
      "The confusion matrix reveals that the model has a very low rate of False Positives,\n",
      "which means it is very effective at avoiding the approval of risky loans. While there are\n",
      "some False Negatives (good loans that were rejected), the balance between precision\n",
      "and recall is strategically favorable for a conservative lending strategy.\n",
      "8. Results and Analysis:\n",
      "Performance Outcomes:\n",
      "We selected Logistic Regression for the loan approval model because it provides high\n",
      "transparency, explainability, and auditability — essential factors in regulated financial\n",
      "environments. While other models like Random Forest marginally outperform it in\n",
      "accuracy\n",
      "·\n",
      "Logistic Regression: is the most favorable model for the loan approval task. It\n",
      "achieves the highest Recall (0.960) and F1 Score (0.913). Ideal for minimizing\n",
      "both false negatives (missing eligible applicants) and false positives (approving\n",
      "risky loans).\n",
      "·\n",
      "Highly interpretable and transparent, making it suitable for regulated\n",
      "environments.\n",
      "·\n",
      "PR-AUC is the primary metric due to class imbalance. The Decision Tree\n",
      "achieves the highest PR-AUC (0.867). However, it offers lower interpretability\n",
      "compared to Logistic Regression, which can be a drawback in scenarios where\n",
      "model explainability is critical.\n",
      "·\n",
      "Handle Imbalance can bias a model to favor the majority class. Imbalance data\n",
      "handled by ‘class_weight’ = ‘balanced’ parameter in Logistic Regression\n",
      "model.\n",
      "𝑤𝑖= 𝑘⋅𝑛𝑖\n",
      "𝑛\n",
      "§\n",
      "wᵢis the weight for class/sample i\n",
      "§\n",
      "k is a constant (e.g., total desired sample size or scaling factor)\n",
      "§\n",
      "nᵢis the count/frequency of class i\n",
      "§\n",
      "n is the total number of samples\n",
      "Performance Scores – Logistic Regression with Optimized Hyper parameters:\n",
      "Best Hyperparameter:\n",
      "'params': {\n",
      "'penalty': ['l2'],\n",
      "\n",
      "\n",
      "--- Page 11 ---\n",
      "'C': [ 1,],\n",
      "'solver': ['liblinear'],\n",
      "'max_iter': [200]\n",
      "}\n",
      "Table 1:\n",
      "Metric\n",
      "Scores\n",
      "Recall\n",
      "95.9%\n",
      "F1-Score\n",
      "88.7%\n",
      "Accuracy\n",
      "87.1%\n",
      "Precision\n",
      "87.1%\n",
      "9. Limitations\n",
      "·\n",
      "Assumes Linear Relationship\n",
      "o\n",
      "Logistic Regression assumes a linear relationship between input features and the\n",
      "log-odds of the outcome (approval or rejection).\n",
      "o\n",
      "In reality, loan approval often depends on non-linear interactions (e.g., age vs.\n",
      "income vs. employment history), which Logistic Regression cannot capture\n",
      "without extensive feature engineering (e.g., polynomial terms or interactions).\n",
      "·\n",
      "Limited Expressiveness for Complex Patterns\n",
      "o\n",
      "While simple and interpretable, Logistic Regression lacks the capacity to model\n",
      "complex decision boundaries.\n",
      "o\n",
      "It may fail to identify intricate relationships or non-linear credit risk patterns (e.g.,\n",
      "when high income combined with high debt leads to risk).\n",
      "o\n",
      "This can lead to underfitting, where the model performs poorly even on training\n",
      "data.\n",
      "·\n",
      "Sensitive to Multicollinearity\n",
      "o\n",
      "Logistic Regression can be unstable when features are highly correlated (e.g.,\n",
      "income and credit score), leading to inflated or misleading coefficient values.\n",
      "o\n",
      "This not only harms predictive performance but also undermines model\n",
      "interpretability and trust among stakeholders.\n",
      "·\n",
      "Imbalanced Data Handling\n",
      "o\n",
      "Logistic Regression, by default, optimizes for overall accuracy, which is\n",
      "inappropriate for imbalanced datasets (common in loan approval, where most\n",
      "applicants are approved).\n",
      "o\n",
      "Without adjustments (like threshold tuning or class weighting), it can ignore the\n",
      "minority class (defaults), resulting in poor recall and higher risk exposure.\n",
      "\n",
      "\n",
      "--- Page 12 ---\n",
      "·\n",
      "Difficulty in Capturing Non-Binary Dependencies\n",
      "o\n",
      "It works best for binary outcomes, but real-world loan decisions are often\n",
      "influenced by multi-level risk factors, such as customer behavior segments, loan\n",
      "types, and geographic risk variations.\n",
      "o\n",
      "Extending logistic regression to multiclass or ordinal settings adds complexity\n",
      "and reduces interpretability.\n",
      "·\n",
      "Assumes Feature Independence\n",
      "o\n",
      "Logistic Regression assumes that features contribute independently to the\n",
      "prediction.\n",
      "o\n",
      "In practice, interactions between variables (e.g., employment type and income\n",
      "stability) can be critical, but are ignored unless explicitly modeled.\n",
      "·\n",
      "Static Nature Without Regular Updates\n",
      "o\n",
      "Logistic models require manual retraining to stay up to date.\n",
      "o\n",
      "They don’t adapt automatically to changes in economic conditions, lending\n",
      "regulations, or fraud patterns, which can reduce effectiveness over time.\n",
      "·\n",
      "Lack of Confidence Calibration\n",
      "o\n",
      "The predicted probabilities from Logistic Regression may be poorly calibrated,\n",
      "especially when applied on unseen or shifted data.\n",
      "o\n",
      "Overconfidence in predictions can lead to incorrect approvals or rejections,\n",
      "particularly in borderline cases.\n",
      "10. Deployment Strategy\n",
      "The loan approval model is deployed as a microservice within Mastercard’s cloud-based\n",
      "infrastructure. This architecture ensures high availability, scalability, and seamless\n",
      "integration with existing systems.\n",
      "·\n",
      "API Integration: The model is exposed via a RESTful API endpoint. When a\n",
      "new loan application is submitted, the relevant features are extracted and sent to\n",
      "this API. The API then returns a probability score and a classification\n",
      "(Approved/Rejected) in real-time.\n",
      "·\n",
      "Scalability: The microservice is containerized using Docker and orchestrated\n",
      "using Kubernetes. This setup allows the system to automatically scale up or\n",
      "down based on the volume of loan applications, ensuring low latency even during\n",
      "peak usage.\n",
      "·\n",
      "Integration with Core Systems: The API is integrated with the front-end\n",
      "application portal, the core banking system, and the loan officer's dashboard.\n",
      "This creates a streamlined workflow where the model's prediction is a primary\n",
      "input to the final decision.\n",
      "11. Ethical Considerations\n",
      "The use of AI in financial decisions, especially for something as significant as a loan,\n",
      "comes with substantial ethical responsibilities. We have embedded ethical\n",
      "considerations throughout the model's lifecycle.\n",
      "\n",
      "\n",
      "--- Page 13 ---\n",
      "·\n",
      "Fairness: Avoid bias toward gender, income, or region.\n",
      "·\n",
      "Transparency: Use clear explanations for all predictions.\n",
      "·\n",
      "Privacy: Protect applicant data and follow data laws.\n",
      "·\n",
      "Human Oversight: Route uncertain or sensitive cases to human reviewers.\n",
      "·\n",
      "Compliance: Regular checks to ensure fair lending practices.\n",
      "·\n",
      "Bias Mitigation: The model includes fairness-aware preprocessing techniques,\n",
      "such as reweighing and stratified sampling, to minimize discrimination based on\n",
      "gender, marital status, and education.\n",
      "·\n",
      "Transparency and Explainability: All loan decisions are supported by SHAP\n",
      "(SHapley Additive exPlanations) visualizations to explain individual predictions.\n",
      "These explanations are accessible to both analysts and applicants, ensuring\n",
      "transparency.\n",
      "·\n",
      "Privacy and Data Security: The system complies with GDPR and other regional\n",
      "regulations by implementing data encryption, anonymization, and secure access\n",
      "protocols during data collection, storage, and processing.\n",
      "·\n",
      "Human Oversight: High-impact or low-confidence decisions are flagged for\n",
      "manual review, especially when applicants are from vulnerable or high-risk\n",
      "segments.\n",
      "·\n",
      "Non-discrimination Compliance: Regular audits are performed to ensure that\n",
      "the model complies with fair lending practices and does not disproportionately\n",
      "disadvantage any protected class or region.\n",
      "·\n",
      "Informed Consent: Applicants are informed when their data is being used in\n",
      "automated decision-making processes, with clear opt-in mechanisms.\n",
      "·\n",
      "Bias Mitigation: Fairness-aware preprocessing (reweighing)\n",
      "·\n",
      "Transparency: SHAP for explainability\n",
      "·\n",
      "Privacy: GDPR-compliant data anonymization\n",
      "12. Future Work\n",
      "Our work on the loan approval system is an ongoing effort. We have a roadmap for\n",
      "future improvements and innovation:\n",
      "·\n",
      "Integration of Alternative Data: We plan to explore the use of non-traditional\n",
      "data sources, such as utility payment history and rental data, to improve the\n",
      "model's predictive power for thin-file applicants who lack a strong credit history.\n",
      "·\n",
      "Explainable AI (XAI): We will continue to invest in research and development of\n",
      "more robust and intuitive XAI tools. Our goal is to move beyond simple feature\n",
      "importance to generate a complete narrative for each decision.\n",
      "·\n",
      "Real-time Feature Engineering: We aim to develop a system that can create\n",
      "real-time, aggregated features from streaming transaction data, allowing for a\n",
      "\n",
      "\n",
      "--- Page 14 ---\n",
      "more dynamic and up-to-the-minute assessment of an applicant's financial\n",
      "health.\n",
      "·\n",
      "Incorporating Economic Indicators: The model will be enhanced to include\n",
      "macroeconomic indicators (e.g., inflation rates, unemployment rates) to make it\n",
      "more resilient to broad economic shifts.\n",
      "15. Fallback Mechanism\n",
      "Robustness is a key tenet of our system design. We have implemented several fallback\n",
      "mechanisms to handle various types of failures.\n",
      "·\n",
      "Rule Based Model: Retraining:The ML model faced performance degradation\n",
      "grater than 8% under these changed patterns. As a countermeasure, dynamic\n",
      "retraining was implemented once in quarter using updated data, along with\n",
      "incremental learning methods to adapt to rapid changes in applicant profiles.\n",
      "·\n",
      "Human-in-the-Loop: As mentioned, a human loan officer always has the final\n",
      "say. The model serves as an automated recommendation, but the ultimate\n",
      "decision-making authority remains with a human to account for any unforeseen\n",
      "circumstances or new information not captured by the model.\n",
      "o\n",
      "Requests marked as emergency or pandemic-related aid\n",
      "o\n",
      "Such cases are flagged by the system for manual assessment to ensure\n",
      "fair and context-aware decisions\n",
      "·\n",
      "System Failures: In the event of an API or service failure, the system defaults to\n",
      "a predefined set of rules that are based on our traditional underwriting criteria.\n",
      "This ensures business continuity.\n",
      "16. Model Monitoring\n",
      "To ensure the long-term viability and performance of the model, a comprehensive\n",
      "monitoring and alerting system is in place.\n",
      "·\n",
      "Real-time Performance Tracking: We track key metrics (precision, recall, AUC)\n",
      "on a daily basis for the most recent loan applications. This allows us to quickly\n",
      "detect any degradation in performance.\n",
      "·\n",
      "Drift Detection: We monitor for two types of drift:\n",
      "o\n",
      "Data Drift: Changes in the distribution of input features over time (e.g., a\n",
      "sudden increase in Applicant_Income or a shift in Property_Area). This\n",
      "can signal a need for retraining.\n",
      "o\n",
      "Concept Drift: Changes in the relationship between the input features\n",
      "and the target variable (e.g., a good credit history no longer being a strong\n",
      "predictor of repayment). This is a more serious issue and often requires a\n",
      "deeper investigation.\n",
      "\n",
      "\n",
      "--- Page 15 ---\n",
      "·\n",
      "Alerting Systems: Automated alerts are triggered if any performance metric falls\n",
      "below a predefined threshold or if significant data/concept drift is detected. These\n",
      "alerts notify the MLOps and Data Science teams to initiate an investigation or a\n",
      "retraining cycle.\n",
      "17. Performance Under Stress Conditions\n",
      "·\n",
      "To improve the model’s ability to assess loan applications during the pandemic,\n",
      "temporary features were introduced—such as flags indicating COVID-19-related\n",
      "job disruptions and loan types categorized under emergency business or\n",
      "personal relief. These helped the model better understand the financial stress\n",
      "context behind the applications, enabling fairer decisions for individuals and\n",
      "businesses affected by the crisis.\n",
      "·\n",
      "During the COVID-19 pandemic, financial uncertainty led to significant changes\n",
      "in loan application patterns. A surge in applications was observed from both\n",
      "individuals (seeking personal loans due to medical emergencies, job losses, and\n",
      "reduced income) and businesses (seeking emergency funding to sustain\n",
      "operations, manage payroll, or restructure debts).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_bytes  = get_raw_data(client_id=client_id, authority=authority,file_path = file_path)\n",
    "pdf_content = get_onedrive_whitepaper(file_bytes)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd Version ID: 6.0, Last Modified: 2025-08-05T08:44:27Z\n",
      "Extracted PDF text (first 1000 chars):\n",
      "\n",
      "--- Page 1 ---\n",
      "Machine Learning-Based Loan Approval Prediction System\n",
      "for Financial Institutions\n",
      "1. Executive Summary\n",
      "The financial services industry faces a critical challenge in automating and de-risking\n",
      "the loan approval process. Traditional methods, often relying on manual review and\n",
      "static rule-based systems, are prone to human error, inconsistency, and significant\n",
      "processing delays. These limitations result in missed opportunities, elevated credit risk,\n",
      "and suboptimal customer experiences. To address these issues, we have developed a\n",
      "robust machine learning-based Loan Approval Classification System. This model\n",
      "leverages a comprehensive set of applicant data to predict the likelihood of loan\n",
      "repayment, classifying applications as either approved or rejected.\n",
      "Our system is designed to provide a high-level overview of an applicant's\n",
      "creditworthiness, offering a data-driven, objective, and transparent decision-making tool.\n",
      "By integrating advanced machine learning techniques, our mod\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n--- Page 1 ---\\nMachine Learning-Based Loan Approval Prediction System\\nfor Financial Institutions\\n1. Executive Summary\\nThe financial services industry faces a critical challenge in automating and de-risking\\nthe loan approval process. Traditional methods, often relying on manual review and\\nstatic rule-based systems, are prone to human error, inconsistency, and significant\\nprocessing delays. These limitations result in missed opportunities, elevated credit risk,\\nand suboptimal customer experiences. To address these issues, we have developed a\\nrobust machine learning-based Loan Approval Classification System. This model\\nleverages a comprehensive set of applicant data to predict the likelihood of loan\\nrepayment, classifying applications as either approved or rejected.\\nOur system is designed to provide a high-level overview of an applicant's\\ncreditworthiness, offering a data-driven, objective, and transparent decision-making tool.\\nBy integrating advanced machine learning techniques, our model achieves superior\\npredictive accuracy compared to traditional methods. It significantly reduces the time\\nfrom application to decision, minimizes the risk of default, and ensures a consistent, fair\\nevaluation process. This strategic asset not only enhances operational efficiency but\\nalso provides a competitive advantage by enabling faster, more confident lending\\ndecisions.\\n2. Introduction\\nThe process of loan approval is a cornerstone of the financial industry. It involves\\nevaluating a multitude of factors to determine an applicant's creditworthiness and ability\\nto repay a loan. Historically, this process has been labor-intensive, relying on credit\\nofficers to manually review application forms, financial statements, and credit reports.\\nThis manual approach is slow, expensive, and susceptible to biases. The rise of digital\\nbanking and the demand for instant financial services have made this traditional model\\nincreasingly unsustainable.\\nThis white paper details a machine learning-based solution designed to modernize and\\noptimize the loan approval workflow. By building a classification model, our system can\\naccurately predict the Loan_Status (Approved or Rejected) for new applications. The\\nprimary motivation for this project is to create a scalable, efficient, and fair system that\\ncan process thousands of applications in real-time, reducing operational costs while\\nsimultaneously improving the quality of lending decisions. Our model is intended to\\nserve as a decision-support tool for loan officers, enabling them to focus on complex\\ncases and customer relationships rather than routine data analysis.\\n\\n--- Page 2 ---\\n3. Related Work / Literature Review\\nThe field of credit scoring and loan prediction has seen extensive research and\\napplication of various machine learning models. A number of algorithms have been\\nemployed to analyze applicant data and forecast loan outcomes:\\n·\\nLogistic Regression: A statistical method used for binary classification, which is\\nwell-suited for predicting a 'yes' or 'no' outcome for loan approval. It is valued for\\nits simplicity and the interpretability of its results, as it shows how different factors\\ninfluence the final decision.\\n·\\nDecision Trees: These models use a tree-like structure of decisions and their\\npossible consequences. They are easy to understand and visualize, as they\\nmimic human decision-making processes.\\n\\n--- Page 3 ---\\n·\\nRandom Forests: An ensemble method that builds multiple decision trees and\\ncombines their predictions to improve accuracy and reduce overfitting.\\n·\\nSteps of ML Algorithms:\\n\\n--- Page 4 ---\\n4. Data Description\\nThe model is trained on a comprehensive dataset of past loan applications. The dataset\\ncontains a mix of demographic, financial, and behavioral features.\\nFeature Definitions:\\n·\\nLoan_ID: A unique identifier for each loan application.\\n·\\nGender: The applicant's gender (Male/Female).\\n·\\nMarried: Marital status of the applicant (Yes/No).\\n·\\nDependents: Number of dependents the applicant has.\\n·\\nEducation: Applicant's education level (Graduate/Not Graduate).\\n·\\nSelf_Employed: Whether the applicant is self-employed (Yes/No).\\n·\\nApplicant_Income: The applicant's monthly income.\\n·\\nCoapplicant_Income: The co-applicant's monthly income.\\n·\\nLoan_Amount: The amount of the loan requested.\\n·\\nLoan_Amount_Term: The term of the loan in months.\\n·\\nCredit_History: A binary variable indicating if the applicant has a good credit\\nhistory (1.0) or not (0.0). This is a critical predictor.\\n·\\nProperty_Area: The area where the property is located\\n(Rural/Semiurban/Urban).\\n·\\nLoan_Status: The target variable, indicating if the loan was approved (Y) or\\nrejected (N).\\nPreprocessing Steps:\\n1.\\nHandling Missing Values: Missing values are common in real-world data. We\\nemploy different strategies based on the feature type:\\no\\nCategorical Features: Missing values in Gender, Married, Dependents,\\nSelf_Employed, and Credit_History are imputed using the mode (most\\nfrequent value) of the respective columns.\\no\\nNumerical Features: Missing values in Loan_Amount and\\nLoan_Amount_Term are imputed using the mean or median to avoid\\nskewing the distribution.\\n1.\\nData Type Consistency: All features are checked for consistent data types.\\nNumerical features are stored as integers or floats, while categorical features are\\nstored as strings or object types.\\n2.\\nData Balancing: An analysis of the Loan_Status target variable revealed an\\nimbalance, with a significantly higher number of approved loans than rejected\\nones. This imbalance can bias a model to favor the majority class. Imbalance\\n\\n--- Page 5 ---\\ndata handled by ‘class_weight’ = ‘balanced’ parameter in Logistic Regression\\nmodel.\\n𝑤𝑖= 𝑘⋅𝑛𝑖\\n𝑛\\n§\\nwᵢis the weight for class/sample i\\n§\\nk is a constant (e.g., total desired sample size or scaling factor)\\n§\\nnᵢis the count/frequency of class i\\n§\\nn is the total number of samples\\no\\nSynthetic Minority Over-sampling Technique (SMOTE) during the\\ntraining phase to create synthetic data points for the minority class,\\nensuring the model is not biased and can accurately identify both\\napproved and rejected applications.\\n3.\\nData Transformation:\\no\\nNumerical Columns: We apply a normalization technique (e.g.,\\nStandardScaler) to numerical columns (Applicant_Income,\\nCoapplicant_Income, Loan_Amount) to ensure they have a zero mean\\nand unit variance. This prevents features with larger magnitudes from\\ndominating the model's training process.\\no\\nCategorical Features: We use One-Hot Encoding to convert categorical\\nfeatures (Gender, Married, Education, etc.) into a numerical format\\nsuitable for the model. This creates new binary columns for each unique\\ncategory, avoiding the assumption of ordinality that simple label encoding\\nmight introduce.\\n5. Model Architecture\\nWhy Logistic Regression: Preferred choice\\nWe selected Logistic Regression for the loan approval model because it provides high\\ntransparency, explainability, and auditability — essential factors in regulated financial\\nenvironments. While other models like Random Forest marginally outperform it in\\naccuracy, Logistic Regression allows us to clearly communicate how each feature\\ncontributes to the final decision, enabling easier compliance with fairness, bias\\ndetection, and model governance requirements\\n·\\nOperating in a highly regulated environment (e.g. banking, insurance)\\n·\\nNeed to explain decisions to compliance officers or regulators\\n·\\nWanted easy-to-track fairness or bias metrics\\n·\\nPrioritize transparency over marginal gains in accuracy\\n\\n--- Page 6 ---\\nFeature Name\\nType\\nGender\\nCategorical\\nMarried\\nCategorical\\nDependents\\nCategorical\\nEducation\\nCategorical\\nSelf_Employed\\nCategorical\\nApplicantIncome\\nNumerical\\nCoapplicantIncome\\nNumerical\\nLoanAmount\\nNumerical\\nLoan_Amount_Term\\nNumerical\\nCredit_History\\nBinary (0 or 1)\\nProperty_Area\\nCategorical\\nGeneral Equation of Logistic Regression\\n𝑃𝑌= 1 =\\n1\\n1 + 𝑒−𝑍\\n𝑍= 𝛽0 + 𝛽1𝑋1 + 𝛽2𝑋2 +⋯+ 𝛽𝑛𝑋𝑛\\n𝑃𝑌= 1 = 𝑝𝑟𝑜𝑏𝑎𝑏𝑖𝑙𝑖𝑡𝑦 𝑜𝑓 𝑙𝑜𝑎𝑛 𝑎𝑝𝑝𝑟𝑜𝑣𝑎𝑙\\n𝛽0 = 𝑖𝑛𝑡𝑒𝑟𝑐𝑒𝑝𝑡 (𝑏𝑖𝑎𝑠 𝑡𝑒𝑟𝑚)\\n𝛽𝑖= 𝑐𝑜𝑒𝑓𝑓𝑖𝑐𝑖𝑒𝑛𝑡 𝑓𝑜𝑟 𝑓𝑒𝑎𝑡𝑢𝑟𝑒 𝑋𝑖\\n𝑍= 𝛽0 + 𝛽1 ⋅𝐺𝑒𝑛𝑑𝑒𝑟+ 𝛽2 ⋅𝑀𝑎𝑟𝑟𝑖𝑒𝑑+ 𝛽3 ⋅𝐷𝑒𝑝𝑒𝑛𝑑𝑒𝑛𝑡𝑠+ 𝛽4 ⋅𝐸𝑑𝑢𝑐𝑎𝑡𝑖𝑜𝑛+ 𝛽5 ⋅\\n𝑆𝑒𝑙𝑓_𝐸𝑚𝑝𝑙𝑜𝑦𝑒𝑑+ 𝛽6 ⋅𝐴𝑝𝑝𝑙𝑖𝑐𝑎𝑛𝑡𝐼𝑛𝑐𝑜𝑚𝑒+ 𝛽7 ⋅𝐶𝑜𝑎𝑝𝑝𝑙𝑖𝑐𝑎𝑛𝑡𝐼𝑛𝑐𝑜𝑚𝑒+ 𝛽8 ⋅𝐿𝑜𝑎𝑛𝐴𝑚𝑜𝑢𝑛𝑡+ 𝛽9 ⋅\\n𝐿𝑜𝑎𝑛_𝐴𝑚𝑜𝑢𝑛𝑡_𝑇𝑒𝑟𝑚+ 𝛽10 ⋅𝐶𝑟𝑒𝑑𝑖𝑡_𝐻𝑖𝑠𝑡𝑜𝑟𝑦+ 𝛽11 ⋅𝑃𝑟𝑜𝑝𝑒𝑟𝑡𝑦_𝐴𝑟𝑒𝑎_𝑈𝑟𝑏𝑎𝑛+ 𝛽12 ⋅\\n𝑃𝑟𝑜𝑝𝑒𝑟𝑡𝑦_𝐴𝑟𝑒𝑎_𝑆𝑒𝑚𝑖𝑢𝑟𝑏𝑎𝑛\\n6. Training Methodology\\nThe training process is meticulously designed to ensure the model is robust, accurate,\\nand ready for production.\\n1.\\nData Split:\\n·\\nDataset split into three parts:\\no\\nTraining set: 70%\\no\\nValidation set: 15%\\no\\nTesting set: 15%\\n·\\nTraining set purpose: Used to train the model.\\n·\\nValidation set purpose: Used for hyperparameter tuning and model selection.\\n\\n--- Page 7 ---\\n·\\nTesting set purpose: Reserved for final, unbiased evaluation of model\\nperformance.\\n2.\\nHyperparameter Tuning: We use techniques like Grid Search or Bayesian\\nOptimization to find the optimal set of hyperparameters for the Logixtic\\nRegression model. Key parameters tuned include\\no\\nPenalty: Specifies the type of regularization used to avoid overfitting\\no\\nC: Inverse of regularization strength (i.e., smaller C means stronger\\nregularization).\\no\\nSolver: Optimization algorithm used for finding model coefficients.\\no\\nMax_iter: Maximum number of iterations taken by the solver to converge.\\n3.\\nValidation Strategy: We employ k-fold cross-validation during the training\\nphase. The training data is divided into k folds. The model is trained k times,\\neach time using a different fold as the validation set. This robust strategy ensures\\nthe model's performance is not specific to a single data split.\\n4.\\nRetraining Pipelines: The model is not a static artifact. It is part of a continuous\\nlearning loop. A retraining pipeline is established to periodically retrain the model\\non new data, typically on a monthly or quarterly basis. This ensures the model\\nremains relevant and its predictions accurate as consumer behavior and\\neconomic conditions change.\\n7. Evaluation Metrics\\nTo assess the model's performance, we utilize a suite of metrics tailored to the financial\\ndomain. A simple accuracy score is often misleading in imbalanced classification\\nproblems, so we rely on a more comprehensive set of metrics.\\nBusiness-Specific Interpretation\\nScenario\\nPrioritize\\nYou want to maximize profit while reducing risky loans\\nPrecision, F1-score\\nYou want to not miss good applicants\\nRecall, F1-score\\nYou’re building a regulatory-compliant, fair model\\nBalanced Accuracy,\\nFairness metrics\\nYou prioritize avoiding defaults (minimize false approvals)\\nHigh Precision\\nYou prioritize financial inclusion (minimize false rejections)\\nHigh Recall\\nYou want a balanced approval system for early model\\nevaluation\\nF1-score\\n\\n--- Page 8 ---\\n1. Our primary objective is to avoid loan defaults, so we prioritize Precision to\\nreduce false approvals.\\n2. To ensure a balanced and inclusive approval system, we use the F1-score,\\nwhich helps capture both Precision and Recall, ensuring we do not miss eligible\\napplicants.\\n3. Since the dataset is highly imbalanced, we use PR-AUC (Precision-Recall\\nAUC) for a more reliable evaluation of model performance.\\n·\\nPrecision: Prioritize avoiding defaults (minimize false approvals)\\nPrecision =\\nTruePositives\\nTruePositives + FalsePositives\\n·\\nRecall (Sensitivity): Prioritize financial inclusion (minimize false rejections)\\nRecall =\\nTruePositives\\nTruePositives + FalseNegatives\\n·\\nF1-Score: Wanted balanced approval system for early model evaluation\\nF1 - Score = 2 × Precision × Recall\\nPrecision + Recall\\n·\\nPR-AUC (Precision-Recall AUC): In our imbalanced setting, we're more interested\\nin how well the model identifies truly eligible applicants. PR-AUC gives a more\\nrealistic view of our model's performance than ROC-AUC or raw precision alone.\\nData is imbalance and more informative than ROC-AUC (e.g., 90% loan denials,\\n10%\\napprovals).\\n𝑃𝑅−𝐴𝑈𝐶=\\n1\\n0\\n𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛𝑅𝑒𝑐𝑎𝑙𝑙𝑑𝑅𝑒𝑐𝑎𝑙𝑙\\nPrecision =\\nTruePositives\\nTruePositives+FalsePositives\\nRecall =\\nTruePositives\\nTruePositives+FalseNegatives\\n\\n--- Page 9 ---\\nConfusion Matrix\\n\\n--- Page 10 ---\\nThe confusion matrix reveals that the model has a very low rate of False Positives,\\nwhich means it is very effective at avoiding the approval of risky loans. While there are\\nsome False Negatives (good loans that were rejected), the balance between precision\\nand recall is strategically favorable for a conservative lending strategy.\\n8. Results and Analysis:\\nPerformance Outcomes:\\nWe selected Logistic Regression for the loan approval model because it provides high\\ntransparency, explainability, and auditability — essential factors in regulated financial\\nenvironments. While other models like Random Forest marginally outperform it in\\naccuracy\\n·\\nLogistic Regression: is the most favorable model for the loan approval task. It\\nachieves the highest Recall (0.960) and F1 Score (0.913). Ideal for minimizing\\nboth false negatives (missing eligible applicants) and false positives (approving\\nrisky loans).\\n·\\nHighly interpretable and transparent, making it suitable for regulated\\nenvironments.\\n·\\nPR-AUC is the primary metric due to class imbalance. The Decision Tree\\nachieves the highest PR-AUC (0.867). However, it offers lower interpretability\\ncompared to Logistic Regression, which can be a drawback in scenarios where\\nmodel explainability is critical.\\n·\\nHandle Imbalance can bias a model to favor the majority class. Imbalance data\\nhandled by ‘class_weight’ = ‘balanced’ parameter in Logistic Regression\\nmodel.\\n𝑤𝑖= 𝑘⋅𝑛𝑖\\n𝑛\\n§\\nwᵢis the weight for class/sample i\\n§\\nk is a constant (e.g., total desired sample size or scaling factor)\\n§\\nnᵢis the count/frequency of class i\\n§\\nn is the total number of samples\\nPerformance Scores – Logistic Regression with Optimized Hyper parameters:\\nBest Hyperparameter:\\n'params': {\\n'penalty': ['l2'],\\n\\n--- Page 11 ---\\n'C': [ 1,],\\n'solver': ['liblinear'],\\n'max_iter': [200]\\n}\\nTable 1:\\nMetric\\nScores\\nRecall\\n95.9%\\nF1-Score\\n88.7%\\nAccuracy\\n87.1%\\nPrecision\\n87.1%\\n9. Limitations\\n·\\nAssumes Linear Relationship\\no\\nLogistic Regression assumes a linear relationship between input features and the\\nlog-odds of the outcome (approval or rejection).\\no\\nIn reality, loan approval often depends on non-linear interactions (e.g., age vs.\\nincome vs. employment history), which Logistic Regression cannot capture\\nwithout extensive feature engineering (e.g., polynomial terms or interactions).\\n·\\nLimited Expressiveness for Complex Patterns\\no\\nWhile simple and interpretable, Logistic Regression lacks the capacity to model\\ncomplex decision boundaries.\\no\\nIt may fail to identify intricate relationships or non-linear credit risk patterns (e.g.,\\nwhen high income combined with high debt leads to risk).\\no\\nThis can lead to underfitting, where the model performs poorly even on training\\ndata.\\n·\\nSensitive to Multicollinearity\\no\\nLogistic Regression can be unstable when features are highly correlated (e.g.,\\nincome and credit score), leading to inflated or misleading coefficient values.\\no\\nThis not only harms predictive performance but also undermines model\\ninterpretability and trust among stakeholders.\\n·\\nImbalanced Data Handling\\no\\nLogistic Regression, by default, optimizes for overall accuracy, which is\\ninappropriate for imbalanced datasets (common in loan approval, where most\\napplicants are approved).\\no\\nWithout adjustments (like threshold tuning or class weighting), it can ignore the\\nminority class (defaults), resulting in poor recall and higher risk exposure.\\n\\n--- Page 12 ---\\n·\\nDifficulty in Capturing Non-Binary Dependencies\\no\\nIt works best for binary outcomes, but real-world loan decisions are often\\ninfluenced by multi-level risk factors, such as customer behavior segments, loan\\ntypes, and geographic risk variations.\\no\\nExtending logistic regression to multiclass or ordinal settings adds complexity\\nand reduces interpretability.\\n·\\nAssumes Feature Independence\\no\\nLogistic Regression assumes that features contribute independently to the\\nprediction.\\no\\nIn practice, interactions between variables (e.g., employment type and income\\nstability) can be critical, but are ignored unless explicitly modeled.\\n·\\nStatic Nature Without Regular Updates\\no\\nLogistic models require manual retraining to stay up to date.\\no\\nThey don’t adapt automatically to changes in economic conditions, lending\\nregulations, or fraud patterns, which can reduce effectiveness over time.\\n·\\nLack of Confidence Calibration\\no\\nThe predicted probabilities from Logistic Regression may be poorly calibrated,\\nespecially when applied on unseen or shifted data.\\no\\nOverconfidence in predictions can lead to incorrect approvals or rejections,\\nparticularly in borderline cases.\\n10. Deployment Strategy\\nThe loan approval model is deployed as a microservice within Mastercard’s cloud-based\\ninfrastructure. This architecture ensures high availability, scalability, and seamless\\nintegration with existing systems.\\n·\\nAPI Integration: The model is exposed via a RESTful API endpoint. When a\\nnew loan application is submitted, the relevant features are extracted and sent to\\nthis API. The API then returns a probability score and a classification\\n(Approved/Rejected) in real-time.\\n·\\nScalability: The microservice is containerized using Docker and orchestrated\\nusing Kubernetes. This setup allows the system to automatically scale up or\\ndown based on the volume of loan applications, ensuring low latency even during\\npeak usage.\\n·\\nIntegration with Core Systems: The API is integrated with the front-end\\napplication portal, the core banking system, and the loan officer's dashboard.\\nThis creates a streamlined workflow where the model's prediction is a primary\\ninput to the final decision.\\n11. Ethical Considerations\\nThe use of AI in financial decisions, especially for something as significant as a loan,\\ncomes with substantial ethical responsibilities. We have embedded ethical\\nconsiderations throughout the model's lifecycle.\\n\\n--- Page 13 ---\\n·\\nFairness: Avoid bias toward gender, income, or region.\\n·\\nTransparency: Use clear explanations for all predictions.\\n·\\nPrivacy: Protect applicant data and follow data laws.\\n·\\nHuman Oversight: Route uncertain or sensitive cases to human reviewers.\\n·\\nCompliance: Regular checks to ensure fair lending practices.\\n·\\nBias Mitigation: The model includes fairness-aware preprocessing techniques,\\nsuch as reweighing and stratified sampling, to minimize discrimination based on\\ngender, marital status, and education.\\n·\\nTransparency and Explainability: All loan decisions are supported by SHAP\\n(SHapley Additive exPlanations) visualizations to explain individual predictions.\\nThese explanations are accessible to both analysts and applicants, ensuring\\ntransparency.\\n·\\nPrivacy and Data Security: The system complies with GDPR and other regional\\nregulations by implementing data encryption, anonymization, and secure access\\nprotocols during data collection, storage, and processing.\\n·\\nHuman Oversight: High-impact or low-confidence decisions are flagged for\\nmanual review, especially when applicants are from vulnerable or high-risk\\nsegments.\\n·\\nNon-discrimination Compliance: Regular audits are performed to ensure that\\nthe model complies with fair lending practices and does not disproportionately\\ndisadvantage any protected class or region.\\n·\\nInformed Consent: Applicants are informed when their data is being used in\\nautomated decision-making processes, with clear opt-in mechanisms.\\n·\\nBias Mitigation: Fairness-aware preprocessing (reweighing)\\n·\\nTransparency: SHAP for explainability\\n·\\nPrivacy: GDPR-compliant data anonymization\\n12. Future Work\\nOur work on the loan approval system is an ongoing effort. We have a roadmap for\\nfuture improvements and innovation:\\n·\\nIntegration of Alternative Data: We plan to explore the use of non-traditional\\ndata sources, such as utility payment history and rental data, to improve the\\nmodel's predictive power for thin-file applicants who lack a strong credit history.\\n·\\nExplainable AI (XAI): We will continue to invest in research and development of\\nmore robust and intuitive XAI tools. Our goal is to move beyond simple feature\\nimportance to generate a complete narrative for each decision.\\n·\\nReal-time Feature Engineering: We aim to develop a system that can create\\nreal-time, aggregated features from streaming transaction data, allowing for a\\n\\n--- Page 14 ---\\nmore dynamic and up-to-the-minute assessment of an applicant's financial\\nhealth.\\n·\\nIncorporating Economic Indicators: The model will be enhanced to include\\nmacroeconomic indicators (e.g., inflation rates, unemployment rates) to make it\\nmore resilient to broad economic shifts.\\n15. Fallback Mechanism\\nRobustness is a key tenet of our system design. We have implemented several fallback\\nmechanisms to handle various types of failures.\\n·\\nRule Based Model: Retraining:The ML model faced performance degradation\\ngrater than 8% under these changed patterns. As a countermeasure, dynamic\\nretraining was implemented once in quarter using updated data, along with\\nincremental learning methods to adapt to rapid changes in applicant profiles.\\n·\\nHuman-in-the-Loop: As mentioned, a human loan officer always has the final\\nsay. The model serves as an automated recommendation, but the ultimate\\ndecision-making authority remains with a human to account for any unforeseen\\ncircumstances or new information not captured by the model.\\no\\nRequests marked as emergency or pandemic-related aid\\no\\nSuch cases are flagged by the system for manual assessment to ensure\\nfair and context-aware decisions\\n·\\nSystem Failures: In the event of an API or service failure, the system defaults to\\na predefined set of rules that are based on our traditional underwriting criteria.\\nThis ensures business continuity.\\n16. Model Monitoring\\nTo ensure the long-term viability and performance of the model, a comprehensive\\nmonitoring and alerting system is in place.\\n·\\nReal-time Performance Tracking: We track key metrics (precision, recall, AUC)\\non a daily basis for the most recent loan applications. This allows us to quickly\\ndetect any degradation in performance.\\n·\\nDrift Detection: We monitor for two types of drift:\\no\\nData Drift: Changes in the distribution of input features over time (e.g., a\\nsudden increase in Applicant_Income or a shift in Property_Area). This\\ncan signal a need for retraining.\\no\\nConcept Drift: Changes in the relationship between the input features\\nand the target variable (e.g., a good credit history no longer being a strong\\npredictor of repayment). This is a more serious issue and often requires a\\ndeeper investigation.\\n\\n--- Page 15 ---\\n·\\nAlerting Systems: Automated alerts are triggered if any performance metric falls\\nbelow a predefined threshold or if significant data/concept drift is detected. These\\nalerts notify the MLOps and Data Science teams to initiate an investigation or a\\nretraining cycle.\\n17. Performance Under Stress Conditions\\n·\\nTo improve the model’s ability to assess loan applications during the pandemic,\\ntemporary features were introduced—such as flags indicating COVID-19-related\\njob disruptions and loan types categorized under emergency business or\\npersonal relief. These helped the model better understand the financial stress\\ncontext behind the applications, enabling fairer decisions for individuals and\\nbusinesses affected by the crisis.\\n·\\nDuring the COVID-19 pandemic, financial uncertainty led to significant changes\\nin loan application patterns. A surge in applications was observed from both\\nindividuals (seeking personal loans due to medical emergencies, job losses, and\\nreduced income) and businesses (seeking emergency funding to sustain\\noperations, manage payroll, or restructure debts).\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "version_id = 1\n",
    "prev_version(client_id=client_id, authority=authority,  file_path= file_path, version_id = version_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87ed6a9",
   "metadata": {},
   "source": [
    "## Get updated file from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9e5ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd \n",
    "import requests\n",
    "import requests\n",
    "import base64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbb8bd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "push_event= pd.read_json('push_events.json', )\n",
    "latest_push_id = push_event[0].tolist()[0]\n",
    "latest_push_id\n",
    "\n",
    "# --- Usage ---\n",
    "owner = \"arunkenwal02\"\n",
    "repo = \"code-validator\"\n",
    "push_id = latest_push_id\n",
    "file_path = \"loan-approval-prediction_v2.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a0dcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sha_pair_from_push_id(owner, repo, push_id):\n",
    "    \"\"\"\n",
    "    Returns (before_sha, head_sha) for the given push_id.\n",
    "    If not found, returns (None, None).\n",
    "    \"\"\"\n",
    "    url = f\"https://api.github.com/repos/{owner}/{repo}/events\"\n",
    "    resp = requests.get(url)\n",
    "    events = resp.json()\n",
    "    for event in events:\n",
    "        if event[\"type\"] == \"PushEvent\" and event[\"id\"] == str(push_id):\n",
    "            before_sha = event[\"payload\"][\"before\"]\n",
    "            head_sha = event[\"payload\"][\"head\"]\n",
    "            print(f\"Push ID: {push_id}\\nbefore: {before_sha}\\nhead: {head_sha}\")\n",
    "            return before_sha, head_sha\n",
    "    print(f\"Push ID {push_id} not found in recent events.\")\n",
    "    return None, None\n",
    "\n",
    "def fetch_latest_file_for_sha(owner, repo, file_path, sha_pairs):\n",
    "    \"\"\"\n",
    "    For each (sha_old, sha_new) in sha_pairs, check if file_path was updated.\n",
    "    If yes, download file from sha_new. Else, download most recently updated version.\n",
    "    \"\"\"\n",
    "    for i, (sha_old, sha_new) in enumerate(sha_pairs):\n",
    "        print(f\"\\nProcessing pair {i+1}: {sha_old} → {sha_new}\")\n",
    "\n",
    "        # 1. Compare the two SHAs\n",
    "        compare_url = f\"https://api.github.com/repos/{owner}/{repo}/compare/{sha_old}...{sha_new}\"\n",
    "        compare_resp = requests.get(compare_url)\n",
    "        compare_data = compare_resp.json()\n",
    "\n",
    "        file_changed = False\n",
    "        for f in compare_data.get(\"files\", []):\n",
    "            if f[\"filename\"] == file_path:\n",
    "                file_changed = True\n",
    "                print(f\"File {file_path} was changed in this push.\")\n",
    "                break\n",
    "\n",
    "        if file_changed:\n",
    "            # Download updated file from sha_new\n",
    "            content_url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{file_path}\"\n",
    "            params = {\"ref\": sha_new}\n",
    "            file_resp = requests.get(content_url, params=params)\n",
    "            file_data = file_resp.json()\n",
    "            \n",
    "        # Check for 'content' key (base64-encoded)\n",
    "            if \"content\" in file_data:\n",
    "                nb_json = base64.b64decode(file_data[\"content\"]).decode(\"utf-8\")\n",
    "                notebook_dict = json.loads(nb_json)\n",
    "                return notebook_dict\n",
    "            else:\n",
    "                raise Exception(\"Notebook not found or could not fetch content. Details: \" + str(file_data))\n",
    "\n",
    "        else:\n",
    "            print(f\"File {file_path} was NOT changed between {sha_old} and {sha_new}.\")\n",
    "            # Get most recent commit where this file was updated\n",
    "            commits_url = f\"https://api.github.com/repos/{owner}/{repo}/commits\"\n",
    "            params = {\"path\": file_path, \"per_page\": 1}\n",
    "            commits_resp = requests.get(commits_url, params=params)\n",
    "            last_update_sha = commits_resp.json()[0][\"sha\"]\n",
    "            print(\"Most recent commit where file was changed:\", last_update_sha)\n",
    "            # Download file at that SHA\n",
    "            content_url = f\"https://api.github.com/repos/{owner}/{repo}/contents/{file_path}\"\n",
    "            params = {\"ref\": last_update_sha}\n",
    "            file_resp = requests.get(content_url, params=params)\n",
    "            file_data = file_resp.json()\n",
    "            \n",
    "            # Check for 'content' key (base64-encoded)\n",
    "            if \"content\" in file_data:\n",
    "                nb_json = base64.b64decode(file_data[\"content\"]).decode(\"utf-8\")\n",
    "                notebook_dict = json.loads(nb_json)\n",
    "                return notebook_dict\n",
    "            else:\n",
    "                raise Exception(\"Notebook not found or could not fetch content. Details: \" + str(file_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5518dac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Push ID: 52764657352\n",
      "before: a92475d2557a2d1dd1e17c0a2f57ff950d60b6ae\n",
      "head: 057c3593f00d2b21d20d4b572095038807df5de1\n",
      "\n",
      "Processing pair 1: a92475d2557a2d1dd1e17c0a2f57ff950d60b6ae → 057c3593f00d2b21d20d4b572095038807df5de1\n",
      "File loan-approval-prediction_v2.ipynb was NOT changed between a92475d2557a2d1dd1e17c0a2f57ff950d60b6ae and 057c3593f00d2b21d20d4b572095038807df5de1.\n",
      "Most recent commit where file was changed: 899191ec55d9a7d44bcdaed22e41395f946059df\n"
     ]
    }
   ],
   "source": [
    "sha_pair = get_sha_pair_from_push_id(owner, repo, push_id)\n",
    "\n",
    "sha_pair = [sha_pair]\n",
    "fetch_latest_file_for_sha \n",
    "# --- Usage example ---\n",
    "notebook_contents = fetch_latest_file_for_sha(owner, repo, file_path, sha_pair)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "befb97bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cell #11:\n",
      "Code:\n",
      "# Data dimension\n",
      "print('Data dimension: {} rows and {} columns'.format(len(df_train), len(df_train.columns)))\n",
      "df_train.head()\n",
      "\n",
      "Outputs:\n",
      "Data dimension: 491 rows and 13 columns\n",
      "\n",
      "Data dimension: 491 rows and 13 columns\n",
      "\n",
      "['    Loan_ID  Gender Married Dependents     Education Self_Employed  \\\\\\n', '0  LP002305  Female      No          0      Graduate            No   \\n', '1  LP001715    Male     Yes         3+  Not Graduate           Yes   \\n', '2  LP002086  Female     Yes          0      Graduate            No   \\n', '3  LP001136    Male     Yes          0  Not Graduate           Yes   \\n', '4  LP002529    Male     Yes          2      Graduate            No   \\n', '\\n', '   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\\\\n', '0             4547                0.0       115.0             360.0   \\n', '1             5703                0.0       130.0             360.0   \\n', '2             4333             2451.0       110.0             360.0   \\n', '3             4695                0.0        96.0               NaN   \\n', '4             6700             1750.0       230.0             300.0   \\n', '\\n', '   Credit_History Property_Area  Loan_Status  \\n', '0             1.0     Semiurban            1  \\n', '1             1.0         Rural            1  \\n', '2             1.0         Urban            0  \\n', '3             1.0         Urban            1  \\n', '4             1.0     Semiurban            1  ']\n",
      "['<div>\\n', '<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\\n', '<table border=\"1\" class=\"dataframe\">\\n', '  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>Loan_ID</th>\\n', '      <th>Gender</th>\\n', '      <th>Married</th>\\n', '      <th>Dependents</th>\\n', '      <th>Education</th>\\n', '      <th>Self_Employed</th>\\n', '      <th>ApplicantIncome</th>\\n', '      <th>CoapplicantIncome</th>\\n', '      <th>LoanAmount</th>\\n', '      <th>Loan_Amount_Term</th>\\n', '      <th>Credit_History</th>\\n', '      <th>Property_Area</th>\\n', '      <th>Loan_Status</th>\\n', '    </tr>\\n', '  </thead>\\n', '  <tbody>\\n', '    <tr>\\n', '      <th>0</th>\\n', '      <td>LP002305</td>\\n', '      <td>Female</td>\\n', '      <td>No</td>\\n', '      <td>0</td>\\n', '      <td>Graduate</td>\\n', '      <td>No</td>\\n', '      <td>4547</td>\\n', '      <td>0.0</td>\\n', '      <td>115.0</td>\\n', '      <td>360.0</td>\\n', '      <td>1.0</td>\\n', '      <td>Semiurban</td>\\n', '      <td>1</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>1</th>\\n', '      <td>LP001715</td>\\n', '      <td>Male</td>\\n', '      <td>Yes</td>\\n', '      <td>3+</td>\\n', '      <td>Not Graduate</td>\\n', '      <td>Yes</td>\\n', '      <td>5703</td>\\n', '      <td>0.0</td>\\n', '      <td>130.0</td>\\n', '      <td>360.0</td>\\n', '      <td>1.0</td>\\n', '      <td>Rural</td>\\n', '      <td>1</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>2</th>\\n', '      <td>LP002086</td>\\n', '      <td>Female</td>\\n', '      <td>Yes</td>\\n', '      <td>0</td>\\n', '      <td>Graduate</td>\\n', '      <td>No</td>\\n', '      <td>4333</td>\\n', '      <td>2451.0</td>\\n', '      <td>110.0</td>\\n', '      <td>360.0</td>\\n', '      <td>1.0</td>\\n', '      <td>Urban</td>\\n', '      <td>0</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>3</th>\\n', '      <td>LP001136</td>\\n', '      <td>Male</td>\\n', '      <td>Yes</td>\\n', '      <td>0</td>\\n', '      <td>Not Graduate</td>\\n', '      <td>Yes</td>\\n', '      <td>4695</td>\\n', '      <td>0.0</td>\\n', '      <td>96.0</td>\\n', '      <td>NaN</td>\\n', '      <td>1.0</td>\\n', '      <td>Urban</td>\\n', '      <td>1</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>4</th>\\n', '      <td>LP002529</td>\\n', '      <td>Male</td>\\n', '      <td>Yes</td>\\n', '      <td>2</td>\\n', '      <td>Graduate</td>\\n', '      <td>No</td>\\n', '      <td>6700</td>\\n', '      <td>1750.0</td>\\n', '      <td>230.0</td>\\n', '      <td>300.0</td>\\n', '      <td>1.0</td>\\n', '      <td>Semiurban</td>\\n', '      <td>1</td>\\n', '    </tr>\\n', '  </tbody>\\n', '</table>\\n', '</div>']\n",
      "----------------------------------------\n",
      "\n",
      "Cell #14:\n",
      "Code:\n",
      "# Data dimension\n",
      "print('Data dimension: {} rows and {} columns'.format(len(df_test), len(df_test.columns)))\n",
      "df_test.head()\n",
      "\n",
      "Outputs:\n",
      "Data dimension: 123 rows and 12 columns\n",
      "\n",
      "Data dimension: 123 rows and 12 columns\n",
      "\n",
      "['    Loan_ID Gender Married Dependents     Education Self_Employed  \\\\\\n', '0  LP001116   Male      No          0  Not Graduate            No   \\n', '1  LP001488   Male     Yes         3+      Graduate            No   \\n', '2  LP002138   Male     Yes          0      Graduate            No   \\n', '3  LP002284   Male      No          0  Not Graduate            No   \\n', '4  LP002328   Male     Yes          0  Not Graduate            No   \\n', '\\n', '   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\\\\n', '0             3748             1668.0       110.0             360.0   \\n', '1             4000             7750.0       290.0             360.0   \\n', '2             2625             6250.0       187.0             360.0   \\n', '3             3902             1666.0       109.0             360.0   \\n', '4             6096                0.0       218.0             360.0   \\n', '\\n', '   Credit_History Property_Area  \\n', '0             1.0     Semiurban  \\n', '1             1.0     Semiurban  \\n', '2             1.0         Rural  \\n', '3             1.0         Rural  \\n', '4             0.0         Rural  ']\n",
      "['<div>\\n', '<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\\n', '<table border=\"1\" class=\"dataframe\">\\n', '  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>Loan_ID</th>\\n', '      <th>Gender</th>\\n', '      <th>Married</th>\\n', '      <th>Dependents</th>\\n', '      <th>Education</th>\\n', '      <th>Self_Employed</th>\\n', '      <th>ApplicantIncome</th>\\n', '      <th>CoapplicantIncome</th>\\n', '      <th>LoanAmount</th>\\n', '      <th>Loan_Amount_Term</th>\\n', '      <th>Credit_History</th>\\n', '      <th>Property_Area</th>\\n', '    </tr>\\n', '  </thead>\\n', '  <tbody>\\n', '    <tr>\\n', '      <th>0</th>\\n', '      <td>LP001116</td>\\n', '      <td>Male</td>\\n', '      <td>No</td>\\n', '      <td>0</td>\\n', '      <td>Not Graduate</td>\\n', '      <td>No</td>\\n', '      <td>3748</td>\\n', '      <td>1668.0</td>\\n', '      <td>110.0</td>\\n', '      <td>360.0</td>\\n', '      <td>1.0</td>\\n', '      <td>Semiurban</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>1</th>\\n', '      <td>LP001488</td>\\n', '      <td>Male</td>\\n', '      <td>Yes</td>\\n', '      <td>3+</td>\\n', '      <td>Graduate</td>\\n', '      <td>No</td>\\n', '      <td>4000</td>\\n', '      <td>7750.0</td>\\n', '      <td>290.0</td>\\n', '      <td>360.0</td>\\n', '      <td>1.0</td>\\n', '      <td>Semiurban</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>2</th>\\n', '      <td>LP002138</td>\\n', '      <td>Male</td>\\n', '      <td>Yes</td>\\n', '      <td>0</td>\\n', '      <td>Graduate</td>\\n', '      <td>No</td>\\n', '      <td>2625</td>\\n', '      <td>6250.0</td>\\n', '      <td>187.0</td>\\n', '      <td>360.0</td>\\n', '      <td>1.0</td>\\n', '      <td>Rural</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>3</th>\\n', '      <td>LP002284</td>\\n', '      <td>Male</td>\\n', '      <td>No</td>\\n', '      <td>0</td>\\n', '      <td>Not Graduate</td>\\n', '      <td>No</td>\\n', '      <td>3902</td>\\n', '      <td>1666.0</td>\\n', '      <td>109.0</td>\\n', '      <td>360.0</td>\\n', '      <td>1.0</td>\\n', '      <td>Rural</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>4</th>\\n', '      <td>LP002328</td>\\n', '      <td>Male</td>\\n', '      <td>Yes</td>\\n', '      <td>0</td>\\n', '      <td>Not Graduate</td>\\n', '      <td>No</td>\\n', '      <td>6096</td>\\n', '      <td>0.0</td>\\n', '      <td>218.0</td>\\n', '      <td>360.0</td>\\n', '      <td>0.0</td>\\n', '      <td>Rural</td>\\n', '    </tr>\\n', '  </tbody>\\n', '</table>\\n', '</div>']\n",
      "----------------------------------------\n",
      "\n",
      "Cell #19:\n",
      "Code:\n",
      "# Data frame metadata\n",
      "df_train.info()\n",
      "\n",
      "Outputs:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 491 entries, 0 to 490\n",
      "Data columns (total 13 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Loan_ID            491 non-null    object \n",
      " 1   Gender             481 non-null    object \n",
      " 2   Married            490 non-null    object \n",
      " 3   Dependents         482 non-null    object \n",
      " 4   Education          491 non-null    object \n",
      " 5   Self_Employed      462 non-null    object \n",
      " 6   ApplicantIncome    491 non-null    int64  \n",
      " 7   CoapplicantIncome  491 non-null    float64\n",
      " 8   LoanAmount         475 non-null    float64\n",
      " 9   Loan_Amount_Term   478 non-null    float64\n",
      " 10  Credit_History     448 non-null    float64\n",
      " 11  Property_Area      491 non-null    object \n",
      " 12  Loan_Status        491 non-null    int64  \n",
      "dtypes: float64(4), int64(2), object(7)\n",
      "memory usage: 50.0+ KB\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 491 entries, 0 to 490\n",
      "Data columns (total 13 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Loan_ID            491 non-null    object \n",
      " 1   Gender             481 non-null    object \n",
      " 2   Married            490 non-null    object \n",
      " 3   Dependents         482 non-null    object \n",
      " 4   Education          491 non-null    object \n",
      " 5   Self_Employed      462 non-null    object \n",
      " 6   ApplicantIncome    491 non-null    int64  \n",
      " 7   CoapplicantIncome  491 non-null    float64\n",
      " 8   LoanAmount         475 non-null    float64\n",
      " 9   Loan_Amount_Term   478 non-null    float64\n",
      " 10  Credit_History     448 non-null    float64\n",
      " 11  Property_Area      491 non-null    object \n",
      " 12  Loan_Status        491 non-null    int64  \n",
      "dtypes: float64(4), int64(2), object(7)\n",
      "memory usage: 50.0+ KB\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Cell #20:\n",
      "Code:\n",
      "# Change column types\n",
      "df_train = df_train.astype({'Credit_History': object, 'Loan_Status': int})\n",
      "df_train.select_dtypes(include = ['object']).dtypes\n",
      "\n",
      "Outputs:\n",
      "['Loan_ID           object\\n', 'Gender            object\\n', 'Married           object\\n', 'Dependents        object\\n', 'Education         object\\n', 'Self_Employed     object\\n', 'Credit_History    object\\n', 'Property_Area     object\\n', 'dtype: object']\n",
      "----------------------------------------\n",
      "\n",
      "Cell #21:\n",
      "Code:\n",
      "# Summary statistics of categorical columns\n",
      "for i in df_train.select_dtypes('object').columns:\n",
      "    print(df_train[i].value_counts(),'\\n')\n",
      "\n",
      "Outputs:\n",
      "Loan_ID\n",
      "LP002777    1\n",
      "LP002305    1\n",
      "LP001715    1\n",
      "LP002086    1\n",
      "LP001136    1\n",
      "           ..\n",
      "LP002379    1\n",
      "LP001011    1\n",
      "LP001508    1\n",
      "LP001112    1\n",
      "LP002130    1\n",
      "Name: count, Length: 491, dtype: int64 \n",
      "\n",
      "Gender\n",
      "Male      393\n",
      "Female     88\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Married\n",
      "Yes    324\n",
      "No     166\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Dependents\n",
      "0     276\n",
      "1      85\n",
      "2      78\n",
      "3+     43\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Education\n",
      "Graduate        388\n",
      "Not Graduate    103\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Self_Employed\n",
      "No     398\n",
      "Yes     64\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Credit_History\n",
      "1.0    380\n",
      "0.0     68\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Property_Area\n",
      "Semiurban    186\n",
      "Urban        155\n",
      "Rural        150\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "\n",
      "Loan_ID\n",
      "LP002777    1\n",
      "LP002305    1\n",
      "LP001715    1\n",
      "LP002086    1\n",
      "LP001136    1\n",
      "           ..\n",
      "LP002379    1\n",
      "LP001011    1\n",
      "LP001508    1\n",
      "LP001112    1\n",
      "LP002130    1\n",
      "Name: count, Length: 491, dtype: int64 \n",
      "\n",
      "Gender\n",
      "Male      393\n",
      "Female     88\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Married\n",
      "Yes    324\n",
      "No     166\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Dependents\n",
      "0     276\n",
      "1      85\n",
      "2      78\n",
      "3+     43\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Education\n",
      "Graduate        388\n",
      "Not Graduate    103\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Self_Employed\n",
      "No     398\n",
      "Yes     64\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Credit_History\n",
      "1.0    380\n",
      "0.0     68\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Property_Area\n",
      "Semiurban    186\n",
      "Urban        155\n",
      "Rural        150\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Cell #23:\n",
      "Code:\n",
      "# Check missing values\n",
      "df_train.isna().sum()\n",
      "\n",
      "Outputs:\n",
      "['Loan_ID               0\\n', 'Gender               10\\n', 'Married               1\\n', 'Dependents            9\\n', 'Education             0\\n', 'Self_Employed        29\\n', 'ApplicantIncome       0\\n', 'CoapplicantIncome     0\\n', 'LoanAmount           16\\n', 'Loan_Amount_Term     13\\n', 'Credit_History       43\\n', 'Property_Area         0\\n', 'Loan_Status           0\\n', 'dtype: int64']\n",
      "----------------------------------------\n",
      "\n",
      "Cell #26:\n",
      "Code:\n",
      "print('Number of missing dependents is about {} rows'.format(df_train['Dependents'].isna().sum()))\n",
      "\n",
      "Outputs:\n",
      "Number of missing dependents is about 9 rows\n",
      "\n",
      "Number of missing dependents is about 9 rows\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Cell #29:\n",
      "Code:\n",
      "print('Number of missing Self_Employed is about {} rows'.format(df_train['Self_Employed'].isna().sum()))\n",
      "\n",
      "Outputs:\n",
      "Number of missing Self_Employed is about 29 rows\n",
      "\n",
      "Number of missing Self_Employed is about 29 rows\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Cell #32:\n",
      "Code:\n",
      "df_train[['Loan_Amount_Term', 'Loan_Status']].groupby('Loan_Status').describe()\n",
      "\n",
      "Outputs:\n",
      "['            Loan_Amount_Term                                             \\\\\\n', '                       count        mean        std   min    25%    50%   \\n', 'Loan_Status                                                               \\n', '0                      143.0  341.790210  73.018891  36.0  360.0  360.0   \\n', '1                      335.0  341.086567  64.320411  12.0  360.0  360.0   \\n', '\\n', '                           \\n', '               75%    max  \\n', 'Loan_Status                \\n', '0            360.0  480.0  \\n', '1            360.0  480.0  ']\n",
      "['<div>\\n', '<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead tr th {\\n', '        text-align: left;\\n', '    }\\n', '\\n', '    .dataframe thead tr:last-of-type th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\\n', '<table border=\"1\" class=\"dataframe\">\\n', '  <thead>\\n', '    <tr>\\n', '      <th></th>\\n', '      <th colspan=\"8\" halign=\"left\">Loan_Amount_Term</th>\\n', '    </tr>\\n', '    <tr>\\n', '      <th></th>\\n', '      <th>count</th>\\n', '      <th>mean</th>\\n', '      <th>std</th>\\n', '      <th>min</th>\\n', '      <th>25%</th>\\n', '      <th>50%</th>\\n', '      <th>75%</th>\\n', '      <th>max</th>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>Loan_Status</th>\\n', '      <th></th>\\n', '      <th></th>\\n', '      <th></th>\\n', '      <th></th>\\n', '      <th></th>\\n', '      <th></th>\\n', '      <th></th>\\n', '      <th></th>\\n', '    </tr>\\n', '  </thead>\\n', '  <tbody>\\n', '    <tr>\\n', '      <th>0</th>\\n', '      <td>143.0</td>\\n', '      <td>341.790210</td>\\n', '      <td>73.018891</td>\\n', '      <td>36.0</td>\\n', '      <td>360.0</td>\\n', '      <td>360.0</td>\\n', '      <td>360.0</td>\\n', '      <td>480.0</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>1</th>\\n', '      <td>335.0</td>\\n', '      <td>341.086567</td>\\n', '      <td>64.320411</td>\\n', '      <td>12.0</td>\\n', '      <td>360.0</td>\\n', '      <td>360.0</td>\\n', '      <td>360.0</td>\\n', '      <td>480.0</td>\\n', '    </tr>\\n', '  </tbody>\\n', '</table>\\n', '</div>']\n",
      "----------------------------------------\n",
      "\n",
      "Cell #33:\n",
      "Code:\n",
      "print('Percentile 20th: {}'.format(df_train['Loan_Amount_Term'].quantile(q = 0.2)))\n",
      "\n",
      "Outputs:\n",
      "Percentile 20th: 360.0\n",
      "\n",
      "Percentile 20th: 360.0\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Cell #36:\n",
      "Code:\n",
      "# Cross tabulation of credit history and loan status\n",
      "df_cred_hist = pd.crosstab(df_train['Credit_History'], df_train['Loan_Status'], margins = True).reset_index()\n",
      "# Remove index name\n",
      "df_cred_hist.columns.name = None\n",
      "# Remove last row for total column attribute\n",
      "df_cred_hist = df_cred_hist.drop([len(df_cred_hist) - 1], axis = 0)\n",
      "df_cred_hist.rename(columns = {'Credit_History':'Credit History', 0:'No', 1:'Yes'}, inplace = True)\n",
      "df_cred_hist\n",
      "\n",
      "Outputs:\n",
      "['  Credit History  No  Yes  All\\n', '0            0.0  62    6   68\\n', '1            1.0  74  306  380']\n",
      "['<div>\\n', '<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\\n', '<table border=\"1\" class=\"dataframe\">\\n', '  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>Credit History</th>\\n', '      <th>No</th>\\n', '      <th>Yes</th>\\n', '      <th>All</th>\\n', '    </tr>\\n', '  </thead>\\n', '  <tbody>\\n', '    <tr>\\n', '      <th>0</th>\\n', '      <td>0.0</td>\\n', '      <td>62</td>\\n', '      <td>6</td>\\n', '      <td>68</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>1</th>\\n', '      <td>1.0</td>\\n', '      <td>74</td>\\n', '      <td>306</td>\\n', '      <td>380</td>\\n', '    </tr>\\n', '  </tbody>\\n', '</table>\\n', '</div>']\n",
      "----------------------------------------\n",
      "\n",
      "Cell #37:\n",
      "Code:\n",
      "# Slice the data frame based on loan status\n",
      "pos_cred_hist0 = df_train[(df_train['Credit_History'].isna()) & (df_train['Loan_Status'] == 0)]\n",
      "pos_cred_hist1 = df_train[(df_train['Credit_History'].isna()) & (df_train['Loan_Status'] == 1)]\n",
      "print('Number of rows with Loan_Status is No but Credit_History is NaN  : {}'.format(len(pos_cred_hist0)))\n",
      "print('Number of rows with Loan_Status is Yes but Credit_History is NaN : {}'.format(len(pos_cred_hist1)))\n",
      "\n",
      "Outputs:\n",
      "Number of rows with Loan_Status is No but Credit_History is NaN  : 12\n",
      "Number of rows with Loan_Status is Yes but Credit_History is NaN : 31\n",
      "\n",
      "Number of rows with Loan_Status is No but Credit_History is NaN  : 12\n",
      "Number of rows with Loan_Status is Yes but Credit_History is NaN : 31\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Cell #41:\n",
      "Code:\n",
      "# Check missing value\n",
      "df_train.isna().sum()\n",
      "\n",
      "Outputs:\n",
      "['Loan_ID              0\\n', 'Gender               0\\n', 'Married              0\\n', 'Dependents           0\\n', 'Education            0\\n', 'Self_Employed        0\\n', 'ApplicantIncome      0\\n', 'CoapplicantIncome    0\\n', 'LoanAmount           0\\n', 'Loan_Amount_Term     0\\n', 'Credit_History       0\\n', 'Property_Area        0\\n', 'Loan_Status          0\\n', 'dtype: int64']\n",
      "----------------------------------------\n",
      "\n",
      "Cell #44:\n",
      "Code:\n",
      "# Data frame metadata\n",
      "df_test.info()\n",
      "\n",
      "Outputs:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 123 entries, 0 to 122\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Loan_ID            123 non-null    object \n",
      " 1   Gender             120 non-null    object \n",
      " 2   Married            121 non-null    object \n",
      " 3   Dependents         117 non-null    object \n",
      " 4   Education          123 non-null    object \n",
      " 5   Self_Employed      120 non-null    object \n",
      " 6   ApplicantIncome    123 non-null    int64  \n",
      " 7   CoapplicantIncome  123 non-null    float64\n",
      " 8   LoanAmount         117 non-null    float64\n",
      " 9   Loan_Amount_Term   122 non-null    float64\n",
      " 10  Credit_History     116 non-null    float64\n",
      " 11  Property_Area      123 non-null    object \n",
      "dtypes: float64(4), int64(1), object(7)\n",
      "memory usage: 11.7+ KB\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 123 entries, 0 to 122\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Loan_ID            123 non-null    object \n",
      " 1   Gender             120 non-null    object \n",
      " 2   Married            121 non-null    object \n",
      " 3   Dependents         117 non-null    object \n",
      " 4   Education          123 non-null    object \n",
      " 5   Self_Employed      120 non-null    object \n",
      " 6   ApplicantIncome    123 non-null    int64  \n",
      " 7   CoapplicantIncome  123 non-null    float64\n",
      " 8   LoanAmount         117 non-null    float64\n",
      " 9   Loan_Amount_Term   122 non-null    float64\n",
      " 10  Credit_History     116 non-null    float64\n",
      " 11  Property_Area      123 non-null    object \n",
      "dtypes: float64(4), int64(1), object(7)\n",
      "memory usage: 11.7+ KB\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Cell #45:\n",
      "Code:\n",
      "# Change column types\n",
      "df_test = df_test.astype({'Credit_History': object})\n",
      "df_test.select_dtypes(include = ['object']).dtypes\n",
      "\n",
      "Outputs:\n",
      "['Loan_ID           object\\n', 'Gender            object\\n', 'Married           object\\n', 'Dependents        object\\n', 'Education         object\\n', 'Self_Employed     object\\n', 'Credit_History    object\\n', 'Property_Area     object\\n', 'dtype: object']\n",
      "----------------------------------------\n",
      "\n",
      "Cell #46:\n",
      "Code:\n",
      "# Summary statistics of categorical columns\n",
      "for i in df_test.select_dtypes('object').columns:\n",
      "    print(df_test[i].value_counts(),'\\n')\n",
      "\n",
      "Outputs:\n",
      "Loan_ID\n",
      "LP001116    1\n",
      "LP001488    1\n",
      "LP002138    1\n",
      "LP002284    1\n",
      "LP002328    1\n",
      "           ..\n",
      "LP002683    1\n",
      "LP002054    1\n",
      "LP002757    1\n",
      "LP002582    1\n",
      "LP001616    1\n",
      "Name: count, Length: 123, dtype: int64 \n",
      "\n",
      "Gender\n",
      "Male      96\n",
      "Female    24\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Married\n",
      "Yes    74\n",
      "No     47\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Dependents\n",
      "0     69\n",
      "2     23\n",
      "1     17\n",
      "3+     8\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Education\n",
      "Graduate        92\n",
      "Not Graduate    31\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Self_Employed\n",
      "No     102\n",
      "Yes     18\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Credit_History\n",
      "1.0    95\n",
      "0.0    21\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Property_Area\n",
      "Semiurban    47\n",
      "Urban        47\n",
      "Rural        29\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "\n",
      "Loan_ID\n",
      "LP001116    1\n",
      "LP001488    1\n",
      "LP002138    1\n",
      "LP002284    1\n",
      "LP002328    1\n",
      "           ..\n",
      "LP002683    1\n",
      "LP002054    1\n",
      "LP002757    1\n",
      "LP002582    1\n",
      "LP001616    1\n",
      "Name: count, Length: 123, dtype: int64 \n",
      "\n",
      "Gender\n",
      "Male      96\n",
      "Female    24\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Married\n",
      "Yes    74\n",
      "No     47\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Dependents\n",
      "0     69\n",
      "2     23\n",
      "1     17\n",
      "3+     8\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Education\n",
      "Graduate        92\n",
      "Not Graduate    31\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Self_Employed\n",
      "No     102\n",
      "Yes     18\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Credit_History\n",
      "1.0    95\n",
      "0.0    21\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Property_Area\n",
      "Semiurban    47\n",
      "Urban        47\n",
      "Rural        29\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Cell #48:\n",
      "Code:\n",
      "# Check missing values\n",
      "df_test.isna().sum()\n",
      "\n",
      "Outputs:\n",
      "['Loan_ID              0\\n', 'Gender               3\\n', 'Married              2\\n', 'Dependents           6\\n', 'Education            0\\n', 'Self_Employed        3\\n', 'ApplicantIncome      0\\n', 'CoapplicantIncome    0\\n', 'LoanAmount           6\\n', 'Loan_Amount_Term     1\\n', 'Credit_History       7\\n', 'Property_Area        0\\n', 'dtype: int64']\n",
      "----------------------------------------\n",
      "\n",
      "Cell #50:\n",
      "Code:\n",
      "print('Number of missing values in Dependents is about {} rows'.format(df_test['Dependents'].isna().sum()))\n",
      "\n",
      "Outputs:\n",
      "Number of missing values in Dependents is about 6 rows\n",
      "\n",
      "Number of missing values in Dependents is about 6 rows\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Cell #53:\n",
      "Code:\n",
      "print('Number of missing values in Self_Employed is about {} rows'.format(df_test['Self_Employed'].isna().sum()))\n",
      "\n",
      "Outputs:\n",
      "Number of missing values in Self_Employed is about 3 rows\n",
      "\n",
      "Number of missing values in Self_Employed is about 3 rows\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Cell #59:\n",
      "Code:\n",
      "# Check missing values\n",
      "df_test.isna().sum()\n",
      "\n",
      "Outputs:\n",
      "['Loan_ID              0\\n', 'Gender               0\\n', 'Married              0\\n', 'Dependents           0\\n', 'Education            0\\n', 'Self_Employed        0\\n', 'ApplicantIncome      0\\n', 'CoapplicantIncome    0\\n', 'LoanAmount           0\\n', 'Loan_Amount_Term     0\\n', 'Credit_History       0\\n', 'Property_Area        0\\n', 'dtype: int64']\n",
      "----------------------------------------\n",
      "\n",
      "Cell #63:\n",
      "Code:\n",
      "# Show the data\n",
      "df_viz_1\n",
      "\n",
      "Outputs:\n",
      "['   Loan_Status  Total\\n', '0  Not default    134\\n', '1      Default    330']\n",
      "['<div>\\n', '<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\\n', '<table border=\"1\" class=\"dataframe\">\\n', '  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>Loan_Status</th>\\n', '      <th>Total</th>\\n', '    </tr>\\n', '  </thead>\\n', '  <tbody>\\n', '    <tr>\\n', '      <th>0</th>\\n', '      <td>Not default</td>\\n', '      <td>134</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>1</th>\\n', '      <td>Default</td>\\n', '      <td>330</td>\\n', '    </tr>\\n', '  </tbody>\\n', '</table>\\n', '</div>']\n",
      "----------------------------------------\n",
      "\n",
      "Cell #64:\n",
      "Code:\n",
      "# Figure size\n",
      "plt.figure(figsize = (6.4,4.8))\n",
      "\n",
      "# Customize colors and other settings\n",
      "colors = ['#80797c','#981220']\n",
      "\n",
      "# Explode 1st slice\n",
      "explode = (0.1, 0)\n",
      "\n",
      "# Create a pie chart\n",
      "plt.pie(\n",
      "    x = 'Total',\n",
      "    labels = 'Loan_Status',\n",
      "    data = df_viz_1,\n",
      "    explode = explode,\n",
      "    colors = colors,\n",
      "    autopct = '%1.1f%%',\n",
      "    shadow = False,\n",
      "    startangle = 140\n",
      ")\n",
      "\n",
      "# Title and axis\n",
      "plt.title('Number of customers by loan status', fontsize = 18)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "\n",
      "Outputs:\n",
      "['<Figure size 640x480 with 1 Axes>']\n",
      "----------------------------------------\n",
      "\n",
      "Cell #67:\n",
      "Code:\n",
      "# Show the data\n",
      "df_viz_2\n",
      "\n",
      "Outputs:\n",
      "['   Loan_Status Dependents  Total\\n', '0  Not default          0     77\\n', '1  Not default          1     30\\n', '2  Not default          2     13\\n', '3  Not default         3+     14\\n', '4      Default          0    191\\n', '5      Default          1     52\\n', '6      Default          2     62\\n', '7      Default         3+     25']\n",
      "['<div>\\n', '<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\\n', '<table border=\"1\" class=\"dataframe\">\\n', '  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>Loan_Status</th>\\n', '      <th>Dependents</th>\\n', '      <th>Total</th>\\n', '    </tr>\\n', '  </thead>\\n', '  <tbody>\\n', '    <tr>\\n', '      <th>0</th>\\n', '      <td>Not default</td>\\n', '      <td>0</td>\\n', '      <td>77</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>1</th>\\n', '      <td>Not default</td>\\n', '      <td>1</td>\\n', '      <td>30</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>2</th>\\n', '      <td>Not default</td>\\n', '      <td>2</td>\\n', '      <td>13</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>3</th>\\n', '      <td>Not default</td>\\n', '      <td>3+</td>\\n', '      <td>14</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>4</th>\\n', '      <td>Default</td>\\n', '      <td>0</td>\\n', '      <td>191</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>5</th>\\n', '      <td>Default</td>\\n', '      <td>1</td>\\n', '      <td>52</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>6</th>\\n', '      <td>Default</td>\\n', '      <td>2</td>\\n', '      <td>62</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>7</th>\\n', '      <td>Default</td>\\n', '      <td>3+</td>\\n', '      <td>25</td>\\n', '    </tr>\\n', '  </tbody>\\n', '</table>\\n', '</div>']\n",
      "----------------------------------------\n",
      "\n",
      "Cell #68:\n",
      "Code:\n",
      "plotnine.options.figure_size = (8, 4.8)\n",
      "(\n",
      "    ggplot(\n",
      "        data = df_viz_2\n",
      "    )+\n",
      "    geom_bar(\n",
      "        aes(\n",
      "            x = 'Dependents',\n",
      "            y = 'Total',\n",
      "            fill = 'Loan_Status'\n",
      "        ),\n",
      "        stat = 'identity',\n",
      "        position = 'fill',\n",
      "        width = 0.5\n",
      "    )+\n",
      "    labs(\n",
      "        title = 'The composition of loan status by the dependents',\n",
      "        fill = 'Loan status'\n",
      "    )+\n",
      "    xlab(\n",
      "        'Dependents'\n",
      "    )+\n",
      "    ylab(\n",
      "        'Frequency'\n",
      "    )+\n",
      "    scale_x_discrete(\n",
      "        limits = ['0', '1', '2', '3+']\n",
      "    )+\n",
      "    scale_fill_manual(\n",
      "        values = ['#981220','#80797c'],\n",
      "        labels = ['Default', 'Not Default']\n",
      "    )+\n",
      "    theme_minimal()\n",
      ")\n",
      "\n",
      "Outputs:\n",
      "----------------------------------------\n",
      "\n",
      "Cell #71:\n",
      "Code:\n",
      "# Show the data\n",
      "df_viz_3\n",
      "\n",
      "Outputs:\n",
      "['   Loan_Status     Education  Total\\n', '0  Not default      Graduate    101\\n', '1  Not default  Not Graduate     33\\n', '2      Default      Graduate    266\\n', '3      Default  Not Graduate     64']\n",
      "['<div>\\n', '<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\\n', '<table border=\"1\" class=\"dataframe\">\\n', '  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>Loan_Status</th>\\n', '      <th>Education</th>\\n', '      <th>Total</th>\\n', '    </tr>\\n', '  </thead>\\n', '  <tbody>\\n', '    <tr>\\n', '      <th>0</th>\\n', '      <td>Not default</td>\\n', '      <td>Graduate</td>\\n', '      <td>101</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>1</th>\\n', '      <td>Not default</td>\\n', '      <td>Not Graduate</td>\\n', '      <td>33</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>2</th>\\n', '      <td>Default</td>\\n', '      <td>Graduate</td>\\n', '      <td>266</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>3</th>\\n', '      <td>Default</td>\\n', '      <td>Not Graduate</td>\\n', '      <td>64</td>\\n', '    </tr>\\n', '  </tbody>\\n', '</table>\\n', '</div>']\n",
      "----------------------------------------\n",
      "\n",
      "Cell #72:\n",
      "Code:\n",
      "plotnine.options.figure_size = (8, 4.8)\n",
      "(\n",
      "    ggplot(\n",
      "        data = df_viz_3\n",
      "    )+\n",
      "    geom_bar(\n",
      "        aes(\n",
      "            x = 'Education',\n",
      "            y = 'Total',\n",
      "            fill = 'Loan_Status'\n",
      "        ),\n",
      "        stat = 'identity',\n",
      "        position = 'fill',\n",
      "        width = 0.5\n",
      "    )+\n",
      "    labs(\n",
      "        title = 'The composition of loan status by the education',\n",
      "        fill = 'Loan status'\n",
      "    )+\n",
      "    xlab(\n",
      "        'Educations'\n",
      "    )+\n",
      "    ylab(\n",
      "        'Frequency'\n",
      "    )+\n",
      "    scale_x_discrete(\n",
      "        limits = ['Graduate', 'Not Graduate']\n",
      "    )+\n",
      "    scale_fill_manual(\n",
      "        values = ['#981220','#80797c'],\n",
      "        labels = ['Default', 'Not Default']\n",
      "    )+\n",
      "    theme_minimal()\n",
      ")\n",
      "\n",
      "Outputs:\n",
      "----------------------------------------\n",
      "\n",
      "Cell #75:\n",
      "Code:\n",
      "# Show the data\n",
      "df_viz_4.head()\n",
      "\n",
      "Outputs:\n",
      "['   ApplicantIncome  Loan_Status\\n', '0             4547      Default\\n', '1             5703      Default\\n', '2             4333  Not default\\n', '3             4695      Default\\n', '4             6700      Default']\n",
      "['<div>\\n', '<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\\n', '<table border=\"1\" class=\"dataframe\">\\n', '  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>ApplicantIncome</th>\\n', '      <th>Loan_Status</th>\\n', '    </tr>\\n', '  </thead>\\n', '  <tbody>\\n', '    <tr>\\n', '      <th>0</th>\\n', '      <td>4547</td>\\n', '      <td>Default</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>1</th>\\n', '      <td>5703</td>\\n', '      <td>Default</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>2</th>\\n', '      <td>4333</td>\\n', '      <td>Not default</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>3</th>\\n', '      <td>4695</td>\\n', '      <td>Default</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>4</th>\\n', '      <td>6700</td>\\n', '      <td>Default</td>\\n', '    </tr>\\n', '  </tbody>\\n', '</table>\\n', '</div>']\n",
      "----------------------------------------\n",
      "\n",
      "Cell #76:\n",
      "Code:\n",
      "plotnine.options.figure_size = (8, 4.8)\n",
      "(\n",
      "    ggplot(\n",
      "        data = df_viz_4\n",
      "    )+\n",
      "    geom_density(\n",
      "        aes(\n",
      "            x = 'ApplicantIncome',\n",
      "            fill = 'Loan_Status'\n",
      "        ),\n",
      "        color = 'white',\n",
      "        alpha = 0.85\n",
      "    )+\n",
      "    labs(\n",
      "        title = 'The distribution of applicant incomes by loan status'\n",
      "    )+\n",
      "    scale_fill_manual(\n",
      "        name = 'Loan Status',\n",
      "        values = ['#981220','#80797c'],\n",
      "        labels = ['Default', 'Not Default']\n",
      "    )+\n",
      "    xlab(\n",
      "        'Applicant income'\n",
      "    )+\n",
      "    ylab(\n",
      "        'Density'\n",
      "    )+\n",
      "    theme_minimal()\n",
      ")\n",
      "\n",
      "Outputs:\n",
      "----------------------------------------\n",
      "\n",
      "Cell #79:\n",
      "Code:\n",
      "# Show the data\n",
      "df_viz_5.head()\n",
      "\n",
      "Outputs:\n",
      "['   LoanAmount  Loan_Status\\n', '0       115.0      Default\\n', '1       130.0      Default\\n', '2       110.0  Not default\\n', '3        96.0      Default\\n', '4       230.0      Default']\n",
      "['<div>\\n', '<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\\n', '<table border=\"1\" class=\"dataframe\">\\n', '  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>LoanAmount</th>\\n', '      <th>Loan_Status</th>\\n', '    </tr>\\n', '  </thead>\\n', '  <tbody>\\n', '    <tr>\\n', '      <th>0</th>\\n', '      <td>115.0</td>\\n', '      <td>Default</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>1</th>\\n', '      <td>130.0</td>\\n', '      <td>Default</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>2</th>\\n', '      <td>110.0</td>\\n', '      <td>Not default</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>3</th>\\n', '      <td>96.0</td>\\n', '      <td>Default</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>4</th>\\n', '      <td>230.0</td>\\n', '      <td>Default</td>\\n', '    </tr>\\n', '  </tbody>\\n', '</table>\\n', '</div>']\n",
      "----------------------------------------\n",
      "\n",
      "Cell #80:\n",
      "Code:\n",
      "plotnine.options.figure_size = (8, 4.8)\n",
      "(\n",
      "    ggplot(\n",
      "        data = df_viz_5\n",
      "    )+\n",
      "    geom_density(\n",
      "        aes(\n",
      "            x = 'LoanAmount',\n",
      "            fill = 'Loan_Status'\n",
      "        ),\n",
      "        color = 'white',\n",
      "        alpha = 0.85\n",
      "    )+\n",
      "    labs(\n",
      "        title = 'The distribution of loan amount by loan status'\n",
      "    )+\n",
      "    scale_fill_manual(\n",
      "        name = 'Loan Status',\n",
      "        values = ['#981220','#80797c'],\n",
      "        labels = ['Default', 'Not Default']\n",
      "    )+\n",
      "    xlab(\n",
      "        'Loan amount'\n",
      "    )+\n",
      "    ylab(\n",
      "        'Density'\n",
      "    )+\n",
      "    theme_minimal()\n",
      ")\n",
      "\n",
      "Outputs:\n",
      "----------------------------------------\n",
      "\n",
      "Cell #84:\n",
      "Code:\n",
      "# Categorical columns\n",
      "cols_obj_train = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Credit_History', 'Property_Area']\n",
      "print(cols_obj_train)\n",
      "\n",
      "Outputs:\n",
      "['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Credit_History', 'Property_Area']\n",
      "\n",
      "['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Credit_History', 'Property_Area']\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Cell #85:\n",
      "Code:\n",
      "# One-hot encoding\n",
      "df_concat = pd.get_dummies(data = df_concat, columns = cols_obj_train, drop_first = True)\n",
      "print('Dimension data: {} rows and {} columns'.format(len(df_concat), len(df_concat.columns)))\n",
      "df_concat.head()\n",
      "\n",
      "Outputs:\n",
      "Dimension data: 570 rows and 15 columns\n",
      "\n",
      "Dimension data: 570 rows and 15 columns\n",
      "\n",
      "['   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\\\\n', '0             4547                0.0       115.0             360.0   \\n', '1             5703                0.0       130.0             360.0   \\n', '2             4333             2451.0       110.0             360.0   \\n', '3             4695                0.0        96.0             360.0   \\n', '4             6700             1750.0       230.0             300.0   \\n', '\\n', '   Loan_Status  Gender_Male  Married_Yes  Dependents_1  Dependents_2  \\\\\\n', '0            1        False        False         False         False   \\n', '1            1         True         True         False         False   \\n', '2            0        False         True         False         False   \\n', '3            1         True         True         False         False   \\n', '4            1         True         True         False          True   \\n', '\\n', '   Dependents_3+  Education_Not Graduate  Self_Employed_Yes  \\\\\\n', '0          False                   False              False   \\n', '1           True                    True               True   \\n', '2          False                   False              False   \\n', '3          False                    True               True   \\n', '4          False                   False              False   \\n', '\\n', '   Credit_History_1.0  Property_Area_Semiurban  Property_Area_Urban  \\n', '0                True                     True                False  \\n', '1                True                    False                False  \\n', '2                True                    False                 True  \\n', '3                True                    False                 True  \\n', '4                True                     True                False  ']\n",
      "['<div>\\n', '<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\\n', '<table border=\"1\" class=\"dataframe\">\\n', '  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>ApplicantIncome</th>\\n', '      <th>CoapplicantIncome</th>\\n', '      <th>LoanAmount</th>\\n', '      <th>Loan_Amount_Term</th>\\n', '      <th>Loan_Status</th>\\n', '      <th>Gender_Male</th>\\n', '      <th>Married_Yes</th>\\n', '      <th>Dependents_1</th>\\n', '      <th>Dependents_2</th>\\n', '      <th>Dependents_3+</th>\\n', '      <th>Education_Not Graduate</th>\\n', '      <th>Self_Employed_Yes</th>\\n', '      <th>Credit_History_1.0</th>\\n', '      <th>Property_Area_Semiurban</th>\\n', '      <th>Property_Area_Urban</th>\\n', '    </tr>\\n', '  </thead>\\n', '  <tbody>\\n', '    <tr>\\n', '      <th>0</th>\\n', '      <td>4547</td>\\n', '      <td>0.0</td>\\n', '      <td>115.0</td>\\n', '      <td>360.0</td>\\n', '      <td>1</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>True</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>1</th>\\n', '      <td>5703</td>\\n', '      <td>0.0</td>\\n', '      <td>130.0</td>\\n', '      <td>360.0</td>\\n', '      <td>1</td>\\n', '      <td>True</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>True</td>\\n', '      <td>True</td>\\n', '      <td>True</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>2</th>\\n', '      <td>4333</td>\\n', '      <td>2451.0</td>\\n', '      <td>110.0</td>\\n', '      <td>360.0</td>\\n', '      <td>0</td>\\n', '      <td>False</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '      <td>True</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>3</th>\\n', '      <td>4695</td>\\n', '      <td>0.0</td>\\n', '      <td>96.0</td>\\n', '      <td>360.0</td>\\n', '      <td>1</td>\\n', '      <td>True</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>True</td>\\n', '      <td>True</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '      <td>True</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>4</th>\\n', '      <td>6700</td>\\n', '      <td>1750.0</td>\\n', '      <td>230.0</td>\\n', '      <td>300.0</td>\\n', '      <td>1</td>\\n', '      <td>True</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>True</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '    </tr>\\n', '  </tbody>\\n', '</table>\\n', '</div>']\n",
      "----------------------------------------\n",
      "\n",
      "Cell #87:\n",
      "Code:\n",
      "# Unique values of Loan_Status\n",
      "df_concat['Loan_Status'].value_counts()\n",
      "\n",
      "Outputs:\n",
      "['Loan_Status\\n', '1      330\\n', '0      134\\n', '999    106\\n', 'Name: count, dtype: int64']\n",
      "----------------------------------------\n",
      "\n",
      "Cell #88:\n",
      "Code:\n",
      "# Training set\n",
      "df_train = df_concat[df_concat['Loan_Status'].isin([0, 1])].reset_index(drop = True)\n",
      "print('Dimension data: {} rows and {} columns'.format(len(df_train), len(df_train.columns)))\n",
      "df_train.head()\n",
      "\n",
      "Outputs:\n",
      "Dimension data: 464 rows and 15 columns\n",
      "\n",
      "Dimension data: 464 rows and 15 columns\n",
      "\n",
      "['   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\\\\n', '0             4547                0.0       115.0             360.0   \\n', '1             5703                0.0       130.0             360.0   \\n', '2             4333             2451.0       110.0             360.0   \\n', '3             4695                0.0        96.0             360.0   \\n', '4             6700             1750.0       230.0             300.0   \\n', '\\n', '   Loan_Status  Gender_Male  Married_Yes  Dependents_1  Dependents_2  \\\\\\n', '0            1        False        False         False         False   \\n', '1            1         True         True         False         False   \\n', '2            0        False         True         False         False   \\n', '3            1         True         True         False         False   \\n', '4            1         True         True         False          True   \\n', '\\n', '   Dependents_3+  Education_Not Graduate  Self_Employed_Yes  \\\\\\n', '0          False                   False              False   \\n', '1           True                    True               True   \\n', '2          False                   False              False   \\n', '3          False                    True               True   \\n', '4          False                   False              False   \\n', '\\n', '   Credit_History_1.0  Property_Area_Semiurban  Property_Area_Urban  \\n', '0                True                     True                False  \\n', '1                True                    False                False  \\n', '2                True                    False                 True  \\n', '3                True                    False                 True  \\n', '4                True                     True                False  ']\n",
      "['<div>\\n', '<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\\n', '<table border=\"1\" class=\"dataframe\">\\n', '  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>ApplicantIncome</th>\\n', '      <th>CoapplicantIncome</th>\\n', '      <th>LoanAmount</th>\\n', '      <th>Loan_Amount_Term</th>\\n', '      <th>Loan_Status</th>\\n', '      <th>Gender_Male</th>\\n', '      <th>Married_Yes</th>\\n', '      <th>Dependents_1</th>\\n', '      <th>Dependents_2</th>\\n', '      <th>Dependents_3+</th>\\n', '      <th>Education_Not Graduate</th>\\n', '      <th>Self_Employed_Yes</th>\\n', '      <th>Credit_History_1.0</th>\\n', '      <th>Property_Area_Semiurban</th>\\n', '      <th>Property_Area_Urban</th>\\n', '    </tr>\\n', '  </thead>\\n', '  <tbody>\\n', '    <tr>\\n', '      <th>0</th>\\n', '      <td>4547</td>\\n', '      <td>0.0</td>\\n', '      <td>115.0</td>\\n', '      <td>360.0</td>\\n', '      <td>1</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>True</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>1</th>\\n', '      <td>5703</td>\\n', '      <td>0.0</td>\\n', '      <td>130.0</td>\\n', '      <td>360.0</td>\\n', '      <td>1</td>\\n', '      <td>True</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>True</td>\\n', '      <td>True</td>\\n', '      <td>True</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>2</th>\\n', '      <td>4333</td>\\n', '      <td>2451.0</td>\\n', '      <td>110.0</td>\\n', '      <td>360.0</td>\\n', '      <td>0</td>\\n', '      <td>False</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '      <td>True</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>3</th>\\n', '      <td>4695</td>\\n', '      <td>0.0</td>\\n', '      <td>96.0</td>\\n', '      <td>360.0</td>\\n', '      <td>1</td>\\n', '      <td>True</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>True</td>\\n', '      <td>True</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '      <td>True</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>4</th>\\n', '      <td>6700</td>\\n', '      <td>1750.0</td>\\n', '      <td>230.0</td>\\n', '      <td>300.0</td>\\n', '      <td>1</td>\\n', '      <td>True</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>True</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '    </tr>\\n', '  </tbody>\\n', '</table>\\n', '</div>']\n",
      "----------------------------------------\n",
      "\n",
      "Cell #89:\n",
      "Code:\n",
      "# Testing set\n",
      "df_test = df_concat[df_concat['Loan_Status'].isin([999])].reset_index(drop = True)\n",
      "print('Data dimension: {} rows and {} columns'.format(len(df_test), len(df_test.columns)))\n",
      "df_test.head()\n",
      "\n",
      "Outputs:\n",
      "Data dimension: 106 rows and 15 columns\n",
      "\n",
      "Data dimension: 106 rows and 15 columns\n",
      "\n",
      "['   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\\\\n', '0             3748             1668.0       110.0             360.0   \\n', '1             4000             7750.0       290.0             360.0   \\n', '2             2625             6250.0       187.0             360.0   \\n', '3             3902             1666.0       109.0             360.0   \\n', '4             6096                0.0       218.0             360.0   \\n', '\\n', '   Loan_Status  Gender_Male  Married_Yes  Dependents_1  Dependents_2  \\\\\\n', '0          999         True        False         False         False   \\n', '1          999         True         True         False         False   \\n', '2          999         True         True         False         False   \\n', '3          999         True        False         False         False   \\n', '4          999         True         True         False         False   \\n', '\\n', '   Dependents_3+  Education_Not Graduate  Self_Employed_Yes  \\\\\\n', '0          False                    True              False   \\n', '1           True                   False              False   \\n', '2          False                   False              False   \\n', '3          False                    True              False   \\n', '4          False                    True              False   \\n', '\\n', '   Credit_History_1.0  Property_Area_Semiurban  Property_Area_Urban  \\n', '0                True                     True                False  \\n', '1                True                     True                False  \\n', '2                True                    False                False  \\n', '3                True                    False                False  \\n', '4               False                    False                False  ']\n",
      "['<div>\\n', '<style scoped>\\n', '    .dataframe tbody tr th:only-of-type {\\n', '        vertical-align: middle;\\n', '    }\\n', '\\n', '    .dataframe tbody tr th {\\n', '        vertical-align: top;\\n', '    }\\n', '\\n', '    .dataframe thead th {\\n', '        text-align: right;\\n', '    }\\n', '</style>\\n', '<table border=\"1\" class=\"dataframe\">\\n', '  <thead>\\n', '    <tr style=\"text-align: right;\">\\n', '      <th></th>\\n', '      <th>ApplicantIncome</th>\\n', '      <th>CoapplicantIncome</th>\\n', '      <th>LoanAmount</th>\\n', '      <th>Loan_Amount_Term</th>\\n', '      <th>Loan_Status</th>\\n', '      <th>Gender_Male</th>\\n', '      <th>Married_Yes</th>\\n', '      <th>Dependents_1</th>\\n', '      <th>Dependents_2</th>\\n', '      <th>Dependents_3+</th>\\n', '      <th>Education_Not Graduate</th>\\n', '      <th>Self_Employed_Yes</th>\\n', '      <th>Credit_History_1.0</th>\\n', '      <th>Property_Area_Semiurban</th>\\n', '      <th>Property_Area_Urban</th>\\n', '    </tr>\\n', '  </thead>\\n', '  <tbody>\\n', '    <tr>\\n', '      <th>0</th>\\n', '      <td>3748</td>\\n', '      <td>1668.0</td>\\n', '      <td>110.0</td>\\n', '      <td>360.0</td>\\n', '      <td>999</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '      <td>True</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>1</th>\\n', '      <td>4000</td>\\n', '      <td>7750.0</td>\\n', '      <td>290.0</td>\\n', '      <td>360.0</td>\\n', '      <td>999</td>\\n', '      <td>True</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>True</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>2</th>\\n', '      <td>2625</td>\\n', '      <td>6250.0</td>\\n', '      <td>187.0</td>\\n', '      <td>360.0</td>\\n', '      <td>999</td>\\n', '      <td>True</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>3</th>\\n', '      <td>3902</td>\\n', '      <td>1666.0</td>\\n', '      <td>109.0</td>\\n', '      <td>360.0</td>\\n', '      <td>999</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '    </tr>\\n', '    <tr>\\n', '      <th>4</th>\\n', '      <td>6096</td>\\n', '      <td>0.0</td>\\n', '      <td>218.0</td>\\n', '      <td>360.0</td>\\n', '      <td>999</td>\\n', '      <td>True</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>True</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '      <td>False</td>\\n', '    </tr>\\n', '  </tbody>\\n', '</table>\\n', '</div>']\n",
      "----------------------------------------\n",
      "\n",
      "Cell #90:\n",
      "Code:\n",
      "# Data partitioning >>> training set into training and validation\n",
      "df_train_final = df_train.reset_index(drop = True)\n",
      "X = df_train_final[df_train_final.columns[~df_train_final.columns.isin(['Loan_Status'])]]\n",
      "y = df_train_final['Loan_Status']\n",
      "\n",
      "# Training = 70% and validation = 30%\n",
      "X_train, X_test, y_train, y_test = train_test_split(X , y, test_size = 0.3, random_state = 42)\n",
      "print('Data dimension of training set   :', X_train.shape)\n",
      "print('Data dimension of validation set :', X_test.shape)\n",
      "\n",
      "# Testing set\n",
      "X_test = df_test[df_test.columns[~df_test.columns.isin(['Loan_Status'])]]\n",
      "print('Data dimension of testing set    :', X_test.shape)\n",
      "\n",
      "Outputs:\n",
      "Data dimension of training set   : (324, 14)\n",
      "Data dimension of validation set : (140, 14)\n",
      "Data dimension of testing set    : (106, 14)\n",
      "\n",
      "Data dimension of training set   : (324, 14)\n",
      "Data dimension of validation set : (140, 14)\n",
      "Data dimension of testing set    : (106, 14)\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "Cell #92:\n",
      "Code:\n",
      "testXGBoost model\n",
      "xgb_model = xgb.XGBClassifier(\n",
      "    objective = 'binary:logistic',\n",
      "    use_label_encoder = False\n",
      ")\n",
      "\n",
      "# Define parameter range \n",
      "params = {\n",
      "    'eta': np.arange(0.1, 0.26, 0.05),\n",
      "    'min_child_weight': np.arange(1, 5, 0.5).tolist(),\n",
      "    'gamma': [5],\n",
      "    'subsample': np.arange(0.5, 1.0, 0.11).tolist(),\n",
      "    'colsample_bytree': np.arange(0.5, 1.0, 0.11).tolist()\n",
      "}\n",
      "\n",
      "# Make a scorer from a performance metric or loss function\n",
      "scorers = {\n",
      "    'f1_score': make_scorer(f1_score),\n",
      "    'precision_score': make_scorer(precision_score),\n",
      "    'recall_score': make_scorer(recall_score),\n",
      "    'accuracy_score': make_scorer(accuracy_score)\n",
      "}\n",
      "\n",
      "# k-fold cross validation\n",
      "skf = KFold(n_splits = 10, shuffle = True)\n",
      "\n",
      "# Set up the grid search CV\n",
      "grid = GridSearchCV(\n",
      "    estimator = xgb_model,\n",
      "    param_grid = params,\n",
      "    scoring = scorers,\n",
      "    n_jobs = -1,\n",
      "    cv = skf.split(X_train, np.array(y_train)),\n",
      "    refit = 'accuracy_score'\n",
      ")\n",
      "\n",
      "# Fit the model\n",
      "grid.fit(X = X_train, y = y_train)\n",
      "\n",
      "# Best parameters\n",
      "grid.best_params_\n",
      "\n",
      "# Create a prediction of training \n",
      "predicted = grid.predict(X_test)\n",
      "\n",
      "# Model evaluation - training data\n",
      "accuracy_baseline = accuracy_score(predicted, np.array(y_test))\n",
      "recall_baseline = recall_score(predicted, np.array(y_test))\n",
      "precision_baseline = precision_score(predicted, np.array(y_test))\n",
      "f1_baseline = f1_score(predicted, np.array(y_test))\n",
      "\n",
      "print('Accuracy for baseline   :{}'.format(round(accuracy_baseline, 5)))\n",
      "print('Recall for baseline     :{}'.format(round(recall_baseline, 5)))\n",
      "print('Precision for baseline  :{}'.format(round(precision_baseline, 5)))\n",
      "print('F1 Score for baseline   :{}'.format(round(f1_baseline, 5)))\n",
      "\n",
      "\n",
      "Outputs:\n",
      "['\"  \\\\n# XGBoost model\\\\nxgb_model = xgb.XGBClassifier(\\\\n    objective = \\'binary:logistic\\',\\\\n    use_label_encoder = False\\\\n)\\\\n\\\\n# Define parameter range \\\\nparams = {\\\\n    \\'eta\\': np.arange(0.1, 0.26, 0.05),\\\\n    \\'min_child_weight\\': np.arange(1, 5, 0.5).tolist(),\\\\n    \\'gamma\\': [5],\\\\n    \\'subsample\\': np.arange(0.5, 1.0, 0.11).tolist(),\\\\n    \\'colsample_bytree\\': np.arange(0.5, 1.0, 0.11).tolist()\\\\n}\\\\n\\\\n# Make a scorer from a performance metric or loss function\\\\nscorers = {\\\\n    \\'f1_score\\': make_scorer(f1_score),\\\\n    \\'precision_score\\': make_scorer(precision_score),\\\\n    \\'recall_score\\': make_scorer(recall_score),\\\\n    \\'accuracy_score\\': make_scorer(accuracy_score)\\\\n}\\\\n\\\\n# k-fold cross validation\\\\nskf = KFold(n_splits = 10, shuffle = True)\\\\n\\\\n# Set up the grid search CV\\\\ngrid = GridSearchCV(\\\\n    estimator = xgb_model,\\\\n    param_grid = params,\\\\n    scoring = scorers,\\\\n    n_jobs = -1,\\\\n    cv = skf.split(X_train, np.array(y_train)),\\\\n    refit = \\'accuracy_score\\'\\\\n)\\\\n\\\\n# Fit the model\\\\ngrid.fit(X = X_train, y = y_train)\\\\n\\\\n# Best parameters\\\\ngrid.best_params_\\\\n\\\\n# Create a prediction of training \\\\npredicted = grid.predict(X_val)\\\\n\\\\n# Model evaluation - training data\\\\naccuracy_baseline = accuracy_score(predicted, np.array(y_val))\\\\nrecall_baseline = recall_score(predicted, np.array(y_val))\\\\nprecision_baseline = precision_score(predicted, np.array(y_val))\\\\nf1_baseline = f1_score(predicted, np.array(y_val))\\\\n\\\\nprint(\\'Accuracy for baseline   :{}\\'.format(round(accuracy_baseline, 5)))\\\\nprint(\\'Recall for baseline     :{}\\'.format(round(recall_baseline, 5)))\\\\nprint(\\'Precision for baseline  :{}\\'.format(round(precision_baseline, 5)))\\\\nprint(\\'F1 Score for baseline   :{}\\'.format(round(f1_baseline, 5)))\\\\n\\\\n\\\\n\"']\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, cell in enumerate(notebook_contents['cells']):\n",
    "    if cell['cell_type'] == 'code' and cell.get('outputs'):\n",
    "        print(f\"\\nCell #{i+1}:\")\n",
    "        print(\"Code:\")\n",
    "        print(\"\".join(cell['source']))\n",
    "        print(\"\\nOutputs:\")\n",
    "        for output in cell['outputs']:\n",
    "            # Print text output (if any)\n",
    "            if 'text' in output:\n",
    "                print(\"\".join(output['text']))\n",
    "            # Print stream output\n",
    "            if output.get('output_type') == 'stream':\n",
    "                print(\"\".join(output.get('text', '')))\n",
    "            # Print execution result (display_data or execute_result)\n",
    "            if output.get('output_type') in ['execute_result', 'display_data']:\n",
    "                data = output.get('data', {})\n",
    "                # Print text/plain or html if present\n",
    "                if 'text/plain' in data:\n",
    "                    print(data['text/plain'])\n",
    "                if 'text/html' in data:\n",
    "                    print(data['text/html'])\n",
    "            # Print errors if any\n",
    "            if output.get('output_type') == 'error':\n",
    "                print(f\"Error: {output.get('ename')} - {output.get('evalue')}\")\n",
    "                print(\"Traceback:\")\n",
    "                print(\"\\n\".join(output.get('traceback', [])))\n",
    "        print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0ef8bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell #11\n",
      "Code:\n",
      "# Data dimension\n",
      "print('Data dimension: {} rows and {} columns'.format(len(df_train), len(df_train.columns)))\n",
      "df_train.head()\n",
      "Output(s):\n",
      "Data dimension: 491 rows and 13 columns\n",
      "Loan_ID  Gender Married Dependents     Education Self_Employed  \\\n",
      "0  LP002305  Female      No          0      Graduate            No   \n",
      "1  LP001715    Male     Yes         3+  Not Graduate           Yes   \n",
      "2  LP002086  Female     Yes          0      Graduate            No   \n",
      "3  LP001136    Male     Yes          0  Not Graduate           Yes   \n",
      "4  LP002529    Male     Yes          2      Graduate            No   \n",
      "\n",
      "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
      "0             4547                0.0       115.0             360.0   \n",
      "1             5703                0.0       130.0             360.0   \n",
      "2             4333             2451.0       110.0             360.0   \n",
      "3             4695                0.0        96.0               NaN   \n",
      "4             6700             1750.0       230.0             300.0   \n",
      "\n",
      "   Credit_History Property_Area  Loan_Status  \n",
      "0             1.0     Semiurban            1  \n",
      "1             1.0         Rural            1  \n",
      "2             1.0         Urban            0  \n",
      "3             1.0         Urban            1  \n",
      "4             1.0     Semiurban            1\n",
      "------------------------------\n",
      "\n",
      "Cell #14\n",
      "Code:\n",
      "# Data dimension\n",
      "print('Data dimension: {} rows and {} columns'.format(len(df_test), len(df_test.columns)))\n",
      "df_test.head()\n",
      "Output(s):\n",
      "Data dimension: 123 rows and 12 columns\n",
      "Loan_ID Gender Married Dependents     Education Self_Employed  \\\n",
      "0  LP001116   Male      No          0  Not Graduate            No   \n",
      "1  LP001488   Male     Yes         3+      Graduate            No   \n",
      "2  LP002138   Male     Yes          0      Graduate            No   \n",
      "3  LP002284   Male      No          0  Not Graduate            No   \n",
      "4  LP002328   Male     Yes          0  Not Graduate            No   \n",
      "\n",
      "   ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
      "0             3748             1668.0       110.0             360.0   \n",
      "1             4000             7750.0       290.0             360.0   \n",
      "2             2625             6250.0       187.0             360.0   \n",
      "3             3902             1666.0       109.0             360.0   \n",
      "4             6096                0.0       218.0             360.0   \n",
      "\n",
      "   Credit_History Property_Area  \n",
      "0             1.0     Semiurban  \n",
      "1             1.0     Semiurban  \n",
      "2             1.0         Rural  \n",
      "3             1.0         Rural  \n",
      "4             0.0         Rural\n",
      "------------------------------\n",
      "\n",
      "Cell #19\n",
      "Code:\n",
      "# Data frame metadata\n",
      "df_train.info()\n",
      "Output(s):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 491 entries, 0 to 490\n",
      "Data columns (total 13 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Loan_ID            491 non-null    object \n",
      " 1   Gender             481 non-null    object \n",
      " 2   Married            490 non-null    object \n",
      " 3   Dependents         482 non-null    object \n",
      " 4   Education          491 non-null    object \n",
      " 5   Self_Employed      462 non-null    object \n",
      " 6   ApplicantIncome    491 non-null    int64  \n",
      " 7   CoapplicantIncome  491 non-null    float64\n",
      " 8   LoanAmount         475 non-null    float64\n",
      " 9   Loan_Amount_Term   478 non-null    float64\n",
      " 10  Credit_History     448 non-null    float64\n",
      " 11  Property_Area      491 non-null    object \n",
      " 12  Loan_Status        491 non-null    int64  \n",
      "dtypes: float64(4), int64(2), object(7)\n",
      "memory usage: 50.0+ KB\n",
      "------------------------------\n",
      "\n",
      "Cell #20\n",
      "Code:\n",
      "# Change column types\n",
      "df_train = df_train.astype({'Credit_History': object, 'Loan_Status': int})\n",
      "df_train.select_dtypes(include = ['object']).dtypes\n",
      "Output(s):\n",
      "Loan_ID           object\n",
      "Gender            object\n",
      "Married           object\n",
      "Dependents        object\n",
      "Education         object\n",
      "Self_Employed     object\n",
      "Credit_History    object\n",
      "Property_Area     object\n",
      "dtype: object\n",
      "------------------------------\n",
      "\n",
      "Cell #21\n",
      "Code:\n",
      "# Summary statistics of categorical columns\n",
      "for i in df_train.select_dtypes('object').columns:\n",
      "    print(df_train[i].value_counts(),'\\n')\n",
      "Output(s):\n",
      "Loan_ID\n",
      "LP002777    1\n",
      "LP002305    1\n",
      "LP001715    1\n",
      "LP002086    1\n",
      "LP001136    1\n",
      "           ..\n",
      "LP002379    1\n",
      "LP001011    1\n",
      "LP001508    1\n",
      "LP001112    1\n",
      "LP002130    1\n",
      "Name: count, Length: 491, dtype: int64 \n",
      "\n",
      "Gender\n",
      "Male      393\n",
      "Female     88\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Married\n",
      "Yes    324\n",
      "No     166\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Dependents\n",
      "0     276\n",
      "1      85\n",
      "2      78\n",
      "3+     43\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Education\n",
      "Graduate        388\n",
      "Not Graduate    103\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Self_Employed\n",
      "No     398\n",
      "Yes     64\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Credit_History\n",
      "1.0    380\n",
      "0.0     68\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Property_Area\n",
      "Semiurban    186\n",
      "Urban        155\n",
      "Rural        150\n",
      "Name: count, dtype: int64\n",
      "------------------------------\n",
      "\n",
      "Cell #23\n",
      "Code:\n",
      "# Check missing values\n",
      "df_train.isna().sum()\n",
      "Output(s):\n",
      "Loan_ID               0\n",
      "Gender               10\n",
      "Married               1\n",
      "Dependents            9\n",
      "Education             0\n",
      "Self_Employed        29\n",
      "ApplicantIncome       0\n",
      "CoapplicantIncome     0\n",
      "LoanAmount           16\n",
      "Loan_Amount_Term     13\n",
      "Credit_History       43\n",
      "Property_Area         0\n",
      "Loan_Status           0\n",
      "dtype: int64\n",
      "------------------------------\n",
      "\n",
      "Cell #26\n",
      "Code:\n",
      "print('Number of missing dependents is about {} rows'.format(df_train['Dependents'].isna().sum()))\n",
      "Output(s):\n",
      "Number of missing dependents is about 9 rows\n",
      "------------------------------\n",
      "\n",
      "Cell #29\n",
      "Code:\n",
      "print('Number of missing Self_Employed is about {} rows'.format(df_train['Self_Employed'].isna().sum()))\n",
      "Output(s):\n",
      "Number of missing Self_Employed is about 29 rows\n",
      "------------------------------\n",
      "\n",
      "Cell #32\n",
      "Code:\n",
      "df_train[['Loan_Amount_Term', 'Loan_Status']].groupby('Loan_Status').describe()\n",
      "Output(s):\n",
      "Loan_Amount_Term                                             \\\n",
      "                       count        mean        std   min    25%    50%   \n",
      "Loan_Status                                                               \n",
      "0                      143.0  341.790210  73.018891  36.0  360.0  360.0   \n",
      "1                      335.0  341.086567  64.320411  12.0  360.0  360.0   \n",
      "\n",
      "                           \n",
      "               75%    max  \n",
      "Loan_Status                \n",
      "0            360.0  480.0  \n",
      "1            360.0  480.0\n",
      "------------------------------\n",
      "\n",
      "Cell #33\n",
      "Code:\n",
      "print('Percentile 20th: {}'.format(df_train['Loan_Amount_Term'].quantile(q = 0.2)))\n",
      "Output(s):\n",
      "Percentile 20th: 360.0\n",
      "------------------------------\n",
      "\n",
      "Cell #36\n",
      "Code:\n",
      "# Cross tabulation of credit history and loan status\n",
      "df_cred_hist = pd.crosstab(df_train['Credit_History'], df_train['Loan_Status'], margins = True).reset_index()\n",
      "# Remove index name\n",
      "df_cred_hist.columns.name = None\n",
      "# Remove last row for total column attribute\n",
      "df_cred_hist = df_cred_hist.drop([len(df_cred_hist) - 1], axis = 0)\n",
      "df_cred_hist.rename(columns = {'Credit_History':'Credit History', 0:'No', 1:'Yes'}, inplace = True)\n",
      "df_cred_hist\n",
      "Output(s):\n",
      "Credit History  No  Yes  All\n",
      "0            0.0  62    6   68\n",
      "1            1.0  74  306  380\n",
      "------------------------------\n",
      "\n",
      "Cell #37\n",
      "Code:\n",
      "# Slice the data frame based on loan status\n",
      "pos_cred_hist0 = df_train[(df_train['Credit_History'].isna()) & (df_train['Loan_Status'] == 0)]\n",
      "pos_cred_hist1 = df_train[(df_train['Credit_History'].isna()) & (df_train['Loan_Status'] == 1)]\n",
      "print('Number of rows with Loan_Status is No but Credit_History is NaN  : {}'.format(len(pos_cred_hist0)))\n",
      "print('Number of rows with Loan_Status is Yes but Credit_History is NaN : {}'.format(len(pos_cred_hist1)))\n",
      "Output(s):\n",
      "Number of rows with Loan_Status is No but Credit_History is NaN  : 12\n",
      "Number of rows with Loan_Status is Yes but Credit_History is NaN : 31\n",
      "------------------------------\n",
      "\n",
      "Cell #41\n",
      "Code:\n",
      "# Check missing value\n",
      "df_train.isna().sum()\n",
      "Output(s):\n",
      "Loan_ID              0\n",
      "Gender               0\n",
      "Married              0\n",
      "Dependents           0\n",
      "Education            0\n",
      "Self_Employed        0\n",
      "ApplicantIncome      0\n",
      "CoapplicantIncome    0\n",
      "LoanAmount           0\n",
      "Loan_Amount_Term     0\n",
      "Credit_History       0\n",
      "Property_Area        0\n",
      "Loan_Status          0\n",
      "dtype: int64\n",
      "------------------------------\n",
      "\n",
      "Cell #44\n",
      "Code:\n",
      "# Data frame metadata\n",
      "df_test.info()\n",
      "Output(s):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 123 entries, 0 to 122\n",
      "Data columns (total 12 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Loan_ID            123 non-null    object \n",
      " 1   Gender             120 non-null    object \n",
      " 2   Married            121 non-null    object \n",
      " 3   Dependents         117 non-null    object \n",
      " 4   Education          123 non-null    object \n",
      " 5   Self_Employed      120 non-null    object \n",
      " 6   ApplicantIncome    123 non-null    int64  \n",
      " 7   CoapplicantIncome  123 non-null    float64\n",
      " 8   LoanAmount         117 non-null    float64\n",
      " 9   Loan_Amount_Term   122 non-null    float64\n",
      " 10  Credit_History     116 non-null    float64\n",
      " 11  Property_Area      123 non-null    object \n",
      "dtypes: float64(4), int64(1), object(7)\n",
      "memory usage: 11.7+ KB\n",
      "------------------------------\n",
      "\n",
      "Cell #45\n",
      "Code:\n",
      "# Change column types\n",
      "df_test = df_test.astype({'Credit_History': object})\n",
      "df_test.select_dtypes(include = ['object']).dtypes\n",
      "Output(s):\n",
      "Loan_ID           object\n",
      "Gender            object\n",
      "Married           object\n",
      "Dependents        object\n",
      "Education         object\n",
      "Self_Employed     object\n",
      "Credit_History    object\n",
      "Property_Area     object\n",
      "dtype: object\n",
      "------------------------------\n",
      "\n",
      "Cell #46\n",
      "Code:\n",
      "# Summary statistics of categorical columns\n",
      "for i in df_test.select_dtypes('object').columns:\n",
      "    print(df_test[i].value_counts(),'\\n')\n",
      "Output(s):\n",
      "Loan_ID\n",
      "LP001116    1\n",
      "LP001488    1\n",
      "LP002138    1\n",
      "LP002284    1\n",
      "LP002328    1\n",
      "           ..\n",
      "LP002683    1\n",
      "LP002054    1\n",
      "LP002757    1\n",
      "LP002582    1\n",
      "LP001616    1\n",
      "Name: count, Length: 123, dtype: int64 \n",
      "\n",
      "Gender\n",
      "Male      96\n",
      "Female    24\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Married\n",
      "Yes    74\n",
      "No     47\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Dependents\n",
      "0     69\n",
      "2     23\n",
      "1     17\n",
      "3+     8\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Education\n",
      "Graduate        92\n",
      "Not Graduate    31\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Self_Employed\n",
      "No     102\n",
      "Yes     18\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Credit_History\n",
      "1.0    95\n",
      "0.0    21\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Property_Area\n",
      "Semiurban    47\n",
      "Urban        47\n",
      "Rural        29\n",
      "Name: count, dtype: int64\n",
      "------------------------------\n",
      "\n",
      "Cell #48\n",
      "Code:\n",
      "# Check missing values\n",
      "df_test.isna().sum()\n",
      "Output(s):\n",
      "Loan_ID              0\n",
      "Gender               3\n",
      "Married              2\n",
      "Dependents           6\n",
      "Education            0\n",
      "Self_Employed        3\n",
      "ApplicantIncome      0\n",
      "CoapplicantIncome    0\n",
      "LoanAmount           6\n",
      "Loan_Amount_Term     1\n",
      "Credit_History       7\n",
      "Property_Area        0\n",
      "dtype: int64\n",
      "------------------------------\n",
      "\n",
      "Cell #50\n",
      "Code:\n",
      "print('Number of missing values in Dependents is about {} rows'.format(df_test['Dependents'].isna().sum()))\n",
      "Output(s):\n",
      "Number of missing values in Dependents is about 6 rows\n",
      "------------------------------\n",
      "\n",
      "Cell #53\n",
      "Code:\n",
      "print('Number of missing values in Self_Employed is about {} rows'.format(df_test['Self_Employed'].isna().sum()))\n",
      "Output(s):\n",
      "Number of missing values in Self_Employed is about 3 rows\n",
      "------------------------------\n",
      "\n",
      "Cell #59\n",
      "Code:\n",
      "# Check missing values\n",
      "df_test.isna().sum()\n",
      "Output(s):\n",
      "Loan_ID              0\n",
      "Gender               0\n",
      "Married              0\n",
      "Dependents           0\n",
      "Education            0\n",
      "Self_Employed        0\n",
      "ApplicantIncome      0\n",
      "CoapplicantIncome    0\n",
      "LoanAmount           0\n",
      "Loan_Amount_Term     0\n",
      "Credit_History       0\n",
      "Property_Area        0\n",
      "dtype: int64\n",
      "------------------------------\n",
      "\n",
      "Cell #63\n",
      "Code:\n",
      "# Show the data\n",
      "df_viz_1\n",
      "Output(s):\n",
      "Loan_Status  Total\n",
      "0  Not default    134\n",
      "1      Default    330\n",
      "------------------------------\n",
      "\n",
      "Cell #64\n",
      "Code:\n",
      "# Figure size\n",
      "plt.figure(figsize = (6.4,4.8))\n",
      "\n",
      "# Customize colors and other settings\n",
      "colors = ['#80797c','#981220']\n",
      "\n",
      "# Explode 1st slice\n",
      "explode = (0.1, 0)\n",
      "\n",
      "# Create a pie chart\n",
      "plt.pie(\n",
      "    x = 'Total',\n",
      "    labels = 'Loan_Status',\n",
      "    data = df_viz_1,\n",
      "    explode = explode,\n",
      "    colors = colors,\n",
      "    autopct = '%1.1f%%',\n",
      "    shadow = False,\n",
      "    startangle = 140\n",
      ")\n",
      "\n",
      "# Title and axis\n",
      "plt.title('Number of customers by loan status', fontsize = 18)\n",
      "plt.axis('equal')\n",
      "plt.show()\n",
      "Output(s):\n",
      "<Figure size 640x480 with 1 Axes>\n",
      "------------------------------\n",
      "\n",
      "Cell #67\n",
      "Code:\n",
      "# Show the data\n",
      "df_viz_2\n",
      "Output(s):\n",
      "Loan_Status Dependents  Total\n",
      "0  Not default          0     77\n",
      "1  Not default          1     30\n",
      "2  Not default          2     13\n",
      "3  Not default         3+     14\n",
      "4      Default          0    191\n",
      "5      Default          1     52\n",
      "6      Default          2     62\n",
      "7      Default         3+     25\n",
      "------------------------------\n",
      "\n",
      "Cell #68\n",
      "Code:\n",
      "plotnine.options.figure_size = (8, 4.8)\n",
      "(\n",
      "    ggplot(\n",
      "        data = df_viz_2\n",
      "    )+\n",
      "    geom_bar(\n",
      "        aes(\n",
      "            x = 'Dependents',\n",
      "            y = 'Total',\n",
      "            fill = 'Loan_Status'\n",
      "        ),\n",
      "        stat = 'identity',\n",
      "        position = 'fill',\n",
      "        width = 0.5\n",
      "    )+\n",
      "    labs(\n",
      "        title = 'The composition of loan status by the dependents',\n",
      "        fill = 'Loan status'\n",
      "    )+\n",
      "    xlab(\n",
      "        'Dependents'\n",
      "    )+\n",
      "    ylab(\n",
      "        'Frequency'\n",
      "    )+\n",
      "    scale_x_discrete(\n",
      "        limits = ['0', '1', '2', '3+']\n",
      "    )+\n",
      "    scale_fill_manual(\n",
      "        values = ['#981220','#80797c'],\n",
      "        labels = ['Default', 'Not Default']\n",
      "    )+\n",
      "    theme_minimal()\n",
      ")\n",
      "Output(s):\n",
      "------------------------------\n",
      "\n",
      "Cell #71\n",
      "Code:\n",
      "# Show the data\n",
      "df_viz_3\n",
      "Output(s):\n",
      "Loan_Status     Education  Total\n",
      "0  Not default      Graduate    101\n",
      "1  Not default  Not Graduate     33\n",
      "2      Default      Graduate    266\n",
      "3      Default  Not Graduate     64\n",
      "------------------------------\n",
      "\n",
      "Cell #72\n",
      "Code:\n",
      "plotnine.options.figure_size = (8, 4.8)\n",
      "(\n",
      "    ggplot(\n",
      "        data = df_viz_3\n",
      "    )+\n",
      "    geom_bar(\n",
      "        aes(\n",
      "            x = 'Education',\n",
      "            y = 'Total',\n",
      "            fill = 'Loan_Status'\n",
      "        ),\n",
      "        stat = 'identity',\n",
      "        position = 'fill',\n",
      "        width = 0.5\n",
      "    )+\n",
      "    labs(\n",
      "        title = 'The composition of loan status by the education',\n",
      "        fill = 'Loan status'\n",
      "    )+\n",
      "    xlab(\n",
      "        'Educations'\n",
      "    )+\n",
      "    ylab(\n",
      "        'Frequency'\n",
      "    )+\n",
      "    scale_x_discrete(\n",
      "        limits = ['Graduate', 'Not Graduate']\n",
      "    )+\n",
      "    scale_fill_manual(\n",
      "        values = ['#981220','#80797c'],\n",
      "        labels = ['Default', 'Not Default']\n",
      "    )+\n",
      "    theme_minimal()\n",
      ")\n",
      "Output(s):\n",
      "------------------------------\n",
      "\n",
      "Cell #75\n",
      "Code:\n",
      "# Show the data\n",
      "df_viz_4.head()\n",
      "Output(s):\n",
      "ApplicantIncome  Loan_Status\n",
      "0             4547      Default\n",
      "1             5703      Default\n",
      "2             4333  Not default\n",
      "3             4695      Default\n",
      "4             6700      Default\n",
      "------------------------------\n",
      "\n",
      "Cell #76\n",
      "Code:\n",
      "plotnine.options.figure_size = (8, 4.8)\n",
      "(\n",
      "    ggplot(\n",
      "        data = df_viz_4\n",
      "    )+\n",
      "    geom_density(\n",
      "        aes(\n",
      "            x = 'ApplicantIncome',\n",
      "            fill = 'Loan_Status'\n",
      "        ),\n",
      "        color = 'white',\n",
      "        alpha = 0.85\n",
      "    )+\n",
      "    labs(\n",
      "        title = 'The distribution of applicant incomes by loan status'\n",
      "    )+\n",
      "    scale_fill_manual(\n",
      "        name = 'Loan Status',\n",
      "        values = ['#981220','#80797c'],\n",
      "        labels = ['Default', 'Not Default']\n",
      "    )+\n",
      "    xlab(\n",
      "        'Applicant income'\n",
      "    )+\n",
      "    ylab(\n",
      "        'Density'\n",
      "    )+\n",
      "    theme_minimal()\n",
      ")\n",
      "Output(s):\n",
      "------------------------------\n",
      "\n",
      "Cell #79\n",
      "Code:\n",
      "# Show the data\n",
      "df_viz_5.head()\n",
      "Output(s):\n",
      "LoanAmount  Loan_Status\n",
      "0       115.0      Default\n",
      "1       130.0      Default\n",
      "2       110.0  Not default\n",
      "3        96.0      Default\n",
      "4       230.0      Default\n",
      "------------------------------\n",
      "\n",
      "Cell #80\n",
      "Code:\n",
      "plotnine.options.figure_size = (8, 4.8)\n",
      "(\n",
      "    ggplot(\n",
      "        data = df_viz_5\n",
      "    )+\n",
      "    geom_density(\n",
      "        aes(\n",
      "            x = 'LoanAmount',\n",
      "            fill = 'Loan_Status'\n",
      "        ),\n",
      "        color = 'white',\n",
      "        alpha = 0.85\n",
      "    )+\n",
      "    labs(\n",
      "        title = 'The distribution of loan amount by loan status'\n",
      "    )+\n",
      "    scale_fill_manual(\n",
      "        name = 'Loan Status',\n",
      "        values = ['#981220','#80797c'],\n",
      "        labels = ['Default', 'Not Default']\n",
      "    )+\n",
      "    xlab(\n",
      "        'Loan amount'\n",
      "    )+\n",
      "    ylab(\n",
      "        'Density'\n",
      "    )+\n",
      "    theme_minimal()\n",
      ")\n",
      "Output(s):\n",
      "------------------------------\n",
      "\n",
      "Cell #84\n",
      "Code:\n",
      "# Categorical columns\n",
      "cols_obj_train = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Credit_History', 'Property_Area']\n",
      "print(cols_obj_train)\n",
      "Output(s):\n",
      "['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Credit_History', 'Property_Area']\n",
      "------------------------------\n",
      "\n",
      "Cell #85\n",
      "Code:\n",
      "# One-hot encoding\n",
      "df_concat = pd.get_dummies(data = df_concat, columns = cols_obj_train, drop_first = True)\n",
      "print('Dimension data: {} rows and {} columns'.format(len(df_concat), len(df_concat.columns)))\n",
      "df_concat.head()\n",
      "Output(s):\n",
      "Dimension data: 570 rows and 15 columns\n",
      "ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
      "0             4547                0.0       115.0             360.0   \n",
      "1             5703                0.0       130.0             360.0   \n",
      "2             4333             2451.0       110.0             360.0   \n",
      "3             4695                0.0        96.0             360.0   \n",
      "4             6700             1750.0       230.0             300.0   \n",
      "\n",
      "   Loan_Status  Gender_Male  Married_Yes  Dependents_1  Dependents_2  \\\n",
      "0            1        False        False         False         False   \n",
      "1            1         True         True         False         False   \n",
      "2            0        False         True         False         False   \n",
      "3            1         True         True         False         False   \n",
      "4            1         True         True         False          True   \n",
      "\n",
      "   Dependents_3+  Education_Not Graduate  Self_Employed_Yes  \\\n",
      "0          False                   False              False   \n",
      "1           True                    True               True   \n",
      "2          False                   False              False   \n",
      "3          False                    True               True   \n",
      "4          False                   False              False   \n",
      "\n",
      "   Credit_History_1.0  Property_Area_Semiurban  Property_Area_Urban  \n",
      "0                True                     True                False  \n",
      "1                True                    False                False  \n",
      "2                True                    False                 True  \n",
      "3                True                    False                 True  \n",
      "4                True                     True                False\n",
      "------------------------------\n",
      "\n",
      "Cell #87\n",
      "Code:\n",
      "# Unique values of Loan_Status\n",
      "df_concat['Loan_Status'].value_counts()\n",
      "Output(s):\n",
      "Loan_Status\n",
      "1      330\n",
      "0      134\n",
      "999    106\n",
      "Name: count, dtype: int64\n",
      "------------------------------\n",
      "\n",
      "Cell #88\n",
      "Code:\n",
      "# Training set\n",
      "df_train = df_concat[df_concat['Loan_Status'].isin([0, 1])].reset_index(drop = True)\n",
      "print('Dimension data: {} rows and {} columns'.format(len(df_train), len(df_train.columns)))\n",
      "df_train.head()\n",
      "Output(s):\n",
      "Dimension data: 464 rows and 15 columns\n",
      "ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
      "0             4547                0.0       115.0             360.0   \n",
      "1             5703                0.0       130.0             360.0   \n",
      "2             4333             2451.0       110.0             360.0   \n",
      "3             4695                0.0        96.0             360.0   \n",
      "4             6700             1750.0       230.0             300.0   \n",
      "\n",
      "   Loan_Status  Gender_Male  Married_Yes  Dependents_1  Dependents_2  \\\n",
      "0            1        False        False         False         False   \n",
      "1            1         True         True         False         False   \n",
      "2            0        False         True         False         False   \n",
      "3            1         True         True         False         False   \n",
      "4            1         True         True         False          True   \n",
      "\n",
      "   Dependents_3+  Education_Not Graduate  Self_Employed_Yes  \\\n",
      "0          False                   False              False   \n",
      "1           True                    True               True   \n",
      "2          False                   False              False   \n",
      "3          False                    True               True   \n",
      "4          False                   False              False   \n",
      "\n",
      "   Credit_History_1.0  Property_Area_Semiurban  Property_Area_Urban  \n",
      "0                True                     True                False  \n",
      "1                True                    False                False  \n",
      "2                True                    False                 True  \n",
      "3                True                    False                 True  \n",
      "4                True                     True                False\n",
      "------------------------------\n",
      "\n",
      "Cell #89\n",
      "Code:\n",
      "# Testing set\n",
      "df_test = df_concat[df_concat['Loan_Status'].isin([999])].reset_index(drop = True)\n",
      "print('Data dimension: {} rows and {} columns'.format(len(df_test), len(df_test.columns)))\n",
      "df_test.head()\n",
      "Output(s):\n",
      "Data dimension: 106 rows and 15 columns\n",
      "ApplicantIncome  CoapplicantIncome  LoanAmount  Loan_Amount_Term  \\\n",
      "0             3748             1668.0       110.0             360.0   \n",
      "1             4000             7750.0       290.0             360.0   \n",
      "2             2625             6250.0       187.0             360.0   \n",
      "3             3902             1666.0       109.0             360.0   \n",
      "4             6096                0.0       218.0             360.0   \n",
      "\n",
      "   Loan_Status  Gender_Male  Married_Yes  Dependents_1  Dependents_2  \\\n",
      "0          999         True        False         False         False   \n",
      "1          999         True         True         False         False   \n",
      "2          999         True         True         False         False   \n",
      "3          999         True        False         False         False   \n",
      "4          999         True         True         False         False   \n",
      "\n",
      "   Dependents_3+  Education_Not Graduate  Self_Employed_Yes  \\\n",
      "0          False                    True              False   \n",
      "1           True                   False              False   \n",
      "2          False                   False              False   \n",
      "3          False                    True              False   \n",
      "4          False                    True              False   \n",
      "\n",
      "   Credit_History_1.0  Property_Area_Semiurban  Property_Area_Urban  \n",
      "0                True                     True                False  \n",
      "1                True                     True                False  \n",
      "2                True                    False                False  \n",
      "3                True                    False                False  \n",
      "4               False                    False                False\n",
      "------------------------------\n",
      "\n",
      "Cell #90\n",
      "Code:\n",
      "# Data partitioning >>> training set into training and validation\n",
      "df_train_final = df_train.reset_index(drop = True)\n",
      "X = df_train_final[df_train_final.columns[~df_train_final.columns.isin(['Loan_Status'])]]\n",
      "y = df_train_final['Loan_Status']\n",
      "\n",
      "# Training = 70% and validation = 30%\n",
      "X_train, X_test, y_train, y_test = train_test_split(X , y, test_size = 0.3, random_state = 42)\n",
      "print('Data dimension of training set   :', X_train.shape)\n",
      "print('Data dimension of validation set :', X_test.shape)\n",
      "\n",
      "# Testing set\n",
      "X_test = df_test[df_test.columns[~df_test.columns.isin(['Loan_Status'])]]\n",
      "print('Data dimension of testing set    :', X_test.shape)\n",
      "Output(s):\n",
      "Data dimension of training set   : (324, 14)\n",
      "Data dimension of validation set : (140, 14)\n",
      "Data dimension of testing set    : (106, 14)\n",
      "------------------------------\n",
      "\n",
      "Cell #92\n",
      "Code:\n",
      "testXGBoost model\n",
      "xgb_model = xgb.XGBClassifier(\n",
      "    objective = 'binary:logistic',\n",
      "    use_label_encoder = False\n",
      ")\n",
      "\n",
      "# Define parameter range \n",
      "params = {\n",
      "    'eta': np.arange(0.1, 0.26, 0.05),\n",
      "    'min_child_weight': np.arange(1, 5, 0.5).tolist(),\n",
      "    'gamma': [5],\n",
      "    'subsample': np.arange(0.5, 1.0, 0.11).tolist(),\n",
      "    'colsample_bytree': np.arange(0.5, 1.0, 0.11).tolist()\n",
      "}\n",
      "\n",
      "# Make a scorer from a performance metric or loss function\n",
      "scorers = {\n",
      "    'f1_score': make_scorer(f1_score),\n",
      "    'precision_score': make_scorer(precision_score),\n",
      "    'recall_score': make_scorer(recall_score),\n",
      "    'accuracy_score': make_scorer(accuracy_score)\n",
      "}\n",
      "\n",
      "# k-fold cross validation\n",
      "skf = KFold(n_splits = 10, shuffle = True)\n",
      "\n",
      "# Set up the grid search CV\n",
      "grid = GridSearchCV(\n",
      "    estimator = xgb_model,\n",
      "    param_grid = params,\n",
      "    scoring = scorers,\n",
      "    n_jobs = -1,\n",
      "    cv = skf.split(X_train, np.array(y_train)),\n",
      "    refit = 'accuracy_score'\n",
      ")\n",
      "\n",
      "# Fit the model\n",
      "grid.fit(X = X_train, y = y_train)\n",
      "\n",
      "# Best parameters\n",
      "grid.best_params_\n",
      "\n",
      "# Create a prediction of training \n",
      "predicted = grid.predict(X_test)\n",
      "\n",
      "# Model evaluation - training data\n",
      "accuracy_baseline = accuracy_score(predicted, np.array(y_test))\n",
      "recall_baseline = recall_score(predicted, np.array(y_test))\n",
      "precision_baseline = precision_score(predicted, np.array(y_test))\n",
      "f1_baseline = f1_score(predicted, np.array(y_test))\n",
      "\n",
      "print('Accuracy for baseline   :{}'.format(round(accuracy_baseline, 5)))\n",
      "print('Recall for baseline     :{}'.format(round(recall_baseline, 5)))\n",
      "print('Precision for baseline  :{}'.format(round(precision_baseline, 5)))\n",
      "print('F1 Score for baseline   :{}'.format(round(f1_baseline, 5)))\n",
      "Output(s):\n",
      "\"  \\n# XGBoost model\\nxgb_model = xgb.XGBClassifier(\\n    objective = 'binary:logistic',\\n    use_label_encoder = False\\n)\\n\\n# Define parameter range \\nparams = {\\n    'eta': np.arange(0.1, 0.26, 0.05),\\n    'min_child_weight': np.arange(1, 5, 0.5).tolist(),\\n    'gamma': [5],\\n    'subsample': np.arange(0.5, 1.0, 0.11).tolist(),\\n    'colsample_bytree': np.arange(0.5, 1.0, 0.11).tolist()\\n}\\n\\n# Make a scorer from a performance metric or loss function\\nscorers = {\\n    'f1_score': make_scorer(f1_score),\\n    'precision_score': make_scorer(precision_score),\\n    'recall_score': make_scorer(recall_score),\\n    'accuracy_score': make_scorer(accuracy_score)\\n}\\n\\n# k-fold cross validation\\nskf = KFold(n_splits = 10, shuffle = True)\\n\\n# Set up the grid search CV\\ngrid = GridSearchCV(\\n    estimator = xgb_model,\\n    param_grid = params,\\n    scoring = scorers,\\n    n_jobs = -1,\\n    cv = skf.split(X_train, np.array(y_train)),\\n    refit = 'accuracy_score'\\n)\\n\\n# Fit the model\\ngrid.fit(X = X_train, y = y_train)\\n\\n# Best parameters\\ngrid.best_params_\\n\\n# Create a prediction of training \\npredicted = grid.predict(X_val)\\n\\n# Model evaluation - training data\\naccuracy_baseline = accuracy_score(predicted, np.array(y_val))\\nrecall_baseline = recall_score(predicted, np.array(y_val))\\nprecision_baseline = precision_score(predicted, np.array(y_val))\\nf1_baseline = f1_score(predicted, np.array(y_val))\\n\\nprint('Accuracy for baseline   :{}'.format(round(accuracy_baseline, 5)))\\nprint('Recall for baseline     :{}'.format(round(recall_baseline, 5)))\\nprint('Precision for baseline  :{}'.format(round(precision_baseline, 5)))\\nprint('F1 Score for baseline   :{}'.format(round(f1_baseline, 5)))\\n\\n\\n\"\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "all_cells_text = \"\"\n",
    "\n",
    "for i, cell in enumerate(notebook_contents['cells']):\n",
    "    if cell['cell_type'] == 'code' and cell.get('outputs'):\n",
    "        # Add cell number and code\n",
    "        all_cells_text += f\"\\nCell #{i+1}\\n\"\n",
    "        all_cells_text += \"Code:\\n\"\n",
    "        all_cells_text += \"\".join(cell['source']).strip() + \"\\n\"\n",
    "        all_cells_text += \"Output(s):\\n\"\n",
    "        # Add outputs\n",
    "        for output in cell['outputs']:\n",
    "            output_text = \"\"\n",
    "            if output.get('output_type') == 'stream':\n",
    "                text = output.get('text', '')\n",
    "                if isinstance(text, list):\n",
    "                    text = \"\".join(text)\n",
    "                output_text += text.strip()\n",
    "            elif output.get('output_type') in ['execute_result', 'display_data']:\n",
    "                data = output.get('data', {})\n",
    "                text = data.get('text/plain', '')\n",
    "                if isinstance(text, list):\n",
    "                    text = \"\".join(text)\n",
    "                output_text += text.strip()\n",
    "            # Skipping errors\n",
    "            if output_text:\n",
    "                all_cells_text += output_text + \"\\n\"\n",
    "        all_cells_text += \"-\" * 30 + \"\\n\"\n",
    "\n",
    "# Optional: remove leading/trailing whitespace\n",
    "all_cells_text = all_cells_text.strip()\n",
    "\n",
    "# Print or use as needed\n",
    "print(all_cells_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
