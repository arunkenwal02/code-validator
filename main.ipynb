{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ac416dc",
   "metadata": {},
   "source": [
    "## Check Docs Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e18532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from IPython.display import Markdown, display\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "import nbformat\n",
    "import streamlit as st\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913e4307",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1. Load White Paper  (PDF)  \n",
    "2. Vectors Embeddings - Text and tables; later include images\n",
    "3. Chroma db\n",
    "4. Retrival - (Accuracy)\n",
    "5. Generation - \n",
    "    Validation-\n",
    "        Data sources - List all sources and metadata\n",
    "        Features - detect any change in features\n",
    "        Changes in Transformation steps\n",
    "        Model Details \n",
    "        Hyperparameter\n",
    "        List of Validation Metrics and resepctive scores\n",
    "        Compare Validation scores of white paper with model's validation scores\n",
    "        Brief of comparision of scores\n",
    "\n",
    "        List and Track of critical metrics - these should not be lower than mentioned (in white paper)\n",
    "\n",
    "6. Respective scores for tracked metrics (confidence on generation)\n",
    "7. If required update prompt and go back to step 4 and reiterate step 4 and 5. reason (geneation have \n",
    "   heiger confidence )\n",
    "8. Outout should be in structured format (This will be input for summary block with affitional 2 inputs)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1358c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\",  \n",
    "                 temperature=0,\n",
    "                 openai_api_key= openai_api_key)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f30bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def read_notebook(file_path):\n",
    "    \"\"\"Read .ipynb notebook and extract content.\"\"\"\n",
    "    nb = nbformat.read(file_path, as_version=4)\n",
    "    content = []\n",
    "    for cell in nb.cells:\n",
    "        if cell.cell_type == 'markdown':\n",
    "            content.append(\"## Markdown Cell:\\n\" + cell.source)\n",
    "        elif cell.cell_type == 'code':\n",
    "            content.append(\"## Code Cell:\\n```python\\n\" + cell.source + \"\\n```\")\n",
    "    return \"\\n\\n\".join(content)\n",
    "\n",
    "\n",
    "def read_file(file_path):\n",
    "    \"\"\"Reads the content of a file.\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "\n",
    "def extract_functionalities_from_code(notebook_content):\n",
    "\n",
    "    \"\"\"Uses LLM to extract functionalities from Python code.\"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are an expert Python code reviewer. Here is a Jupyter notebook:\n",
    "    {notebook_content}\n",
    "\n",
    "    The following is a Jupyter notebook content (code and markdown). \n",
    "    Please extract the following:\n",
    "    Analyze the notebook and answer:\n",
    "\n",
    "    1. List of features used in the model.\n",
    "    2. Name/type of ML model used, only name of model\n",
    "    3. Accuracy metrics (e.g., accuracy, F1, precision, recall, AUC, etc.), only metrics name. \n",
    "    4. What is the purpose of this notebook?\n",
    "    5. What are the main operations and their results?\n",
    "    6. Are there any errors or anomalies in outputs?\n",
    "    7. What conclusions can be drawn from the outputs?\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke([SystemMessage(content=\"You are a helpful assistant.\"), HumanMessage(content=prompt)])\n",
    "\n",
    "    return response.content.strip()\n",
    "\n",
    "\n",
    "def extract_functionalities_from_whitepaper(whitepaper_text):\n",
    "    \"\"\"Uses LLM to extract functionalities from whitepaper.\"\"\"\n",
    "    prompt = [\n",
    "        SystemMessage(content=\"You are a product analyst.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Here is the whitepaper or product requirement document:\n",
    "\n",
    "        {whitepaper_text}\n",
    "\n",
    "        List all functionalities or features the whitepaper mentions. Use bullet points.\n",
    "        \"\"\")\n",
    "            ]\n",
    "    return llm(prompt).content.strip()\n",
    "\n",
    "\n",
    "def compare_functionalities(whitepaper_funcs, code_funcs):\n",
    "    \"\"\"Compares two sets of functionalities using the LLM.\"\"\"\n",
    "    prompt = [\n",
    "        SystemMessage(content=\"You are a software QA expert.\"),\n",
    "        HumanMessage(content=f\"\"\"\n",
    "        Whitepaper Functionalities:\n",
    "        {whitepaper_funcs}\n",
    "\n",
    "        Code Functionalities:\n",
    "        {code_funcs}\n",
    "        Extract validation metrics from code funcs eg, precision, recall and other validation are in output cell.\n",
    "        Compare the two lists and identify which functionalities from the whitepape, if functionality is implemented in code but not available in white paper, print: white paper is not updated please update the document. and show details of each section \n",
    "        listmissing sections like feature and if model varies according to white paper and same for validation metrics.\n",
    "        compare validation scores : Compare scores of code function with white paper.\n",
    "        Also compare critical validation metrics: make sure critical metrics of code should be grater then white paper critical metrics\n",
    "        if thereis no changhe in metrics of docuemt and code_funcs: keep output 'white paper is updated please proceed to next steps. no other information is required'  \n",
    "        \n",
    "        \"\"\")\n",
    "            ]\n",
    "    return llm(prompt).content.strip()\n",
    "\n",
    "\n",
    "def read_notebook_with_outputs(file_path):\n",
    "    \"\"\"Read .ipynb notebook and include both code and output.\"\"\"\n",
    "    nb = nbformat.read(file_path, as_version=4)\n",
    "    cells_content = []\n",
    "\n",
    "    for cell in nb.cells:\n",
    "        if cell.cell_type == 'markdown':\n",
    "            cells_content.append(f\"## Markdown Cell:\\n{cell.source}\")\n",
    "        elif cell.cell_type == 'code':\n",
    "            code = f\"## Code Cell:\\n```python\\n{cell.source}\\n```\"\n",
    "            outputs = []\n",
    "\n",
    "            for output in cell.get(\"outputs\", []):\n",
    "                if output.output_type == \"stream\":\n",
    "                    outputs.append(f\"Output (stream):\\n{output.text}\")\n",
    "                elif output.output_type == \"execute_result\":\n",
    "                    # Display the result of the cell (e.g., print(2+2))\n",
    "                    result = output.get(\"data\", {}).get(\"text/plain\", \"\")\n",
    "                    outputs.append(f\"Output (execute_result):\\n{result}\")\n",
    "                elif output.output_type == \"error\":\n",
    "                    outputs.append(\"Error:\\n\" + \"\\n\".join(output.get(\"traceback\", [])))\n",
    "\n",
    "            full_output = \"\\n\".join(outputs)\n",
    "            if full_output:\n",
    "                code += f\"\\n\\n### Output:\\n```\\n{full_output}\\n```\"\n",
    "            cells_content.append(code)\n",
    "\n",
    "    return \"\\n\\n\".join(cells_content)\n",
    "\n",
    "def read_notebook(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return nbformat.read(f, as_version=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcfe8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def main():\n",
    "    st.set_page_config(page_title=\"Functionality Coverage Checker\", layout=\"wide\")\n",
    "    \n",
    "    st.title(\"🧠 AI Feature Mapping Validator\")\n",
    "    st.subheader(\"Compare functionalities between a Whitepaper and its Codebase\")\n",
    "\n",
    "    uploaded_whitepaper = st.file_uploader(\"📄 Upload Whitepaper File\", type=[\"txt\", \"md\", \"pdf\"])\n",
    "    uploaded_code = st.file_uploader(\"💻 Upload Code File\", type=[\"py\", \"txt\", \"ipynb\"])\n",
    "\n",
    "    if uploaded_whitepaper and uploaded_code:\n",
    "        if st.button(\"Click to Process Files\"):\n",
    "            # Read whitepaper content\n",
    "            whitepaper = uploaded_whitepaper.read().decode(\"utf-8\")\n",
    "\n",
    "            # Handle .ipynb or other code files\n",
    "            if uploaded_code.name.endswith(\".ipynb\"):\n",
    "                # Write the raw content to a temp file\n",
    "                with tempfile.NamedTemporaryFile(delete=False, suffix=\".ipynb\", mode='wb') as tmp_file:\n",
    "                    tmp_file.write(uploaded_code.read())\n",
    "                    temp_file_path = tmp_file.name\n",
    "\n",
    "                # notebook_contents = read_notebook(temp_file_path)\n",
    "                notebook_contents = read_notebook_with_outputs(temp_file_path)\n",
    "                code_funcs = extract_functionalities_from_code(notebook_contents)\n",
    "            else:\n",
    "                code = uploaded_code.read().decode(\"utf-8\")\n",
    "                code_funcs = extract_functionalities_from_code(code)\n",
    "\n",
    "            whitepaper_funcs = extract_functionalities_from_whitepaper(whitepaper)\n",
    "\n",
    "            st.markdown(\"### ⚖️ Comparing Functionalities\")\n",
    "            missing_funcs = compare_functionalities(whitepaper_funcs, code_funcs)\n",
    "            st.markdown(missing_funcs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c540a32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29f7a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f1b071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c25394f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a51519b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import nbformat\n",
    "# from openai import OpenAI\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.schema import SystemMessage, HumanMessage\n",
    "# from langchain.chat_models import ChatOpenAI\n",
    "# from langchain.schema import SystemMessage, HumanMessage\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# from dotenv import load_dotenv\n",
    "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# import chromadb\n",
    "# from chromadb.config import Settings\n",
    "\n",
    "# import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3769bc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries = [\n",
    "#             \"Summary/Objective of white paper \",\n",
    "#             \"Features mentioned\",\n",
    "#             \"Preprocessing steps and data transformation steps\",\n",
    "#             \"Model selected for classification\",\n",
    "#             \"Training and resting methodology\",\n",
    "#             \"List of Hyper parameters and respective values\",\n",
    "#             \"What are list of validation scores and the performance scores?\",\n",
    "#             \"Ethical considerations\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b706715a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def queryFun(query, embedding_model,collection):\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    l_docs = []\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=5)\n",
    "    for doc in results[\"documents\"][0]:\n",
    "        l_docs.append(doc)\n",
    "        # print(\"🔎 Match:\", l_docs.append(doc))\n",
    "    return l_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81786eca",
   "metadata": {},
   "source": [
    "##  Check Embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4fc45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from chromadb import PersistentClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723ab5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./chroma_openai1\"\n",
    "\n",
    "# Step 1: Load the persistent client\n",
    "chroma_client = PersistentClient(path=path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935ed8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "collections = chroma_client.list_collections()\n",
    "for col in collections:\n",
    "    print(col.name)\n",
    "    # print(col.metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e06b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"notebook_f887cf79f48bf8b101631e9ebdb3ca7220bd2c6e47a6b82041ca192aa98cf16b\"\n",
    "# Step 2: Access the existing collection\n",
    "collection = chroma_client.get_collection(name=collection_name)\n",
    "data = collection.get()\n",
    "\n",
    "# Optional: View details\n",
    "print(\"IDs:\", data['ids'])\n",
    "print(\"Documents:\", data['documents'][:2])  # print only first 2 docs\n",
    "print(\"Metadata:\", data.get('metadatas'))  # only if metadata was stored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418da824",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7484104f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: View details\n",
    "print(\"IDs:\", data['ids'])\n",
    "print(\"Documents:\", data['documents'][:2])  # print only first 2 docs\n",
    "print(\"Metadata:\", data.get('metadatas'))  # only if metadata was stored\n",
    "print(\"Embeddings:\", data['embeddings'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211070c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6279493b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253ac37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = client.get_collection(\"whitepaper_02958f41437b5bbcf9490a38b0edb5d41a365101ce7979d2822648a320dfdc73\")\n",
    "\n",
    "results = collections[6].get(\n",
    "    ids=[\"id1\", \"id2\"],    # optional\n",
    "    where={\"type\": \"pdf\"}, # optional\n",
    "    include=[\"documents\", \"metadatas\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1181a8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d63925d",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Embedding stored or not \n",
    "import os\n",
    "import nbformat\n",
    "from openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "import streamlit as st\n",
    "import fitz\n",
    "import tempfile\n",
    "import hashlib\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, openai_api_key=openai_api_key)\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b40a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_hash(uploaded_file):\n",
    "    uploaded_file.seek(0)\n",
    "    hash_val = hashlib.sha256(uploaded_file.read()).hexdigest()\n",
    "    uploaded_file.seek(0)\n",
    "    return hash_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85886d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_from_pdf(uploaded_file):\n",
    "    doc = fitz.open(stream=uploaded_file.read(), filetype=\"pdf\")\n",
    "    extracted_text = \"\"\n",
    "    for page_num, page in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "        extracted_text += f\"\\n\\n--- Page {page_num + 1} ---\\n{text}\"\n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537bf1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collection_exists(collection_name):\n",
    "    try:\n",
    "        chroma_client.get_collection(collection_name)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60a085c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=750,     # faster, smaller chunk\n",
    "        chunk_overlap=100   # reduced overlap\n",
    "    )\n",
    "    return text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b051a7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_or_create_embeddings(uploaded_file, text, _embedding_model, collection_name):\n",
    "    chunks = create_chunks(text)\n",
    "\n",
    "    if collection_exists(collection_name):\n",
    "        collection = chroma_client.get_collection(collection_name)\n",
    "    else:\n",
    "        embeddings = _embedding_model.embed_documents(chunks)\n",
    "        print(embeddings)\n",
    "        collection = store_in_chromaDB(chunks, embeddings, collection_name)\n",
    "\n",
    "    return collection, chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad320aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "import fitz\n",
    "\n",
    "def get_file_hash(file_obj):\n",
    "    file_obj.seek(0)\n",
    "    hash_val = hashlib.sha256(file_obj.read()).hexdigest()\n",
    "    file_obj.seek(0)\n",
    "    return hash_val\n",
    "\n",
    "def extract_from_pdf(file_obj):\n",
    "    file_obj.seek(0)\n",
    "    doc = fitz.open(stream=file_obj.read(), filetype=\"pdf\")\n",
    "    extracted_text = \"\"\n",
    "    for page_num, page in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "        extracted_text += f\"\\n\\n--- Page {page_num + 1} ---\\n{text}\"\n",
    "    return extracted_text\n",
    "\n",
    "file_path = \"Load Prediction Whitepaper.pdf\"\n",
    "\n",
    "with open(file_path, \"rb\") as f:\n",
    "    # 1. Compute hash\n",
    "    file_hash = get_file_hash(f)\n",
    "    # 2. Extract text\n",
    "    text = extract_from_pdf(f)\n",
    "    # 3. If needed, reset pointer for further use (not always needed)\n",
    "    f.seek(0)\n",
    "    # 4. Pass to embedding function (if needed)\n",
    "    collection, chunks = get_or_create_embeddings(\n",
    "        uploaded_file=f,              # If function needs file object\n",
    "        text=text,                    # If function needs text\n",
    "        _embedding_model=embedding_model,\n",
    "        collection_name=f\"whitepaper_{file_hash}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec64fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb95ef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_summary_of_events():\n",
    "    # with open(\"push_events.json\", \"r\") as file:\n",
    "    #     data = json.load(file)\n",
    "    # owner = data[2]\n",
    "    # repo_name = data[3]\n",
    "    api_url = f\"https://api.github.com/repos/arunkenwal02/code-validator/events\"\n",
    "    response = requests.get(api_url)\n",
    "    events = response.json()\n",
    "    push_ids = [id['id'] for id in events]\n",
    "    print(push_ids)\n",
    "    data = ['52949273211','52821120274']\n",
    "    push_events = [e for e in events if e['type'] == 'PushEvent']\n",
    "\n",
    "    ids = [e['id'] for e in push_events]\n",
    "    try:\n",
    "        idx1 = ids.index(data[0])\n",
    "        idx2 = ids.index(data[1])\n",
    "    except ValueError:\n",
    "        return {\"error\": \"One or both push IDs not found in recent events.\"}\n",
    "\n",
    "    start = min(idx1, idx2)\n",
    "    end = max(idx1, idx2)\n",
    "\n",
    "    history_between = push_events[start:end+1]  \n",
    "    grouped_push_events = []\n",
    "    commits_list = []\n",
    "    for event in history_between:\n",
    "        push_id = event['id']\n",
    "        created_at = event['created_at']\n",
    "        repo = event['repo']['name']\n",
    "        commits_list = []\n",
    "\n",
    "        for commit in event[\"payload\"][\"commits\"]:\n",
    "            sha = commit['sha']\n",
    "            author = commit['author']['name']\n",
    "            message = commit['message']\n",
    "\n",
    "            commit_detail_url = f\"https://api.github.com/repos/arunkenwal02/code-validator/commits/{sha}\"\n",
    "            commit_detail_response = requests.get(commit_detail_url)\n",
    "\n",
    "            if commit_detail_response.status_code != 200:\n",
    "                diff = \"❌ Failed to fetch diff\"\n",
    "            else:\n",
    "                commit_detail = commit_detail_response.json()\n",
    "                diffs = []\n",
    "                for file in commit_detail.get('files', []):\n",
    "                    patch = file.get('patch')\n",
    "                    if patch:\n",
    "                        diffs.append(f\"File: {file['filename']}\\n{patch}\")\n",
    "                diff = \"\\n\\n\".join(diffs) if diffs else \"No diff available\"\n",
    "\n",
    "            commits_list.append({\n",
    "                \"sha\": sha,\n",
    "                \"author\": author,\n",
    "                \"commit_message\": message,\n",
    "                \"code_diff\": diff\n",
    "            })\n",
    "\n",
    "        grouped_push_events.append({\n",
    "            \"push_id\": push_id,\n",
    "            \"repo\": repo,\n",
    "            \"created_at\": created_at,\n",
    "            \"commits\": commits_list\n",
    "        })\n",
    "    return grouped_push_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdfe6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = create_summary_of_events()\n",
    "test\n",
    "push id , commit summary, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a64e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499acb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc3313b",
   "metadata": {},
   "source": [
    "## One drive file access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bacde4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import msal\n",
    "import requests\n",
    "import time\n",
    "import fitz\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() \n",
    "Permission_ID =\"6a94cb3a-9869-4b54-ae0b-f4f523df2614\"\n",
    "client_id = Permission_ID\n",
    "authority = \"https://login.microsoftonline.com/consumers\"\n",
    "scopes = [\"Files.Read\"]\n",
    "source_folder = \"Documents/GitHub/code-validator/\"\n",
    "file_name = \"Load Prediction Whitepaper.pdf\"\n",
    "version_id = int(3)\n",
    "file_path = source_folder+file_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b81431f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract updated version \n",
    "\n",
    "def access_token_key(client_id, authority):\n",
    "    scopes = [\"Files.Read\"]\n",
    "    app = msal.PublicClientApplication(client_id=client_id, authority=authority)\n",
    "    result = None\n",
    "\n",
    "    accounts = app.get_accounts()\n",
    "    if accounts:\n",
    "        result = app.acquire_token_silent(scopes, account=accounts[0])\n",
    "    if not result:\n",
    "        result = app.acquire_token_interactive(scopes=scopes)\n",
    "    if not result or \"access_token\" not in result:\n",
    "        print(\"MSAL Error:\", result)\n",
    "    access_token = result[\"access_token\"]\n",
    "\n",
    "    return access_token\n",
    "\n",
    "\n",
    "def get_raw_data(client_id, authority ,file_path ):\n",
    "    access_token= access_token_key(client_id=client_id, authority=authority)\n",
    "    url = f\"https://graph.microsoft.com/v1.0/me/drive/root:/{file_path}:/content\"\n",
    "    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "    time.sleep(2)\n",
    "    response = requests.get(url, headers=headers)\n",
    "    print(f\"Response code: {response.status_code}\")\n",
    "    if response.status_code == 200:\n",
    "        file_bytes = response.content\n",
    "        print(\"File read into memory!\")\n",
    "        return file_bytes\n",
    "    else:\n",
    "        print(\"Failed:\", response.status_code, response.text)\n",
    "        return None\n",
    "\n",
    "\n",
    "def get_onedrive_whitepaper(file_bytes):\n",
    "    \n",
    "    # file_bytes is from above\n",
    "    doc = fitz.open(stream=file_bytes, filetype=\"pdf\")\n",
    "    text = \"\"\n",
    "    for page_num, page in enumerate(doc):\n",
    "        text += f\"\\n\\n--- Page {page_num + 1} ---\\n{page.get_text()}\"\n",
    "\n",
    "    print(\"First 1000 chars of PDF text:\", text)\n",
    "    \n",
    "    return text\n",
    "   \n",
    "\n",
    "def prev_version( client_id, authority, file_path, version_id):\n",
    "    access_token= access_token_key(client_id=client_id, authority=authority)\n",
    "\n",
    "    versions_url = f\"https://graph.microsoft.com/v1.0/me/drive/root:/{file_path}:/versions\"\n",
    "    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n",
    "    response = requests.get(versions_url, headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        versions = response.json()[\"value\"]\n",
    "        if len(versions) >= int(version_id):\n",
    "            # 3. Get the 2nd version (index 1)\n",
    "            version_id = versions[1]['id']\n",
    "            print(f\"2nd Version ID: {version_id}, Last Modified: {versions[1]['lastModifiedDateTime']}\")\n",
    "            \n",
    "            # 4. Fetch 2nd version's PDF bytes\n",
    "            download_url = f\"https://graph.microsoft.com/v1.0/me/drive/root:/{file_path}:/versions/{version_id}/content\"\n",
    "            version_response = requests.get(download_url, headers=headers)\n",
    "            if version_response.status_code == 200:\n",
    "                pdf_bytes = version_response.content  # This is your PDF in memory\n",
    "                \n",
    "                # 5. Extract text from the PDF (in memory, no save)\n",
    "                doc = fitz.open(stream=pdf_bytes, filetype=\"pdf\")\n",
    "                all_text = \"\"\n",
    "                for page_num, page in enumerate(doc):\n",
    "                    all_text += f\"\\n--- Page {page_num+1} ---\\n{page.get_text()}\"\n",
    "                \n",
    "                print(\"Extracted PDF text (first 1000 chars):\")\n",
    "                print(all_text[:1000])\n",
    "                return all_text\n",
    "                # You can use `all_text` as needed (search, LLM input, etc)\n",
    "            else:\n",
    "                print(\"Failed to download 2nd version:\", version_response.status_code, version_response.text)\n",
    "        else:\n",
    "            print(\"Less than 2 versions available!\")\n",
    "    else:\n",
    "        print(\"Failed to fetch versions:\", response.status_code, response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c4bc5b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response code: 200\n",
      "File read into memory!\n",
      "First 1000 chars of PDF text: \n",
      "\n",
      "--- Page 1 ---\n",
      "Machine Learning-Based Loan Approval Prediction System\n",
      "for Financial Institutions\n",
      "1. Executive Summary\n",
      "The financial services industry faces a critical challenge in automating and de-risking\n",
      "the loan approval process. Traditional methods, often relying on manual review and\n",
      "static rule-based systems, are prone to human error, inconsistency, and significant\n",
      "processing delays. These limitations result in missed opportunities, elevated credit risk,\n",
      "and suboptimal customer experiences. To address these issues, we have developed a\n",
      "robust machine learning-based Loan Approval Classification System. This model\n",
      "leverages a comprehensive set of applicant data to predict the likelihood of loan\n",
      "repayment, classifying applications as either approved or rejected.\n",
      "Our system is designed to provide a high-level overview of an applicant's\n",
      "creditworthiness, offering a data-driven, objective, and transparent decision-making tool.\n",
      "By integrating advanced machine learning techniques, our model achieves superior\n",
      "predictive accuracy compared to traditional methods. It significantly reduces the time\n",
      "from application to decision, minimizes the risk of default, and ensures a consistent, fair\n",
      "evaluation process. This strategic asset not only enhances operational efficiency but\n",
      "also provides a competitive advantage by enabling faster, more confident lending\n",
      "decisions.\n",
      "2. Introduction\n",
      "The process of loan approval is a cornerstone of the financial industry. It involves\n",
      "evaluating a multitude of factors to determine an applicant's creditworthiness and ability\n",
      "to repay a loan. Historically, this process has been labor-intensive, relying on credit\n",
      "officers to manually review application forms, financial statements, and credit reports.\n",
      "This manual approach is slow, expensive, and susceptible to biases. The rise of digital\n",
      "banking and the demand for instant financial services have made this traditional model\n",
      "increasingly unsustainable.\n",
      "This white paper details a machine learning-based solution designed to modernize and\n",
      "optimize the loan approval workflow. By building a classification model, our system can\n",
      "accurately predict the Loan_Status (Approved or Rejected) for new applications. The\n",
      "primary motivation for this project is to create a scalable, efficient, and fair system that\n",
      "can process thousands of applications in real-time, reducing operational costs while\n",
      "simultaneously improving the quality of lending decisions. Our model is intended to\n",
      "serve as a decision-support tool for loan officers, enabling them to focus on complex\n",
      "cases and customer relationships rather than routine data analysis.\n",
      "\n",
      "\n",
      "--- Page 2 ---\n",
      "3. Related Work / Literature Review\n",
      "The field of credit scoring and loan prediction has seen extensive research and\n",
      "application of various machine learning models. A number of algorithms have been\n",
      "employed to analyze applicant data and forecast loan outcomes:\n",
      "·\n",
      "Logistic Regression: A statistical method used for binary classification, which is\n",
      "well-suited for predicting a 'yes' or 'no' outcome for loan approval. It is valued for\n",
      "its simplicity and the interpretability of its results, as it shows how different factors\n",
      "influence the final decision.\n",
      "·\n",
      "Decision Trees: These models use a tree-like structure of decisions and their\n",
      "possible consequences. They are easy to understand and visualize, as they\n",
      "mimic human decision-making processes.\n",
      "\n",
      "\n",
      "--- Page 3 ---\n",
      "·\n",
      "Random Forests: An ensemble method that builds multiple decision trees and\n",
      "combines their predictions to improve accuracy and reduce overfitting.\n",
      "·\n",
      "Steps of ML Algorithms:\n",
      "\n",
      "\n",
      "--- Page 4 ---\n",
      "4. Data Description\n",
      "The model is trained on a comprehensive dataset of past loan applications. The dataset\n",
      "contains a mix of demographic, financial, and behavioral features.\n",
      "Feature Name and Definition:\n",
      "·\n",
      "Loan_ID: A unique identifier for each loan application.\n",
      "·\n",
      "Gender: The applicant's gender (Male/Female).\n",
      "·\n",
      "Married: Marital status of the applicant (Yes/No).\n",
      "·\n",
      "Dependents: Number of dependents the applicant has.\n",
      "·\n",
      "Education: Applicant's education level (Graduate/Not Graduate).\n",
      "·\n",
      "Self_Employed: Whether the applicant is self-employed (Yes/No).\n",
      "·\n",
      "Applicant_Income: The applicant's monthly income.\n",
      "·\n",
      "Coapplicant_Income: The co-applicant's monthly income.\n",
      "·\n",
      "Loan_Amount: The amount of the loan requested.\n",
      "·\n",
      "Loan_Amount_Term: The term of the loan in months.\n",
      "·\n",
      "Credit_History: A binary variable indicating if the applicant has a good credit\n",
      "history (1.0) or not (0.0). This is a critical predictor.\n",
      "·\n",
      "Property_Area: The area where the property is located\n",
      "(Rural/Semiurban/Urban).\n",
      "·\n",
      "Loan_Status: The target variable, indicating if the loan was approved (Y) or\n",
      "rejected (N).\n",
      "Preprocessing Steps:\n",
      "1.\n",
      "Handling Missing Values: Missing values are common in real-world data. We\n",
      "employ different strategies based on the feature type:\n",
      "o\n",
      "Categorical Features: Missing values in Gender, Married, Dependents,\n",
      "Self_Employed, and Credit_History are imputed using the mode (most\n",
      "frequent value) of the respective columns.\n",
      "o\n",
      "Numerical Features: Missing values in Loan_Amount and\n",
      "Loan_Amount_Term are imputed using the mean or median to avoid\n",
      "skewing the distribution.\n",
      "1.\n",
      "Data Type Consistency: All features are checked for consistent data types.\n",
      "Numerical features are stored as integers or floats, while categorical features are\n",
      "stored as strings or object types.\n",
      "2.\n",
      "Data Balancing: An analysis of the Loan_Status target variable revealed an\n",
      "imbalance, with a significantly higher number of approved loans than rejected\n",
      "ones. This imbalance can bias a model to favor the majority class. Imbalance\n",
      "\n",
      "\n",
      "--- Page 5 ---\n",
      "data handled by ‘class_weight’ = ‘balanced’ parameter in Logistic Regression\n",
      "model.\n",
      "𝑤𝑖= 𝑘⋅𝑛𝑖\n",
      "𝑛\n",
      "§\n",
      "wᵢis the weight for class/sample i\n",
      "§\n",
      "k is a constant (e.g., total desired sample size or scaling factor)\n",
      "§\n",
      "nᵢis the count/frequency of class i\n",
      "§\n",
      "n is the total number of samples\n",
      "o\n",
      "Synthetic Minority Over-sampling Technique (SMOTE) during the\n",
      "training phase to create synthetic data points for the minority class,\n",
      "ensuring the model is not biased and can accurately identify both\n",
      "approved and rejected applications.\n",
      "3.\n",
      "Data Transformation:\n",
      "o\n",
      "Numerical Columns: We apply a normalization technique (e.g.,\n",
      "StandardScaler) to numerical columns (Applicant_Income,\n",
      "Coapplicant_Income, Loan_Amount) to ensure they have a zero mean\n",
      "and unit variance. This prevents features with larger magnitudes from\n",
      "dominating the model's training process.\n",
      "o\n",
      "Categorical Features: We use One-Hot Encoding to convert categorical\n",
      "features (Gender, Married, Education, etc.) into a numerical format\n",
      "suitable for the model. This creates new binary columns for each unique\n",
      "category, avoiding the assumption of ordinality that simple label encoding\n",
      "might introduce.\n",
      "5. Model Architecture\n",
      "Why Logistic Regression: Preferred choice\n",
      "We selected Logistic Regression for the loan approval model because it provides high\n",
      "transparency, explainability, and auditability — essential factors in regulated financial\n",
      "environments. While other models like Random Forest marginally outperform it in\n",
      "accuracy, Logistic Regression allows us to clearly communicate how each feature\n",
      "contributes to the final decision, enabling easier compliance with fairness, bias\n",
      "detection, and model governance requirements\n",
      "·\n",
      "Operating in a highly regulated environment (e.g. banking, insurance)\n",
      "·\n",
      "Need to explain decisions to compliance officers or regulators\n",
      "·\n",
      "Wanted easy-to-track fairness or bias metrics\n",
      "·\n",
      "Prioritize transparency over marginal gains in accuracy\n",
      "\n",
      "\n",
      "--- Page 6 ---\n",
      "Feature Name\n",
      "Type\n",
      "Gender\n",
      "Categorical\n",
      "Married\n",
      "Categorical\n",
      "Dependents\n",
      "Categorical\n",
      "Education\n",
      "Categorical\n",
      "Self_Employed\n",
      "Categorical\n",
      "ApplicantIncome\n",
      "Numerical\n",
      "CoapplicantIncome\n",
      "Numerical\n",
      "LoanAmount\n",
      "Numerical\n",
      "Loan_Amount_Term\n",
      "Numerical\n",
      "Credit_History\n",
      "Binary (0 or 1)\n",
      "Property_Area\n",
      "Categorical\n",
      "General Equation of Logistic Regression\n",
      "𝑃𝑌= 1 =\n",
      "1\n",
      "1 + 𝑒−𝑍\n",
      "𝑍= 𝛽0 + 𝛽1𝑋1 + 𝛽2𝑋2 +⋯+ 𝛽𝑛𝑋𝑛\n",
      "𝑃𝑌= 1 = 𝑝𝑟𝑜𝑏𝑎𝑏𝑖𝑙𝑖𝑡𝑦 𝑜𝑓 𝑙𝑜𝑎𝑛 𝑎𝑝𝑝𝑟𝑜𝑣𝑎𝑙\n",
      "𝛽0 = 𝑖𝑛𝑡𝑒𝑟𝑐𝑒𝑝𝑡 (𝑏𝑖𝑎𝑠 𝑡𝑒𝑟𝑚)\n",
      "𝛽𝑖= 𝑐𝑜𝑒𝑓𝑓𝑖𝑐𝑖𝑒𝑛𝑡 𝑓𝑜𝑟 𝑓𝑒𝑎𝑡𝑢𝑟𝑒 𝑋𝑖\n",
      "𝑍= 𝛽0 + 𝛽1 ⋅𝐺𝑒𝑛𝑑𝑒𝑟+ 𝛽2 ⋅𝑀𝑎𝑟𝑟𝑖𝑒𝑑+ 𝛽3 ⋅𝐷𝑒𝑝𝑒𝑛𝑑𝑒𝑛𝑡𝑠+ 𝛽4 ⋅𝐸𝑑𝑢𝑐𝑎𝑡𝑖𝑜𝑛+ 𝛽5 ⋅\n",
      "𝑆𝑒𝑙𝑓_𝐸𝑚𝑝𝑙𝑜𝑦𝑒𝑑+ 𝛽6 ⋅𝐴𝑝𝑝𝑙𝑖𝑐𝑎𝑛𝑡𝐼𝑛𝑐𝑜𝑚𝑒+ 𝛽7 ⋅𝐶𝑜𝑎𝑝𝑝𝑙𝑖𝑐𝑎𝑛𝑡𝐼𝑛𝑐𝑜𝑚𝑒+ 𝛽8 ⋅𝐿𝑜𝑎𝑛𝐴𝑚𝑜𝑢𝑛𝑡+ 𝛽9 ⋅\n",
      "𝐿𝑜𝑎𝑛_𝐴𝑚𝑜𝑢𝑛𝑡_𝑇𝑒𝑟𝑚+ 𝛽10 ⋅𝐶𝑟𝑒𝑑𝑖𝑡_𝐻𝑖𝑠𝑡𝑜𝑟𝑦+ 𝛽11 ⋅𝑃𝑟𝑜𝑝𝑒𝑟𝑡𝑦_𝐴𝑟𝑒𝑎_𝑈𝑟𝑏𝑎𝑛+ 𝛽12 ⋅\n",
      "𝑃𝑟𝑜𝑝𝑒𝑟𝑡𝑦_𝐴𝑟𝑒𝑎_𝑆𝑒𝑚𝑖𝑢𝑟𝑏𝑎𝑛\n",
      "6. Training Methodology\n",
      "The training process is meticulously designed to ensure the model is robust, accurate,\n",
      "and ready for production.\n",
      "1.\n",
      "Data Split:\n",
      "·\n",
      "Dataset split into three parts:\n",
      "o\n",
      "Training set: 70%\n",
      "o\n",
      "Testing set: 30%\n",
      "·\n",
      "Training set purpose: Used to train the model.\n",
      "·\n",
      "Validation set purpose: Used for hyperparameter tuning and model selection.\n",
      "\n",
      "\n",
      "--- Page 7 ---\n",
      "·\n",
      "Testing set purpose: Reserved for final, unbiased evaluation of model\n",
      "performance.\n",
      "2.\n",
      "Hyperparameter Tuning: We use techniques like Grid Search or Bayesian\n",
      "Optimization to find the optimal set of hyperparameters for the Logixtic\n",
      "Regression model. Key parameters tuned include\n",
      "o\n",
      "Penalty: Specifies the type of regularization used to avoid overfitting\n",
      "o\n",
      "C: Inverse of regularization strength (i.e., smaller C means stronger\n",
      "regularization).\n",
      "o\n",
      "Solver: Optimization algorithm used for finding model coefficients.\n",
      "o\n",
      "Max_iter: Maximum number of iterations taken by the solver to converge.\n",
      "3.\n",
      "Validation Strategy: We employ k-fold cross-validation during the training\n",
      "phase. The training data is divided into k folds. The model is trained k times,\n",
      "each time using a different fold as the validation set. This robust strategy ensures\n",
      "the model's performance is not specific to a single data split.\n",
      "4.\n",
      "Retraining Pipelines: The model is not a static artifact. It is part of a continuous\n",
      "learning loop. A retraining pipeline is established to periodically retrain the model\n",
      "on new data, typically on a monthly or quarterly basis. This ensures the model\n",
      "remains relevant and its predictions accurate as consumer behavior and\n",
      "economic conditions change.\n",
      "7. Evaluation Metrics\n",
      "To assess the model's performance, we utilize a suite of metrics tailored to the financial\n",
      "domain. A simple accuracy score is often misleading in imbalanced classification\n",
      "problems, so we rely on a more comprehensive set of metrics.\n",
      "Business-Specific Interpretation\n",
      "Scenario\n",
      "Prioritize\n",
      "You want to maximize profit while reducing risky loans\n",
      "Precision, F1-score\n",
      "You want to not miss good applicants\n",
      "Recall, F1-score\n",
      "You’re building a regulatory-compliant, fair model\n",
      "Balanced Accuracy,\n",
      "Fairness metrics\n",
      "You prioritize avoiding defaults (minimize false approvals)\n",
      "High Precision\n",
      "You prioritize financial inclusion (minimize false rejections)\n",
      "High Recall\n",
      "You want a balanced approval system for early model\n",
      "evaluation\n",
      "F1-score\n",
      "\n",
      "\n",
      "--- Page 8 ---\n",
      "1. Our primary objective is to avoid loan defaults, so we prioritize Precision to\n",
      "reduce false approvals.\n",
      "2. To ensure a balanced and inclusive approval system, we use the F1-score,\n",
      "which helps capture both Precision and Recall, ensuring we do not miss eligible\n",
      "applicants.\n",
      "3. Since the dataset is highly imbalanced, we use PR-AUC (Precision-Recall\n",
      "AUC) for a more reliable evaluation of model performance.\n",
      "·\n",
      "Precision: Prioritize avoiding defaults (minimize false approvals)\n",
      "Precision =\n",
      "TruePositives\n",
      "TruePositives + FalsePositives\n",
      "·\n",
      "Recall (Sensitivity): Prioritize financial inclusion (minimize false rejections)\n",
      "Recall =\n",
      "TruePositives\n",
      "TruePositives + FalseNegatives\n",
      "·\n",
      "F1-Score: Wanted balanced approval system for early model evaluation\n",
      "F1 - Score = 2 × Precision × Recall\n",
      "Precision + Recall\n",
      "·\n",
      "PR-AUC (Precision-Recall AUC): In our imbalanced setting, we're more interested\n",
      "in how well the model identifies truly eligible applicants. PR-AUC gives a more\n",
      "realistic view of our model's performance than ROC-AUC or raw precision alone.\n",
      "Data is imbalance and more informative than ROC-AUC (e.g., 90% loan denials,\n",
      "10%\n",
      "approvals).\n",
      "𝑃𝑅−𝐴𝑈𝐶=\n",
      "1\n",
      "0\n",
      "𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛𝑅𝑒𝑐𝑎𝑙𝑙𝑑𝑅𝑒𝑐𝑎𝑙𝑙\n",
      "Precision =\n",
      "TruePositives\n",
      "TruePositives+FalsePositives\n",
      "Recall =\n",
      "TruePositives\n",
      "TruePositives+FalseNegatives\n",
      "\n",
      "\n",
      "--- Page 9 ---\n",
      "Confusion Matrix\n",
      "\n",
      "\n",
      "--- Page 10 ---\n",
      "The confusion matrix reveals that the model has a very low rate of False Positives,\n",
      "which means it is very effective at avoiding the approval of risky loans. While there are\n",
      "some False Negatives (good loans that were rejected), the balance between precision\n",
      "and recall is strategically favorable for a conservative lending strategy.\n",
      "8. Results and Analysis:\n",
      "Performance Outcomes:\n",
      "We selected Logistic Regression for the loan approval model because it provides high\n",
      "transparency, explainability, and auditability — essential factors in regulated financial\n",
      "environments. While other models like Random Forest marginally outperform it in\n",
      "accuracy\n",
      "·\n",
      "Logistic Regression: is the most favorable model for the loan approval task. It\n",
      "achieves the highest Recall (0.960) and F1 Score (0.913). Ideal for minimizing\n",
      "both false negatives (missing eligible applicants) and false positives (approving\n",
      "risky loans).\n",
      "·\n",
      "Highly interpretable and transparent, making it suitable for regulated\n",
      "environments.\n",
      "·\n",
      "PR-AUC is the primary metric due to class imbalance. The Decision Tree\n",
      "achieves the highest PR-AUC (0.867). However, it offers lower interpretability\n",
      "compared to Logistic Regression, which can be a drawback in scenarios where\n",
      "model explainability is critical.\n",
      "·\n",
      "Handle Imbalance can bias a model to favor the majority class. Imbalance data\n",
      "handled by ‘class_weight’ = ‘balanced’ parameter in Logistic Regression\n",
      "model.\n",
      "𝑤𝑖= 𝑘⋅𝑛𝑖\n",
      "𝑛\n",
      "§\n",
      "wᵢis the weight for class/sample i\n",
      "§\n",
      "k is a constant (e.g., total desired sample size or scaling factor)\n",
      "§\n",
      "nᵢis the count/frequency of class i\n",
      "§\n",
      "n is the total number of samples\n",
      "Performance Scores – Logistic Regression with Optimized Hyper parameters:\n",
      "Best Hyperparameter:\n",
      "'params': {\n",
      "'penalty': ['l2'],\n",
      "\n",
      "\n",
      "--- Page 11 ---\n",
      "'C': [ 1,],\n",
      "'solver': ['liblinear'],\n",
      "'max_iter': [200]\n",
      "}\n",
      "Table 1:\n",
      "Metric\n",
      "Scores\n",
      "Recall\n",
      "95.9%\n",
      "F1-Score\n",
      "88.7%\n",
      "Accuracy\n",
      "87.1%\n",
      "Precision\n",
      "87.1%\n",
      "9. Limitations\n",
      "·\n",
      "Assumes Linear Relationship\n",
      "o\n",
      "Logistic Regression assumes a linear relationship between input features and the\n",
      "log-odds of the outcome (approval or rejection).\n",
      "o\n",
      "In reality, loan approval often depends on non-linear interactions (e.g., age vs.\n",
      "income vs. employment history), which Logistic Regression cannot capture\n",
      "without extensive feature engineering (e.g., polynomial terms or interactions).\n",
      "·\n",
      "Limited Expressiveness for Complex Patterns\n",
      "o\n",
      "While simple and interpretable, Logistic Regression lacks the capacity to model\n",
      "complex decision boundaries.\n",
      "o\n",
      "It may fail to identify intricate relationships or non-linear credit risk patterns (e.g.,\n",
      "when high income combined with high debt leads to risk).\n",
      "o\n",
      "This can lead to underfitting, where the model performs poorly even on training\n",
      "data.\n",
      "·\n",
      "Sensitive to Multicollinearity\n",
      "o\n",
      "Logistic Regression can be unstable when features are highly correlated (e.g.,\n",
      "income and credit score), leading to inflated or misleading coefficient values.\n",
      "o\n",
      "This not only harms predictive performance but also undermines model\n",
      "interpretability and trust among stakeholders.\n",
      "·\n",
      "Imbalanced Data Handling\n",
      "o\n",
      "Logistic Regression, by default, optimizes for overall accuracy, which is\n",
      "inappropriate for imbalanced datasets (common in loan approval, where most\n",
      "applicants are approved).\n",
      "o\n",
      "Without adjustments (like threshold tuning or class weighting), it can ignore the\n",
      "minority class (defaults), resulting in poor recall and higher risk exposure.\n",
      "\n",
      "\n",
      "--- Page 12 ---\n",
      "·\n",
      "Difficulty in Capturing Non-Binary Dependencies\n",
      "o\n",
      "It works best for binary outcomes, but real-world loan decisions are often\n",
      "influenced by multi-level risk factors, such as customer behavior segments, loan\n",
      "types, and geographic risk variations.\n",
      "o\n",
      "Extending logistic regression to multiclass or ordinal settings adds complexity\n",
      "and reduces interpretability.\n",
      "·\n",
      "Assumes Feature Independence\n",
      "o\n",
      "Logistic Regression assumes that features contribute independently to the\n",
      "prediction.\n",
      "o\n",
      "In practice, interactions between variables (e.g., employment type and income\n",
      "stability) can be critical, but are ignored unless explicitly modeled.\n",
      "·\n",
      "Static Nature Without Regular Updates\n",
      "o\n",
      "Logistic models require manual retraining to stay up to date.\n",
      "o\n",
      "They don’t adapt automatically to changes in economic conditions, lending\n",
      "regulations, or fraud patterns, which can reduce effectiveness over time.\n",
      "·\n",
      "Lack of Confidence Calibration\n",
      "o\n",
      "The predicted probabilities from Logistic Regression may be poorly calibrated,\n",
      "especially when applied on unseen or shifted data.\n",
      "o\n",
      "Overconfidence in predictions can lead to incorrect approvals or rejections,\n",
      "particularly in borderline cases.\n",
      "10. Deployment Strategy\n",
      "The loan approval model is deployed as a microservice within Mastercard’s cloud-based\n",
      "infrastructure. This architecture ensures high availability, scalability, and seamless\n",
      "integration with existing systems.\n",
      "·\n",
      "API Integration: The model is exposed via a RESTful API endpoint. When a\n",
      "new loan application is submitted, the relevant features are extracted and sent to\n",
      "this API. The API then returns a probability score and a classification\n",
      "(Approved/Rejected) in real-time.\n",
      "·\n",
      "Scalability: The microservice is containerized using Docker and orchestrated\n",
      "using Kubernetes. This setup allows the system to automatically scale up or\n",
      "down based on the volume of loan applications, ensuring low latency even during\n",
      "peak usage.\n",
      "·\n",
      "Integration with Core Systems: The API is integrated with the front-end\n",
      "application portal, the core banking system, and the loan officer's dashboard.\n",
      "This creates a streamlined workflow where the model's prediction is a primary\n",
      "input to the final decision.\n",
      "11. Ethical Considerations\n",
      "The use of AI in financial decisions, especially for something as significant as a loan,\n",
      "comes with substantial ethical responsibilities. We have embedded ethical\n",
      "considerations throughout the model's lifecycle.\n",
      "\n",
      "\n",
      "--- Page 13 ---\n",
      "·\n",
      "Fairness: Avoid bias toward gender, income, or region.\n",
      "·\n",
      "Transparency: Use clear explanations for all predictions.\n",
      "·\n",
      "Privacy: Protect applicant data and follow data laws.\n",
      "·\n",
      "Human Oversight: Route uncertain or sensitive cases to human reviewers.\n",
      "·\n",
      "Compliance: Regular checks to ensure fair lending practices.\n",
      "·\n",
      "Bias Mitigation: The model includes fairness-aware preprocessing techniques,\n",
      "such as reweighing and stratified sampling, to minimize discrimination based on\n",
      "gender, marital status, and education.\n",
      "·\n",
      "Transparency and Explainability: All loan decisions are supported by SHAP\n",
      "(SHapley Additive exPlanations) visualizations to explain individual predictions.\n",
      "These explanations are accessible to both analysts and applicants, ensuring\n",
      "transparency.\n",
      "·\n",
      "Privacy and Data Security: The system complies with GDPR and other regional\n",
      "regulations by implementing data encryption, anonymization, and secure access\n",
      "protocols during data collection, storage, and processing.\n",
      "·\n",
      "Human Oversight: High-impact or low-confidence decisions are flagged for\n",
      "manual review, especially when applicants are from vulnerable or high-risk\n",
      "segments.\n",
      "·\n",
      "Non-discrimination Compliance: Regular audits are performed to ensure that\n",
      "the model complies with fair lending practices and does not disproportionately\n",
      "disadvantage any protected class or region.\n",
      "·\n",
      "Informed Consent: Applicants are informed when their data is being used in\n",
      "automated decision-making processes, with clear opt-in mechanisms.\n",
      "·\n",
      "Bias Mitigation: Fairness-aware preprocessing (reweighing)\n",
      "·\n",
      "Transparency: SHAP for explainability\n",
      "·\n",
      "Privacy: GDPR-compliant data anonymization\n",
      "12. Future Work\n",
      "Our work on the loan approval system is an ongoing effort. We have a roadmap for\n",
      "future improvements and innovation:\n",
      "·\n",
      "Integration of Alternative Data: We plan to explore the use of non-traditional\n",
      "data sources, such as utility payment history and rental data, to improve the\n",
      "model's predictive power for thin-file applicants who lack a strong credit history.\n",
      "·\n",
      "Explainable AI (XAI): We will continue to invest in research and development of\n",
      "more robust and intuitive XAI tools. Our goal is to move beyond simple feature\n",
      "importance to generate a complete narrative for each decision.\n",
      "·\n",
      "Real-time Feature Engineering: We aim to develop a system that can create\n",
      "real-time, aggregated features from streaming transaction data, allowing for a\n",
      "\n",
      "\n",
      "--- Page 14 ---\n",
      "more dynamic and up-to-the-minute assessment of an applicant's financial\n",
      "health.\n",
      "·\n",
      "Incorporating Economic Indicators: The model will be enhanced to include\n",
      "macroeconomic indicators (e.g., inflation rates, unemployment rates) to make it\n",
      "more resilient to broad economic shifts.\n",
      "15. Fallback Mechanism\n",
      "Robustness is a key tenet of our system design. We have implemented several fallback\n",
      "mechanisms to handle various types of failures.\n",
      "·\n",
      "Rule Based Model: Retraining:The ML model faced performance degradation\n",
      "grater than 8% under these changed patterns. As a countermeasure, dynamic\n",
      "retraining was implemented once in quarter using updated data, along with\n",
      "incremental learning methods to adapt to rapid changes in applicant profiles.\n",
      "·\n",
      "Human-in-the-Loop: As mentioned, a human loan officer always has the final\n",
      "say. The model serves as an automated recommendation, but the ultimate\n",
      "decision-making authority remains with a human to account for any unforeseen\n",
      "circumstances or new information not captured by the model.\n",
      "o\n",
      "Requests marked as emergency or pandemic-related aid\n",
      "o\n",
      "Such cases are flagged by the system for manual assessment to ensure\n",
      "fair and context-aware decisions\n",
      "·\n",
      "System Failures: In the event of an API or service failure, the system defaults to\n",
      "a predefined set of rules that are based on our traditional underwriting criteria.\n",
      "This ensures business continuity.\n",
      "16. Model Monitoring\n",
      "To ensure the long-term viability and performance of the model, a comprehensive\n",
      "monitoring and alerting system is in place.\n",
      "·\n",
      "Real-time Performance Tracking: We track key metrics (precision, recall, AUC)\n",
      "on a daily basis for the most recent loan applications. This allows us to quickly\n",
      "detect any degradation in performance.\n",
      "·\n",
      "Drift Detection: We monitor for two types of drift:\n",
      "o\n",
      "Data Drift: Changes in the distribution of input features over time (e.g., a\n",
      "sudden increase in Applicant_Income or a shift in Property_Area). This\n",
      "can signal a need for retraining.\n",
      "o\n",
      "Concept Drift: Changes in the relationship between the input features\n",
      "and the target variable (e.g., a good credit history no longer being a strong\n",
      "predictor of repayment). This is a more serious issue and often requires a\n",
      "deeper investigation.\n",
      "\n",
      "\n",
      "--- Page 15 ---\n",
      "·\n",
      "Alerting Systems: Automated alerts are triggered if any performance metric falls\n",
      "below a predefined threshold or if significant data/concept drift is detected. These\n",
      "alerts notify the MLOps and Data Science teams to initiate an investigation or a\n",
      "retraining cycle.\n",
      "17. Performance Under Stress Conditions\n",
      "·\n",
      "To improve the model’s ability to assess loan applications during the pandemic,\n",
      "temporary features were introduced—such as flags indicating COVID-19-related\n",
      "job disruptions and loan types categorized under emergency business or\n",
      "personal relief. These helped the model better understand the financial stress\n",
      "context behind the applications, enabling fairer decisions for individuals and\n",
      "businesses affected by the crisis.\n",
      "·\n",
      "During the COVID-19 pandemic, financial uncertainty led to significant changes\n",
      "in loan application patterns. A surge in applications was observed from both\n",
      "individuals (seeking personal loans due to medical emergencies, job losses, and\n",
      "reduced income) and businesses (seeking emergency funding to sustain\n",
      "operations, manage payroll, or restructure debts).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_bytes  = get_raw_data(client_id=client_id, authority=authority,file_path = file_path)\n",
    "pdf_content = get_onedrive_whitepaper(file_bytes)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd Version ID: 6.0, Last Modified: 2025-08-05T08:44:27Z\n",
      "Extracted PDF text (first 1000 chars):\n",
      "\n",
      "--- Page 1 ---\n",
      "Machine Learning-Based Loan Approval Prediction System\n",
      "for Financial Institutions\n",
      "1. Executive Summary\n",
      "The financial services industry faces a critical challenge in automating and de-risking\n",
      "the loan approval process. Traditional methods, often relying on manual review and\n",
      "static rule-based systems, are prone to human error, inconsistency, and significant\n",
      "processing delays. These limitations result in missed opportunities, elevated credit risk,\n",
      "and suboptimal customer experiences. To address these issues, we have developed a\n",
      "robust machine learning-based Loan Approval Classification System. This model\n",
      "leverages a comprehensive set of applicant data to predict the likelihood of loan\n",
      "repayment, classifying applications as either approved or rejected.\n",
      "Our system is designed to provide a high-level overview of an applicant's\n",
      "creditworthiness, offering a data-driven, objective, and transparent decision-making tool.\n",
      "By integrating advanced machine learning techniques, our mod\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n--- Page 1 ---\\nMachine Learning-Based Loan Approval Prediction System\\nfor Financial Institutions\\n1. Executive Summary\\nThe financial services industry faces a critical challenge in automating and de-risking\\nthe loan approval process. Traditional methods, often relying on manual review and\\nstatic rule-based systems, are prone to human error, inconsistency, and significant\\nprocessing delays. These limitations result in missed opportunities, elevated credit risk,\\nand suboptimal customer experiences. To address these issues, we have developed a\\nrobust machine learning-based Loan Approval Classification System. This model\\nleverages a comprehensive set of applicant data to predict the likelihood of loan\\nrepayment, classifying applications as either approved or rejected.\\nOur system is designed to provide a high-level overview of an applicant's\\ncreditworthiness, offering a data-driven, objective, and transparent decision-making tool.\\nBy integrating advanced machine learning techniques, our model achieves superior\\npredictive accuracy compared to traditional methods. It significantly reduces the time\\nfrom application to decision, minimizes the risk of default, and ensures a consistent, fair\\nevaluation process. This strategic asset not only enhances operational efficiency but\\nalso provides a competitive advantage by enabling faster, more confident lending\\ndecisions.\\n2. Introduction\\nThe process of loan approval is a cornerstone of the financial industry. It involves\\nevaluating a multitude of factors to determine an applicant's creditworthiness and ability\\nto repay a loan. Historically, this process has been labor-intensive, relying on credit\\nofficers to manually review application forms, financial statements, and credit reports.\\nThis manual approach is slow, expensive, and susceptible to biases. The rise of digital\\nbanking and the demand for instant financial services have made this traditional model\\nincreasingly unsustainable.\\nThis white paper details a machine learning-based solution designed to modernize and\\noptimize the loan approval workflow. By building a classification model, our system can\\naccurately predict the Loan_Status (Approved or Rejected) for new applications. The\\nprimary motivation for this project is to create a scalable, efficient, and fair system that\\ncan process thousands of applications in real-time, reducing operational costs while\\nsimultaneously improving the quality of lending decisions. Our model is intended to\\nserve as a decision-support tool for loan officers, enabling them to focus on complex\\ncases and customer relationships rather than routine data analysis.\\n\\n--- Page 2 ---\\n3. Related Work / Literature Review\\nThe field of credit scoring and loan prediction has seen extensive research and\\napplication of various machine learning models. A number of algorithms have been\\nemployed to analyze applicant data and forecast loan outcomes:\\n·\\nLogistic Regression: A statistical method used for binary classification, which is\\nwell-suited for predicting a 'yes' or 'no' outcome for loan approval. It is valued for\\nits simplicity and the interpretability of its results, as it shows how different factors\\ninfluence the final decision.\\n·\\nDecision Trees: These models use a tree-like structure of decisions and their\\npossible consequences. They are easy to understand and visualize, as they\\nmimic human decision-making processes.\\n\\n--- Page 3 ---\\n·\\nRandom Forests: An ensemble method that builds multiple decision trees and\\ncombines their predictions to improve accuracy and reduce overfitting.\\n·\\nSteps of ML Algorithms:\\n\\n--- Page 4 ---\\n4. Data Description\\nThe model is trained on a comprehensive dataset of past loan applications. The dataset\\ncontains a mix of demographic, financial, and behavioral features.\\nFeature Definitions:\\n·\\nLoan_ID: A unique identifier for each loan application.\\n·\\nGender: The applicant's gender (Male/Female).\\n·\\nMarried: Marital status of the applicant (Yes/No).\\n·\\nDependents: Number of dependents the applicant has.\\n·\\nEducation: Applicant's education level (Graduate/Not Graduate).\\n·\\nSelf_Employed: Whether the applicant is self-employed (Yes/No).\\n·\\nApplicant_Income: The applicant's monthly income.\\n·\\nCoapplicant_Income: The co-applicant's monthly income.\\n·\\nLoan_Amount: The amount of the loan requested.\\n·\\nLoan_Amount_Term: The term of the loan in months.\\n·\\nCredit_History: A binary variable indicating if the applicant has a good credit\\nhistory (1.0) or not (0.0). This is a critical predictor.\\n·\\nProperty_Area: The area where the property is located\\n(Rural/Semiurban/Urban).\\n·\\nLoan_Status: The target variable, indicating if the loan was approved (Y) or\\nrejected (N).\\nPreprocessing Steps:\\n1.\\nHandling Missing Values: Missing values are common in real-world data. We\\nemploy different strategies based on the feature type:\\no\\nCategorical Features: Missing values in Gender, Married, Dependents,\\nSelf_Employed, and Credit_History are imputed using the mode (most\\nfrequent value) of the respective columns.\\no\\nNumerical Features: Missing values in Loan_Amount and\\nLoan_Amount_Term are imputed using the mean or median to avoid\\nskewing the distribution.\\n1.\\nData Type Consistency: All features are checked for consistent data types.\\nNumerical features are stored as integers or floats, while categorical features are\\nstored as strings or object types.\\n2.\\nData Balancing: An analysis of the Loan_Status target variable revealed an\\nimbalance, with a significantly higher number of approved loans than rejected\\nones. This imbalance can bias a model to favor the majority class. Imbalance\\n\\n--- Page 5 ---\\ndata handled by ‘class_weight’ = ‘balanced’ parameter in Logistic Regression\\nmodel.\\n𝑤𝑖= 𝑘⋅𝑛𝑖\\n𝑛\\n§\\nwᵢis the weight for class/sample i\\n§\\nk is a constant (e.g., total desired sample size or scaling factor)\\n§\\nnᵢis the count/frequency of class i\\n§\\nn is the total number of samples\\no\\nSynthetic Minority Over-sampling Technique (SMOTE) during the\\ntraining phase to create synthetic data points for the minority class,\\nensuring the model is not biased and can accurately identify both\\napproved and rejected applications.\\n3.\\nData Transformation:\\no\\nNumerical Columns: We apply a normalization technique (e.g.,\\nStandardScaler) to numerical columns (Applicant_Income,\\nCoapplicant_Income, Loan_Amount) to ensure they have a zero mean\\nand unit variance. This prevents features with larger magnitudes from\\ndominating the model's training process.\\no\\nCategorical Features: We use One-Hot Encoding to convert categorical\\nfeatures (Gender, Married, Education, etc.) into a numerical format\\nsuitable for the model. This creates new binary columns for each unique\\ncategory, avoiding the assumption of ordinality that simple label encoding\\nmight introduce.\\n5. Model Architecture\\nWhy Logistic Regression: Preferred choice\\nWe selected Logistic Regression for the loan approval model because it provides high\\ntransparency, explainability, and auditability — essential factors in regulated financial\\nenvironments. While other models like Random Forest marginally outperform it in\\naccuracy, Logistic Regression allows us to clearly communicate how each feature\\ncontributes to the final decision, enabling easier compliance with fairness, bias\\ndetection, and model governance requirements\\n·\\nOperating in a highly regulated environment (e.g. banking, insurance)\\n·\\nNeed to explain decisions to compliance officers or regulators\\n·\\nWanted easy-to-track fairness or bias metrics\\n·\\nPrioritize transparency over marginal gains in accuracy\\n\\n--- Page 6 ---\\nFeature Name\\nType\\nGender\\nCategorical\\nMarried\\nCategorical\\nDependents\\nCategorical\\nEducation\\nCategorical\\nSelf_Employed\\nCategorical\\nApplicantIncome\\nNumerical\\nCoapplicantIncome\\nNumerical\\nLoanAmount\\nNumerical\\nLoan_Amount_Term\\nNumerical\\nCredit_History\\nBinary (0 or 1)\\nProperty_Area\\nCategorical\\nGeneral Equation of Logistic Regression\\n𝑃𝑌= 1 =\\n1\\n1 + 𝑒−𝑍\\n𝑍= 𝛽0 + 𝛽1𝑋1 + 𝛽2𝑋2 +⋯+ 𝛽𝑛𝑋𝑛\\n𝑃𝑌= 1 = 𝑝𝑟𝑜𝑏𝑎𝑏𝑖𝑙𝑖𝑡𝑦 𝑜𝑓 𝑙𝑜𝑎𝑛 𝑎𝑝𝑝𝑟𝑜𝑣𝑎𝑙\\n𝛽0 = 𝑖𝑛𝑡𝑒𝑟𝑐𝑒𝑝𝑡 (𝑏𝑖𝑎𝑠 𝑡𝑒𝑟𝑚)\\n𝛽𝑖= 𝑐𝑜𝑒𝑓𝑓𝑖𝑐𝑖𝑒𝑛𝑡 𝑓𝑜𝑟 𝑓𝑒𝑎𝑡𝑢𝑟𝑒 𝑋𝑖\\n𝑍= 𝛽0 + 𝛽1 ⋅𝐺𝑒𝑛𝑑𝑒𝑟+ 𝛽2 ⋅𝑀𝑎𝑟𝑟𝑖𝑒𝑑+ 𝛽3 ⋅𝐷𝑒𝑝𝑒𝑛𝑑𝑒𝑛𝑡𝑠+ 𝛽4 ⋅𝐸𝑑𝑢𝑐𝑎𝑡𝑖𝑜𝑛+ 𝛽5 ⋅\\n𝑆𝑒𝑙𝑓_𝐸𝑚𝑝𝑙𝑜𝑦𝑒𝑑+ 𝛽6 ⋅𝐴𝑝𝑝𝑙𝑖𝑐𝑎𝑛𝑡𝐼𝑛𝑐𝑜𝑚𝑒+ 𝛽7 ⋅𝐶𝑜𝑎𝑝𝑝𝑙𝑖𝑐𝑎𝑛𝑡𝐼𝑛𝑐𝑜𝑚𝑒+ 𝛽8 ⋅𝐿𝑜𝑎𝑛𝐴𝑚𝑜𝑢𝑛𝑡+ 𝛽9 ⋅\\n𝐿𝑜𝑎𝑛_𝐴𝑚𝑜𝑢𝑛𝑡_𝑇𝑒𝑟𝑚+ 𝛽10 ⋅𝐶𝑟𝑒𝑑𝑖𝑡_𝐻𝑖𝑠𝑡𝑜𝑟𝑦+ 𝛽11 ⋅𝑃𝑟𝑜𝑝𝑒𝑟𝑡𝑦_𝐴𝑟𝑒𝑎_𝑈𝑟𝑏𝑎𝑛+ 𝛽12 ⋅\\n𝑃𝑟𝑜𝑝𝑒𝑟𝑡𝑦_𝐴𝑟𝑒𝑎_𝑆𝑒𝑚𝑖𝑢𝑟𝑏𝑎𝑛\\n6. Training Methodology\\nThe training process is meticulously designed to ensure the model is robust, accurate,\\nand ready for production.\\n1.\\nData Split:\\n·\\nDataset split into three parts:\\no\\nTraining set: 70%\\no\\nValidation set: 15%\\no\\nTesting set: 15%\\n·\\nTraining set purpose: Used to train the model.\\n·\\nValidation set purpose: Used for hyperparameter tuning and model selection.\\n\\n--- Page 7 ---\\n·\\nTesting set purpose: Reserved for final, unbiased evaluation of model\\nperformance.\\n2.\\nHyperparameter Tuning: We use techniques like Grid Search or Bayesian\\nOptimization to find the optimal set of hyperparameters for the Logixtic\\nRegression model. Key parameters tuned include\\no\\nPenalty: Specifies the type of regularization used to avoid overfitting\\no\\nC: Inverse of regularization strength (i.e., smaller C means stronger\\nregularization).\\no\\nSolver: Optimization algorithm used for finding model coefficients.\\no\\nMax_iter: Maximum number of iterations taken by the solver to converge.\\n3.\\nValidation Strategy: We employ k-fold cross-validation during the training\\nphase. The training data is divided into k folds. The model is trained k times,\\neach time using a different fold as the validation set. This robust strategy ensures\\nthe model's performance is not specific to a single data split.\\n4.\\nRetraining Pipelines: The model is not a static artifact. It is part of a continuous\\nlearning loop. A retraining pipeline is established to periodically retrain the model\\non new data, typically on a monthly or quarterly basis. This ensures the model\\nremains relevant and its predictions accurate as consumer behavior and\\neconomic conditions change.\\n7. Evaluation Metrics\\nTo assess the model's performance, we utilize a suite of metrics tailored to the financial\\ndomain. A simple accuracy score is often misleading in imbalanced classification\\nproblems, so we rely on a more comprehensive set of metrics.\\nBusiness-Specific Interpretation\\nScenario\\nPrioritize\\nYou want to maximize profit while reducing risky loans\\nPrecision, F1-score\\nYou want to not miss good applicants\\nRecall, F1-score\\nYou’re building a regulatory-compliant, fair model\\nBalanced Accuracy,\\nFairness metrics\\nYou prioritize avoiding defaults (minimize false approvals)\\nHigh Precision\\nYou prioritize financial inclusion (minimize false rejections)\\nHigh Recall\\nYou want a balanced approval system for early model\\nevaluation\\nF1-score\\n\\n--- Page 8 ---\\n1. Our primary objective is to avoid loan defaults, so we prioritize Precision to\\nreduce false approvals.\\n2. To ensure a balanced and inclusive approval system, we use the F1-score,\\nwhich helps capture both Precision and Recall, ensuring we do not miss eligible\\napplicants.\\n3. Since the dataset is highly imbalanced, we use PR-AUC (Precision-Recall\\nAUC) for a more reliable evaluation of model performance.\\n·\\nPrecision: Prioritize avoiding defaults (minimize false approvals)\\nPrecision =\\nTruePositives\\nTruePositives + FalsePositives\\n·\\nRecall (Sensitivity): Prioritize financial inclusion (minimize false rejections)\\nRecall =\\nTruePositives\\nTruePositives + FalseNegatives\\n·\\nF1-Score: Wanted balanced approval system for early model evaluation\\nF1 - Score = 2 × Precision × Recall\\nPrecision + Recall\\n·\\nPR-AUC (Precision-Recall AUC): In our imbalanced setting, we're more interested\\nin how well the model identifies truly eligible applicants. PR-AUC gives a more\\nrealistic view of our model's performance than ROC-AUC or raw precision alone.\\nData is imbalance and more informative than ROC-AUC (e.g., 90% loan denials,\\n10%\\napprovals).\\n𝑃𝑅−𝐴𝑈𝐶=\\n1\\n0\\n𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛𝑅𝑒𝑐𝑎𝑙𝑙𝑑𝑅𝑒𝑐𝑎𝑙𝑙\\nPrecision =\\nTruePositives\\nTruePositives+FalsePositives\\nRecall =\\nTruePositives\\nTruePositives+FalseNegatives\\n\\n--- Page 9 ---\\nConfusion Matrix\\n\\n--- Page 10 ---\\nThe confusion matrix reveals that the model has a very low rate of False Positives,\\nwhich means it is very effective at avoiding the approval of risky loans. While there are\\nsome False Negatives (good loans that were rejected), the balance between precision\\nand recall is strategically favorable for a conservative lending strategy.\\n8. Results and Analysis:\\nPerformance Outcomes:\\nWe selected Logistic Regression for the loan approval model because it provides high\\ntransparency, explainability, and auditability — essential factors in regulated financial\\nenvironments. While other models like Random Forest marginally outperform it in\\naccuracy\\n·\\nLogistic Regression: is the most favorable model for the loan approval task. It\\nachieves the highest Recall (0.960) and F1 Score (0.913). Ideal for minimizing\\nboth false negatives (missing eligible applicants) and false positives (approving\\nrisky loans).\\n·\\nHighly interpretable and transparent, making it suitable for regulated\\nenvironments.\\n·\\nPR-AUC is the primary metric due to class imbalance. The Decision Tree\\nachieves the highest PR-AUC (0.867). However, it offers lower interpretability\\ncompared to Logistic Regression, which can be a drawback in scenarios where\\nmodel explainability is critical.\\n·\\nHandle Imbalance can bias a model to favor the majority class. Imbalance data\\nhandled by ‘class_weight’ = ‘balanced’ parameter in Logistic Regression\\nmodel.\\n𝑤𝑖= 𝑘⋅𝑛𝑖\\n𝑛\\n§\\nwᵢis the weight for class/sample i\\n§\\nk is a constant (e.g., total desired sample size or scaling factor)\\n§\\nnᵢis the count/frequency of class i\\n§\\nn is the total number of samples\\nPerformance Scores – Logistic Regression with Optimized Hyper parameters:\\nBest Hyperparameter:\\n'params': {\\n'penalty': ['l2'],\\n\\n--- Page 11 ---\\n'C': [ 1,],\\n'solver': ['liblinear'],\\n'max_iter': [200]\\n}\\nTable 1:\\nMetric\\nScores\\nRecall\\n95.9%\\nF1-Score\\n88.7%\\nAccuracy\\n87.1%\\nPrecision\\n87.1%\\n9. Limitations\\n·\\nAssumes Linear Relationship\\no\\nLogistic Regression assumes a linear relationship between input features and the\\nlog-odds of the outcome (approval or rejection).\\no\\nIn reality, loan approval often depends on non-linear interactions (e.g., age vs.\\nincome vs. employment history), which Logistic Regression cannot capture\\nwithout extensive feature engineering (e.g., polynomial terms or interactions).\\n·\\nLimited Expressiveness for Complex Patterns\\no\\nWhile simple and interpretable, Logistic Regression lacks the capacity to model\\ncomplex decision boundaries.\\no\\nIt may fail to identify intricate relationships or non-linear credit risk patterns (e.g.,\\nwhen high income combined with high debt leads to risk).\\no\\nThis can lead to underfitting, where the model performs poorly even on training\\ndata.\\n·\\nSensitive to Multicollinearity\\no\\nLogistic Regression can be unstable when features are highly correlated (e.g.,\\nincome and credit score), leading to inflated or misleading coefficient values.\\no\\nThis not only harms predictive performance but also undermines model\\ninterpretability and trust among stakeholders.\\n·\\nImbalanced Data Handling\\no\\nLogistic Regression, by default, optimizes for overall accuracy, which is\\ninappropriate for imbalanced datasets (common in loan approval, where most\\napplicants are approved).\\no\\nWithout adjustments (like threshold tuning or class weighting), it can ignore the\\nminority class (defaults), resulting in poor recall and higher risk exposure.\\n\\n--- Page 12 ---\\n·\\nDifficulty in Capturing Non-Binary Dependencies\\no\\nIt works best for binary outcomes, but real-world loan decisions are often\\ninfluenced by multi-level risk factors, such as customer behavior segments, loan\\ntypes, and geographic risk variations.\\no\\nExtending logistic regression to multiclass or ordinal settings adds complexity\\nand reduces interpretability.\\n·\\nAssumes Feature Independence\\no\\nLogistic Regression assumes that features contribute independently to the\\nprediction.\\no\\nIn practice, interactions between variables (e.g., employment type and income\\nstability) can be critical, but are ignored unless explicitly modeled.\\n·\\nStatic Nature Without Regular Updates\\no\\nLogistic models require manual retraining to stay up to date.\\no\\nThey don’t adapt automatically to changes in economic conditions, lending\\nregulations, or fraud patterns, which can reduce effectiveness over time.\\n·\\nLack of Confidence Calibration\\no\\nThe predicted probabilities from Logistic Regression may be poorly calibrated,\\nespecially when applied on unseen or shifted data.\\no\\nOverconfidence in predictions can lead to incorrect approvals or rejections,\\nparticularly in borderline cases.\\n10. Deployment Strategy\\nThe loan approval model is deployed as a microservice within Mastercard’s cloud-based\\ninfrastructure. This architecture ensures high availability, scalability, and seamless\\nintegration with existing systems.\\n·\\nAPI Integration: The model is exposed via a RESTful API endpoint. When a\\nnew loan application is submitted, the relevant features are extracted and sent to\\nthis API. The API then returns a probability score and a classification\\n(Approved/Rejected) in real-time.\\n·\\nScalability: The microservice is containerized using Docker and orchestrated\\nusing Kubernetes. This setup allows the system to automatically scale up or\\ndown based on the volume of loan applications, ensuring low latency even during\\npeak usage.\\n·\\nIntegration with Core Systems: The API is integrated with the front-end\\napplication portal, the core banking system, and the loan officer's dashboard.\\nThis creates a streamlined workflow where the model's prediction is a primary\\ninput to the final decision.\\n11. Ethical Considerations\\nThe use of AI in financial decisions, especially for something as significant as a loan,\\ncomes with substantial ethical responsibilities. We have embedded ethical\\nconsiderations throughout the model's lifecycle.\\n\\n--- Page 13 ---\\n·\\nFairness: Avoid bias toward gender, income, or region.\\n·\\nTransparency: Use clear explanations for all predictions.\\n·\\nPrivacy: Protect applicant data and follow data laws.\\n·\\nHuman Oversight: Route uncertain or sensitive cases to human reviewers.\\n·\\nCompliance: Regular checks to ensure fair lending practices.\\n·\\nBias Mitigation: The model includes fairness-aware preprocessing techniques,\\nsuch as reweighing and stratified sampling, to minimize discrimination based on\\ngender, marital status, and education.\\n·\\nTransparency and Explainability: All loan decisions are supported by SHAP\\n(SHapley Additive exPlanations) visualizations to explain individual predictions.\\nThese explanations are accessible to both analysts and applicants, ensuring\\ntransparency.\\n·\\nPrivacy and Data Security: The system complies with GDPR and other regional\\nregulations by implementing data encryption, anonymization, and secure access\\nprotocols during data collection, storage, and processing.\\n·\\nHuman Oversight: High-impact or low-confidence decisions are flagged for\\nmanual review, especially when applicants are from vulnerable or high-risk\\nsegments.\\n·\\nNon-discrimination Compliance: Regular audits are performed to ensure that\\nthe model complies with fair lending practices and does not disproportionately\\ndisadvantage any protected class or region.\\n·\\nInformed Consent: Applicants are informed when their data is being used in\\nautomated decision-making processes, with clear opt-in mechanisms.\\n·\\nBias Mitigation: Fairness-aware preprocessing (reweighing)\\n·\\nTransparency: SHAP for explainability\\n·\\nPrivacy: GDPR-compliant data anonymization\\n12. Future Work\\nOur work on the loan approval system is an ongoing effort. We have a roadmap for\\nfuture improvements and innovation:\\n·\\nIntegration of Alternative Data: We plan to explore the use of non-traditional\\ndata sources, such as utility payment history and rental data, to improve the\\nmodel's predictive power for thin-file applicants who lack a strong credit history.\\n·\\nExplainable AI (XAI): We will continue to invest in research and development of\\nmore robust and intuitive XAI tools. Our goal is to move beyond simple feature\\nimportance to generate a complete narrative for each decision.\\n·\\nReal-time Feature Engineering: We aim to develop a system that can create\\nreal-time, aggregated features from streaming transaction data, allowing for a\\n\\n--- Page 14 ---\\nmore dynamic and up-to-the-minute assessment of an applicant's financial\\nhealth.\\n·\\nIncorporating Economic Indicators: The model will be enhanced to include\\nmacroeconomic indicators (e.g., inflation rates, unemployment rates) to make it\\nmore resilient to broad economic shifts.\\n15. Fallback Mechanism\\nRobustness is a key tenet of our system design. We have implemented several fallback\\nmechanisms to handle various types of failures.\\n·\\nRule Based Model: Retraining:The ML model faced performance degradation\\ngrater than 8% under these changed patterns. As a countermeasure, dynamic\\nretraining was implemented once in quarter using updated data, along with\\nincremental learning methods to adapt to rapid changes in applicant profiles.\\n·\\nHuman-in-the-Loop: As mentioned, a human loan officer always has the final\\nsay. The model serves as an automated recommendation, but the ultimate\\ndecision-making authority remains with a human to account for any unforeseen\\ncircumstances or new information not captured by the model.\\no\\nRequests marked as emergency or pandemic-related aid\\no\\nSuch cases are flagged by the system for manual assessment to ensure\\nfair and context-aware decisions\\n·\\nSystem Failures: In the event of an API or service failure, the system defaults to\\na predefined set of rules that are based on our traditional underwriting criteria.\\nThis ensures business continuity.\\n16. Model Monitoring\\nTo ensure the long-term viability and performance of the model, a comprehensive\\nmonitoring and alerting system is in place.\\n·\\nReal-time Performance Tracking: We track key metrics (precision, recall, AUC)\\non a daily basis for the most recent loan applications. This allows us to quickly\\ndetect any degradation in performance.\\n·\\nDrift Detection: We monitor for two types of drift:\\no\\nData Drift: Changes in the distribution of input features over time (e.g., a\\nsudden increase in Applicant_Income or a shift in Property_Area). This\\ncan signal a need for retraining.\\no\\nConcept Drift: Changes in the relationship between the input features\\nand the target variable (e.g., a good credit history no longer being a strong\\npredictor of repayment). This is a more serious issue and often requires a\\ndeeper investigation.\\n\\n--- Page 15 ---\\n·\\nAlerting Systems: Automated alerts are triggered if any performance metric falls\\nbelow a predefined threshold or if significant data/concept drift is detected. These\\nalerts notify the MLOps and Data Science teams to initiate an investigation or a\\nretraining cycle.\\n17. Performance Under Stress Conditions\\n·\\nTo improve the model’s ability to assess loan applications during the pandemic,\\ntemporary features were introduced—such as flags indicating COVID-19-related\\njob disruptions and loan types categorized under emergency business or\\npersonal relief. These helped the model better understand the financial stress\\ncontext behind the applications, enabling fairer decisions for individuals and\\nbusinesses affected by the crisis.\\n·\\nDuring the COVID-19 pandemic, financial uncertainty led to significant changes\\nin loan application patterns. A surge in applications was observed from both\\nindividuals (seeking personal loans due to medical emergencies, job losses, and\\nreduced income) and businesses (seeking emergency funding to sustain\\noperations, manage payroll, or restructure debts).\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_version(client_id=client_id, authority=authority,  file_path= file_path, version_id = version_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87ed6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get updated file from github"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
